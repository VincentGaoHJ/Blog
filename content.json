{"posts":[{"title":"A Comprehensive Look at The Multi-Armed Bandit Problem","text":"The multi-armed bandit problem is a class example to demonstrate the exploration versus exploitation dilemma. This post introduces the bandit problem and how to solve it using different exploration strategies. [toc] What is Multi-Armed Bandit?Exploitation vs ExplorationExploration refers to the discovery of new products, resources, knowledge and opportunities, and it is associated with radical changes and learning through experimentation. Exploitation refers to the refinement of existing products, resources, knowledge and competencies, and is associated with incremental changes and learning through local search (Benner &amp; Tushman, 2003; March, 1991). The best long-term strategy may involve short-term sacrifices. For example, one exploration trial could be a total failure, but it warns us of not taking that action too often in the future. DefinitionThe multi-armed bandit (short: bandit or MAB) can be seen as a set of real distributions $B={R_{1},\\dots ,R_{K}}$, each distribution being associated with the rewards delivered by one of the $K\\in \\mathbb {N} ^{+}$ levers. Let $\\mu _{1},\\dots ,\\mu _{K}$ be the mean values associated with these reward distributions. The gambler iteratively plays one lever per round and observes the associated reward. The objective is to maximize the sum of the collected rewards. The horizon $H$ is the number of rounds that remain to be played. The loss that we incur due to time/rounds spent due to the learning is called regret. In other words, we want to maximise my reward even during the learning phase. Regret is very aptly named, as it quantifies exactly how much you regret not picking the optimal arm. The bandit problem is formally equivalent to a one-state Markov decision process. Bandit Strategiesε-Greedy AlgorithmAccording to the ε-greedy algorithm, with a small probability $\\epsilon$ we take a random action, but otherwise (which should be the most of the time, probability $1-epsilon$ ) we pick the best action that we have learnt so far. With probability 1- epsilon – we choose action with maximum value (argmaxa Qt(a)) With probability epsilon – we randomly choose an action from a set of all actions A Upper Confidence BoundsUpper Confidence Bound (UCB) is the most widely used solution method for multi-armed bandit problems. This algorithm is based on the principle of optimism in the face of uncertainty. In other words, the more uncertain we are about an arm, the more important it becomes to explore that arm. UCB says that we should choose the arm a1 and receive a reward making us less uncertain about its action-value. For the next trial/timestep, if we still are very uncertain about a1, we will choose it again until the uncertainty is reduced below a threshold. UCB is actually a family of algorithms. Here, we will discuss UCB1. UCB1Steps involved in UCB1: Play each of the K actions once, giving initial values for mean rewards corresponding to each action at For each round t = K: Let Nt(a) represent the number of times action a was played so far Play the action at maximising the following expression Observe the reward and update the mean reward or expected payoff for the chosen action $$\\begin{equation}Q(a)+\\sqrt{\\frac{2 \\log t}{N_{t}(a)}}\\end{equation}$$ Each time a is selected, the uncertainty is presumably reduced: $N_t(a)$ increments, and, as it appears in the denominator, the uncertainty term decreases. On the other hand, each time an action other than a is selected, t increases, but Nt(a) does not; because t appears in the numerator, the uncertainty estimate increases. Regret Comparison Among all the algorithms given in this article, only the UCB algorithm provides a strategy where the regret increases as log(t), while in the other algorithms we get linear regret with different slopes. SummaryWe need exploration because information is valuable. In terms of the exploration strategies, we can do no exploration at all, focusing on the short-term returns. Or we occasionally explore at random. Or even further, we explore and we are picky about which options to explore — actions with higher uncertainty are favored because they can provide higher information gain. ReferencesBlog The Multi-Armed Bandit Problem and Its Solutions – Lil’Log Solving the Multi-Armed Bandit Problem – Anson Wong Paper [Multi-armed Bandit Algorithms and Empirical Evaluation – Joann`es Vermorel and Mehryar Mohri](https://link.springer.com/chapter/10.1007/11564096_42)","link":"/Blog/2020/11/06/A-Comprehensive-Look-at-The%20Multi-Armed-Bandit-Problem/"},{"title":"AWS (Amazon Web Services) Study Notes","text":"The study notes during the internship period of AWS, including AWS Global Infrastructure, Networking in AWS, Storage in AWS, Compute in AWS and Database in AWS, to name a few. [TOC] AWS Global InfrastructureComponents Regions Available Zone Fully isolated data centers Meaningful distance of separation 80公里 Long distance will lead to the high synchronization delay. Short distance will bring high risk that multiple available zone are affected at the same time. Data Centers Points of Presence(PoP) POP is primarily the infrastructure that allows remote users connect to the Internet. Network Custom Hardware Machine Learning on AWS Application Developers SageMaker Rekognition Polly Lex etc. Data Scientists &amp; Researchers AWS Deep Learning AMI Optimized for distributed machine Networking in AWSVPC (Virtual Private Cloud)Amazon Virtual Private Cloud (Amazon VPC) lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. The relations between VPC and region, VPC and AZ are below: VPCs deploy into 1 of the 18 AWS regions A VPC can host resources from any Available Zone within its region SubnetsWhen you create a subnet, you specify the CIDR block for the subnet, which is a subset of the VPC CIDR block. When you create a VPC, you must specify a range of IPv4 addresses for the VPC in the form of a Classless Inter-Domain Routing (CIDR) block; for example, 10.0.0.0/16. This is the primary CIDR block for your VPC. GatewayInternet Gateway An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. It therefore imposes no availability risks or bandwidth constraints on your network traffic. An internet gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses. NAT Gateway You can use a NAT device to enable instances in a private subnet to connect to the internet (for example, for software updates) or other AWS services, but prevent the internet from initiating connections with the instances. A NAT device forwards traffic from the instances in the private subnet to the internet or other AWS services, and then sends the response back to the instances. When traffic goes to the internet, the source IPv4 address is replaced with the NAT device’s address and similarly, when the response traffic goes to those instances, the NAT device translates the address back to those instances’ private IPv4 addresses. Elastic Network InterfacesYou can create a network interface, attach it to an instance, detach it from an instance, and attach it to another instance. The attributes of a network interface follow it as it’s attached or detached from an instance and reattached to another instance. When you move a network interface from one instance to another, network traffic is redirected to the new instance. 弹性 IP 就是固定 IP ，弹性是指的可以弹性的关联实例，对于实例来说，弹性 IP 的作用就是固定 IP 。 SecuritySecurity Groups A security group acts as a virtual firewall for your instance to control inbound and outbound traffic. When you launch an instance in a VPC, you can assign up to five security groups to the instance. Security groups act at the instance level, not the subnet level. Therefore, each instance in a subnet in your VPC could be assigned to a different set of security groups. If you don’t specify a particular group at launch time, the instance is automatically assigned to the default security group for the VPC. For each security group, you add rules that control the inbound traffic to instances, and a separate set of rules that control the outbound traffic. This section describes the basic things you need to know about security groups for your VPC and their rules. Network ACLs A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC. Comparison of Security Groups and Network ACLs Traffic from an Internet gateway is routed to the appropriate subnet using the routes in the routing table. The rules of the network ACL associated with the subnet control which traffic is allowed to the subnet. The rules of the security group associated with an instance control which traffic is allowed to the instance. AWS Direct ConnectAWS Direct Connect links your internal network to an AWS Direct Connect location over a standard Ethernet fiber-optic cable. One end of the cable is connected to your router, the other to an AWS Direct Connect router. With this connection, you can create virtual interfaces directly to public AWS services (for example, to Amazon S3) or to Amazon VPC, bypassing internet service providers in your network path. An AWS Direct Connect location provides access to AWS in the Region with which it is associated. You can use a single connection in a public Region or AWS GovCloud (US) to access public AWS services in all other public Regions. VPC PeeringAmazon Virtual Private Cloud (Amazon VPC) enables you to launch AWS resources into a virtual network that you’ve defined. A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an inter-region VPC peering connection). Transit GatewayAWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway. With AWS Transit Gateway, you only have to create and manage a single connection from the central gateway in to each Amazon VPC, on-premises data center, or remote office across your network. Transit Gateway acts as a hub that controls how traffic is routed among all the connected networks which act like spokes. This hub and spoke model significantly simplifies management and reduces operational costs because each network only has to connect to the Transit Gateway and not to every other network. Any new VPC is simply connected to the Transit Gateway and is then automatically available to every other network that is connected to the Transit Gateway. This ease of connectivity makes it easy to scale your network as you grow. VPC EndpointsA VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network. 主要是出于安全及合规考虑，访问 AWS 公有服务时，不走 Internet。主要分为 2 类： Gateway VPC Endpoint 早期的技术实现，主要针对 S3 和 DynamoDB，将这些 AWS 服务的公网路由注入 VPC 及 Subnet 的路由表中（用 PL-xxxxxxxx 标识、作为 Destination），VPC Endpoint 作为 Target（用 vpce-xxxxxxxx 标识，应该是提供 NAT 功能）。可以在 VPC Endpoint 配置 IAM 策略，能够访问哪些 S3 Bucket；也可以在 S3 Bucket 配置 IAM 策略，能够被哪些 VPC 或 VPC Endpoint 访问，不能采用基于源 IP 地址的策略。此外在安全组也可以引用 PL-xxxxxxxx、配置策略，网络 ACL 中不能引用 PL-xxxxxxxx。 Interface VPC Endpoint 最新的技术实现，基于 AWS PrivateLink 技术，针对 EC2、ELB、Kinesis 等，为这些 AWS 服务在 Consumer VPC 增加了一个或多个 ENI 接口及 IP 地址，同时为这些 ENI 接口提供 Region 及 Zone 的 DNS 域名（公网可解析、返回私网 IP 地址），也可以在 Consumer VPC 内部将标准 AWS 服务域名（如：ec2.us-east-2.amazonaws.com）解析为这些 ENI 接口的私有 IP 地址。 通过 PrivateLink 技术，我们自己也可以对外发布 Endpoint Service：在 Provider VPC 创建 Network ELB 及 Back-end 服务器、基于 ELB 创建 Endpoint Service；在 Consumer VPC 创建 Interface VPC Endpoint、引用 Provider VPC 的 Endpoint Service。 ELB (Elastic Load Balancing)Elastic Load Balancing distributes incoming application or network traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses, in multiple Availability Zones. Elastic Load Balancing scales your load balancer as traffic to your application changes over time, and can scale to the vast majority of workloads automatically. Elastic Load Balancing supports three types of load balancers: Application Load Balancers Network Load Balancers Classic Load Balancers You can select the appropriate load balancer based on your application needs. If you need flexible application management, we recommend that you use an Application Load Balancer. If extreme performance and static IP is needed for your application, we recommend that you use a Network Load Balancer. If you have an existing application that was built within the EC2-Classic network, then you should use a Classic Load Balancer. Tips Please pay attention to the FAQs. API GatewayAmazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services as well as data stored in the AWS Cloud. Features of API Gateway Support for stateful (WebSocket) and stateless (REST) APIs. Powerful, flexible authentication mechanisms, such as AWS Identity and Access Management policies, Lambda authorizer functions, and Amazon Cognito user pools. Developer portal for publishing your APIs. Canary release deployments for safely rolling out changes. CloudTrail logging and monitoring of API usage and API changes. Integration with AWS WAF for protecting your APIs against common web exploits. Integration with AWS X-Ray for understanding and triaging performance latencies. Storage in AWSS3 (Simple Storage Service)Amazon Simple Storage Service (Amazon S3) is storage for the internet. You can use Amazon S3 to store and retrieve any amount of data at any time, from anywhere on the web. Buckets A bucket is a container for objects stored in Amazon S3. Every object is contained in a bucket. Objects Objects are the fundamental entities stored in Amazon S3. Keys A key is the unique identifier for an object within a bucket. Regions You can choose the geographical region where Amazon S3 will store the buckets you create. S3 GlacierGlacier is an extremely low-cost storage service that provides durable storage with security features for data archiving and backup. With Glacier, customers can store their data cost effectively for months, years, or even decades. EBS (Elastic Block Store)Amazon Elastic Block Store (Amazon EBS) provides block level storage volumes for use with EC2 instances. EBS volumes behave like raw, unformatted block devices. You can mount these volumes as devices on your instances. Amazon EBS is recommended when data must be quickly accessible and requires long-term persistence. Comparison of EBS and S3 EBS 是块存储，S3 是对象存储。EBS 仅能与 EC2 实例结合使用。你可以把 EBS 想象成 EC2 的硬盘，把 S3 就想象成一个网盘； 收费：EBS 的卷存储按每月预置的 GB 量计费，无论使用与否，而 S3 按照实际使用 GB 量收费； 请求：EBS 按卷的 I/O 请求进行收费，S3 对 GET 及所有其他请求按次数进行收费，对于小文件 S3 的请求费用甚至会高于传输费用； 数据传出：两者数据传出至 Internet 的费用目前一致；S3 的数据存储相对更为可靠，S3 通过冗余方式将数据同步到多个设备，而 EBS 卷的持久性取决于您的卷大小和自上次快照后数据更新的比例，因此 EBS 提高持久性需定期快照至 S3。 Compute in AWSEC2 (Elastic Compute Cloud)Amazon Elastic Compute Cloud (Amazon EC2) provides scalable computing capacity in the Amazon Web Services (AWS) cloud. Using Amazon EC2 eliminates your need to invest in hardware up front, so you can develop and deploy applications faster. An Amazon EC2 Windows instance is similar to the traditional Windows Server. After you launch an instance, it briefly goes into the pending state while registration takes place, then it goes into the running state. The instance remains active until you stop or terminate it. You can’t restart an instance after you terminate it. You can create a backup image of your instance while it’s running, and launch a new instance from that backup image. Instance Purchasing Options On-Demand With On-Demand instances, you pay for compute capacity by per hour or per second depending on which instances you run. Spot instances Amazon EC2 Spot instances allow you to request spare Amazon EC2 computing capacity for up to 90% off the On-Demand price. Reserved Instances Reserved Instances provide you with a significant discount (up to 75%) compared to On-Demand instance pricing. Differences between Dedicated Hosts and Dedicated Instances Dedicated Hosts and Dedicated Instances can both be used to launch Amazon EC2 instances onto physical servers that are dedicated for your use. Dedicated Instances 知道在一个独享的设备上运行，但是不知道在那个设备上。Dedicated Host 不仅独享一个设备，而且知道是在那一个设备上运行。 Dedicated Host Dedicated Instance Visibility of sockets, cores, and host ID Provides visibility of the number of sockets and physical cores No visibility Host and instance affinity Allows you to consistently deploy your instances to the same physical server over time Not supported Targeted instance placement Provides additional visibility and control over how instances are placed on a physical server Not supported Automatic instance recovery Supported. For more information, see Host Recovery. Supported Bring Your Own License (BYOL) Supported Not supported Database in AWSOverview Relational Database RDS Redshift Non-relational Database Aurora DocumentDB DynamoDB Neptune 类型 关系型数据库 非关系型数据库 特性 1、采用了关系模型来组织数据的数据库； 2、最大特点就是事务的一致性； 3、简单来说，关系模型指的就是二维表格模型， 而一个关系型数据库就是由二维表及其之间的联系所组成的一个数据组织。 1、使用键值对存储数据； 2、分布式； 3、一般不支持 ACID 特性； 4、非关系型数据库严格上不是一种数据库，应该是一种数据结构化存储方法的集合。 优点 1、容易理解：二维表结构是非常贴近逻辑世界一个概念，关系模型相对网状、层次等其他模型来说更容易理解； 2、使用方便：通用的 SQL 语言使得操作关系型数据库非常方便； 3、易于维护：丰富的完整性 (实体完整性、参照完整性和用户定义的完整性) 大大减低了数据冗余和数据不一致的概率； 4、支持 SQL，可用于复杂的查询。 1、无需经过 sql 层的解析，读写性能很高； 2、基于键值对，数据没有耦合性，容易扩展； 3、存储数据的格式：nosql 的存储格式是 key,value 形式、文档形式、图片形式等等，文档形式、图片形式等等，而关系型数据库则只支持基础类型。 缺点 1、为了维护一致性所付出的巨大代价就是其读写性能比较差； 2、固定的表结构； 3、高并发读写需求； 4、海量数据的高效率读写； 1、不提供 sql 支持，学习和使用成本较高； 2、无事务处理，附加功能 bi 和报表等支持也不好； Management &amp; Governance in AWSAuto ScalingAWS Auto Scaling enables you to configure automatic scaling for the AWS resources that are part of your application in a matter of minutes. With AWS Auto Scaling, you configure and manage scaling for your resources through a scaling plan. The scaling plan uses dynamic scaling and predictive scaling to automatically scale your application’s resources. Ways to Auto Scale Scheduled – Excellent for predictable workloads Dynamic – Most used Predictive – Could combine with machine learning CloudWatchAmazon CloudWatch monitors your Amazon Web Services (AWS) resources and the applications you run on AWS in real time. You can use CloudWatch to collect and track metrics, which are variables you can measure for your resources and applications. Amazon CloudWatch is basically a metrics repository. An AWS service—such as Amazon EC2—puts metrics into the repository, and you retrieve statistics based on those metrics. If you put your own custom metrics into the repository, you can retrieve statistics on these metrics as well. The CloudWatch overview home page appears. What can I do with Cloudwatch? Dashboards - Creates awesome dashboards to see what is happening with your AWS environment. Alarms - Allows you to set Alarms that notify you when particular thresholds are hit. Events - CloudWatch Events helps you to respond to state changes in your AWS resources. Logs - CloudWatch Logs helps you to aggregate, monitor, and store logs. CloudFormationAWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing those resources and more time focusing on your applications that run in AWS. You create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and AWS CloudFormation takes care of provisioning and configuring those resources for you. The following scenarios demonstrate how AWS CloudFormation can help. Simplify Infrastructure Management Quickly Replicate Your Infrastructure Easily Control and Track Changes to Your Infrastructure Related Information Application Integration in AWSOverview SQS - Queue SNS - Notification SWF - Work Flow SQS (Simple Queue Service)Amazon Simple Queue Service (Amazon SQS) offers a secure, durable, and available hosted queue that lets you integrate and decouple distributed software systems and components. Amazon SQS offers common constructs such as dead-letter queues and cost allocation tags. It provides a generic web services API and it can be accessed by any programming language that the AWS SDK supports. Amazon SQS supports both standard and FIFO queues. Standard Queue FIFO Queue Unlimited Throughput – Standard queues support a nearly unlimited number of transactions per second (TPS) per API action. High Throughput – By default, FIFO queues support up to 3,000 messages per second, per API action , with batching. FIFO queues support up to 300 messages per second, per API action without batching. At-Least-Once Delivery – A message is delivered at least once, but occasionally more than one copy of a message is delivered. Exactly-Once Processing – A message is delivered once and remains available until a consumer processes and deletes it. Duplicates aren’t introduced into the queue. Best-Effort Ordering – Occasionally, messages might be delivered in an order different from which they were sent. First-In-First-Out Delivery – The order in which messages are sent and received is strictly preserved. Send data between applications when the throughput is important, for example: Decouple live user requests from intensive background work: let users upload media while resizing or encoding it.Allocate tasks to multiple worker nodes: process a high number of credit card validation requests.Batch messages for future processing: schedule multiple entries to be added to a database. Send data between applications when the order of events is important, for example: Ensure that user-entered commands are executed in the right order.Display the correct product price by sending price modifications in the right order.Prevent a student from enrolling in a course before registering for an account. SQS EXAM TIPS SQS is a distributed message queueing system Allows you to decouple the components of an application So that they are independent Pull-based, not push-based Standard Queues (default)- best effort ordering message delivered at least once FIFO Queues (First In First Out)一ordering strictly preserved, message delivered once, no duplicates. e.g. good for banking transactions which need to happen in strict order. Visibility Timeout Default is 30 seconds - Increase If your task takes &gt; 30 seconds to complete Max 12 hours Short Polling-peturned immediately even if no messages are in the queue Long Polling - polls the queue perodically and only returns a response when a message is in the queue or the timeout reached SNS (Simple Notification Service)Amazon Simple Notification Service (Amazon SNS) is a web service that coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. In Amazon SNS, there are two types of clients—publishers and subscribers—also referred to as producers and consumers. Publishers communicate asynchronously with subscribers by producing and sending a message to a topic, which is a logical access point and communication channel. Differences between SNS and SQSSNS 是分布式发布订阅系统。当发布者发送给 SNS 时，邮件会被推送到订阅者。 SQS 是分布式排队系统。消息不会推送到接收器。接收器必须轮询 SQS 以接收消息。消息不能由多个接收器同时接收。任何一个接收器可以接收消息，处理和删除它。其他接收器稍后不接收相同的消息。轮询固有地在 SQS 中的消息传递中引入了一些延迟，而不是 SNS，其中消息被立即推送到订户。 SNS 支持多个端点，例如电子邮件，短信，http 端点和 SQS。如果想要未知数量和类型的订阅者接收邮件，您需要 SNS。 SQS 主要用于解耦应用程序或集成应用程序。消息可以短期存储在 SQS 中 (最多 14 天)。 SNS 将多个消息副本分发给多个订户。例如，假设您要将应用程序生成的数据复制到多个存储系统。您可以使用 SNS 并将此数据发送给多个订阅者，每个订阅者都会将其收到的消息复制到不同的存储系统 (s3，主机上的硬盘，数据库等)。 SWF (Simple Workflow Service)Using the Amazon Simple Workflow Service (Amazon SWF), you can implement distributed, asynchronous applications as workflows. Workflows coordinate and manage the execution of activities that can be run asynchronously across multiple computing devices and that can feature both sequential and parallel processing. For example, the following figure shows a simple e-commerce order-processing workflow involving both people and automated processes. Workflow Execution Bringing together the ideas discussed in the preceding sections, here is an overview of the steps to develop and run a workflow in Amazon SWF: Write activity workers that implement the processing steps in your workflow. An activity worker is a program that receives activity tasks, performs them, and provides results back. Note that the task itself might actually be performed by a person, in which case the person would use the activity worker software for the receipt and disposition of the task. An example might be a statistical analyst, who receives sets of data, analyzes them, and then sends back the analysis. Write a decider to implement the coordination logic of your workflow. The coordination logic in a workflow is contained in a software program called a decider. The decider schedules activity tasks, provides input data to the activity workers, processes events that arrive while the workflow is in progress, and ultimately ends (or closes) the workflow when the objective has been completed. Register your activities and workflow with Amazon SWF. You can do this step programmatically or by using the AWS Management Console. Start your activity workers and decider. These actors can run on any computing device that can access an Amazon SWF endpoint. For example, you could use compute instances in the cloud, such as Amazon Elastic Compute Cloud (Amazon EC2); servers in your data center; or even a mobile device, to host a decider or activity worker. Once started, the decider and activity workers should start polling Amazon SWF for tasks. Start one or more executions of your workflow. Executions can be initiated either programmatically or via the AWS Management Console. Each execution runs independently and you can provide each with its own set of input data. When an execution is started, Amazon SWF schedules the initial decision task. In response, your decider begins generating decisions which initiate activity tasks. Execution continues until your decider makes a decision to close the execution. View workflow executions using the AWS Management Console. You can filter and view complete details of running as well as completed executions. For example, you can select an open execution to see which tasks have completed and what their results were. Differences between SWF and SQS Amazon SWF presents a task oriented API, whereas Amazon SQS offers a message oriented API. Amazon SWF ensures that a task is assigned only once and is never duplicated. With Amazon SQS, you need to handle duplicated messages and may also need to ensure that a message is processed only once. Amazon SWF keeps track of all the tasks and events in an application. With Amazon SQS, you need toimplement your own application-level tracking especially if your application uses multiple queues. Media Services in AWSElastic TranscoderAmazon Elastic Transcoder lets you convert media files that you have stored in Amazon Simple Storage Service (Amazon S3) into media files in the formats required by consumer playback devices. For example, you can convert large, high-quality digital media files into formats that users can play back on mobile devices, tablets, web browsers, and connected televisions. Elastic Transcoder has four components: Jobs do the work of transcoding. Each job converts one file into up to 30 formats. For example, if you want to convert a media file into six different formats, you can create files in all six formats by creating a single job. Pipelines are queues that manage your transcoding jobs. When you create a job, you specify which pipeline you want to add the job to. Elastic Transcoder starts processing the jobs in a pipeline in the order in which you added them. If you configure a job to transcode into more than one format, Elastic Transcoder creates the files for each format in the order in which you specify the formats in the job. Presets are templates that contain most of the settings for transcoding media files from one format to another. Elastic Transcoder includes some default presets for common formats, for example, several iPod and iPhone versions. You can also create your own presets for formats that aren’t included among the default presets. You specify which preset you want to use when you create a job. Notifications let you optionally configure Elastic Transcoder and Amazon Simple Notification Service to keep you apprised of the status of a job: when Elastic Transcoder starts processing the job, when Elastic Transcoder finishes the job, and whether Elastic Transcoder encounters warning or error conditions during processing. Analytics in AWSKinesisAmazon Kinesis makes it easy to collect, process, and analyze video and data streams in real time. SQS EXAM TIPSKnow the differences between video stream and Firehose, and choose the proper service under specific scenarios. Kinesis Video StreamsAmazon Kinesis Video Streams is a fully managed AWS service that enables you to stream live video from devices to the AWS Cloud and durably store it. You can then build your own applications for real-time video processing or perform batch-oriented video analytics. Kinesis Data FirehoseAmazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon Elasticsearch Service (Amazon ES), and Splunk. For Amazon S3 destinations, streaming data is delivered to your S3 bucket. If data transformation is enabled, you can optionally back up source data to another Amazon S3 bucket. Reference AWS Documentation AWS Certification Catalog 经验分享：成功通过 AWS Advanced Networking Specialty 认证考试 A CLOUD GURU","link":"/Blog/2019/08/07/AWS-Amazon-Web-Services-Study-Notes/"},{"title":"AWS Certified Machine Learning – Specialty","text":"Content Outline Domain percentage of Examination Domain 1: Data Engineering 20% Domain 2: Exploratory Data Analysis 24% Domain 3: Modeling 36% Domain 4: Machine Learning Implementation and Operations 20% TOTAL 100% Domain 1: Data Engineering 1.1 Create data repositories for machine learning. 1.2 Identify and implement a data-ingestion solution. 1.3 Identify and implement a data-transformation solution. Domain 2: Exploratory Data Analysis 2.1 Sanitize and prepare data for modeling. 2.2 Perform feature engineering. 2.3 Analyze and visualize data for machine learning. Domain 3: Modeling 3.1 Frame business problems as machine learning problems. 3.2 Select the appropriate model(s) for a given machine learning problem. 3.3 Train machine learning models. 3.4 Perform hyper-parameter optimization. 3.5 Evaluate machine learning models. Domain 4: Machine Learning Implementation and Operations 4.1 Build machine learning solutions for performance, availability, scalability, resiliency, and fault tolerance. 4.2 Recommend and implement the appropriate machine learning services and features for a given problem. 4.3 Apply basic AWS security practices to machine learning solutions. 4.4 Deploy and operationalize machine learning solutions. ReferenceAWS Certified Machine Learning – Specialty Exam Guide Jayendra’s Cloud Certification Blog","link":"/Blog/2021/04/18/AWS-Certified-Machine-Learning-Specialty/"},{"title":"AWS Certified Machine Learning (Specialty) - Topic 1: Data Engineering","text":"AWS Certified Machine Learning - Specialty is an advanced certification a bit different from the others, because it is the only one which focuses on specific sector knowledge not strictly tied to AWS services. In fact, in order to pass the exam and obtain the certification, it’s fundamental being able to recognize, analyze and optimize different machine learning problems starting from use cases’ descriptions, without them being exclusively linked to peculiar AWS’ solutions. [toc] Data EngineeringAmazon S3This is my previous article about Amazon Storage(for solution architect), and you can get pretty much everything you need to know about AWS Storage. AWS S3 for Machine Learning Backbone for many AWS ML services (e.g. SageMaker) Create a “Data Lake” Infinite size, no provisioning 99.999999999% durability Decoupling of storage (S3) to compute (EC2, Amazon Athena, Amazon Redshift Spectrum, Amazon Rekognition, and AWS Glue) Centralized Architecture: All your data can be in one place Object Storage =&gt; Supports any file format (Common format for ML: CSV, JSON) AWS S3 Data Partitioning Pattern for speeding up range queries (ex: AWS Athena) You can define whatever partitioning strategy you like Data Partitioning will be handled by some tools we use (e.g. AWS Glue) S3 Encryption for Objects There are 4 methods of encrypting objects in S3 SSE-S3: encrypts S3 objects using keys handled &amp; managed by AWS SSE-KMS: use AWS Key Management Service to manage encryption keys Additional security (user must have access to KMS key) Audit trail for KMS key usage SSE-C: when you want to manage your own encryption keys Client Side Encryption From an ML perspective, SSE-S3 and SSE-KMS will be most likely used. AWS Kinesis ReferencesDive Into Exam","link":"/Blog/2021/04/18/AWS-Machine-Learning-Specialty-1-Data-Engineer/"},{"title":"AWS Solution Architect(Associate) - Topic 1: Identity Access Management and S3","text":"Key Terminology For IAM What is S3 ? How does data consistency work for S3 ? S3 - Guarantees / Features / Storage Classes / Charges / Pricing Tiers / Security &amp; Encryption / Version Control / Object Lock / Glacier Vault Lock / Performance / Select &amp; Glacier Select Exam Tips &amp; REMEMBER TO READ FAQ [toc] IAMKey Terminology For IAM Users End Users such as people, employees and organizations etc. Groups A collection of users. Each users in the group will inherit the permissions of the group. Policies Policies are made up of documents, called Policy documents. These documents are in a format called JSON and they give permissions as what a user/group/role is able to do. Roles You create roles and then assign them to AWS Resources. For example, you might give a virtual machine inside AWS as the ability to write files to S3 which is a type of storage within the AWS. Exam Tips IAM is universal. It doesn’t apply to regions at this time. Always setup Multi-factor Authentication on your root account. You can create and customize your own password rotation policies. S3What is S3 It’s Object-based storage(allows you to upload files.) Key Value Version ID(Important for versioning) Metadata Subresources Access Control Lists Torrent Files can be from 0 Bytes to 5 TB. Files are stored in Buckets. S3 is a universal namespace. That is, names must be unique globally. HTTP 200 code if the upload wad successful. Web Address https://acloudguru.s3.amazonaws.com/ (default region) https://acloudguru.eu-west-1.amazonaws.com/ How does data consistency work for S3 Read after Write consistency for PUTS of NEW Objects. It means you write a new file and read it immediately afterwards, you will be able to view that data. Eventual consistency for overwrite PUTS and DELETES(can take some time to propagate). If you update AN EXISTING file or delete a file and read it immediately, you may get the older version, or you may not. Basically changes to objects can take a little bit of time to propagate. Guarantees 99.9% availability 11 * 9 durability Features Tiered Storage Available Lifecycle Management Tier A to Tier B to Glacier Versioning Encryption MFA Delete Secure your data using Access Control Lists and Bucket Policies Storage Classes S3 Standard S3 - IA (Infrequently Accessed) requires rapid access when needed Charged a retrieval fee S3 One Zone - IA Phased out version: RRS (S3 Reduce Redundancy Storage), still exists. S3 - Intelligent Tiering Designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. S3 Glacier Retrieval times configurable from minutes to hours. S3 Glacier Deep Archive retrieval time of 12 hours is acceptable. Charges Storage Requests Storage Management Pricing Data Transfer Pricing Transfer Acceleration Cross Region Replication Pricing Pricing Tiers What makes up the cost of S3? Storage (Understand how to get the best value out of S3) Requests and Data Retrievals Data Transfer Management &amp; Replication Security &amp; Encryption Encryption In Transit is achieved by SSL/TLS HTTPS、SSL、TLS 三者之间的联系和区别 聊聊 HTTPS 和 SSL/TLS 协议 Encryption At Rest (Server Side) is achieved by S3 Managed Keys: SSE (Server Side Encryption) - S3 AWS Key Management Service, Managed Keys: SSE-KMS This is where you and Amazon manage the keys together. Server Side Encryption With Customer Provided Keys: SSE-C This is where you actually give Amazon your own keys that you manage and you can encrypt your S3 objects Client Side means encrypt documents before uploading to S3 Version Control Using Versioning With S3 Stores all versions of an object Great backup tools Once enables, Versioning cannot be disabled, only suspended. Integrates with Life-cycle rules. Versioning’s MFA Delete capability, which uses multi-factor authentication, can be used to provide an additional layer of security. Object Lock &amp; Glacier Vault LockObject LockYou can use S3 Object Lock to store objects using a write once, read many (WORM) model. It can help you prevent objects from being deleted or modified for a fixed amount of time or indefinitely. Governance Mode: users can’t overwrite or delete an object version or alter its lock settings unless they have special permissions. Compliance Mode: a protected object version can’t be overwritten or deleted by any user, including the root user in your aws account. Compliance mode ensures an object version can’t be overwritten or deleted for the duration of the retention period. Retention Period &amp; Legal Holds Retention period: Protects an object version for a fixed amount of time. After the retention period expires, the object version can be overwritten or deleted unless you also placed a legal hold on the object version. Legal Holds: S3 Object Lock also enables you to place a legal hold on an object version. Like a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn’t have an associated retention period and remains in effect until removed. Legal holds can be freely placed and removed by any user who has the S3:PutObjectLegalHold permission. Glacier Vault LockYou can easily deploy and enforce compliance controls for individual S3 glacier vaults with a Vault Lock policy. You can specify controls, such as WORM, in a Vault Lock policy and lock the policy from future edits. Once locked, the policy can no longer be changed. PerformanceYou can a achieve a high number of requests: 3500 PUT/COPY/POST/DELETE and 5500 GET/HEAD requests per second per prefix. You can get better performance by spreading your reads across different prefixes. The more prefixed we have, the better performance we can achieve. S3 LIMITATION WHEN USING KMS Using SSE-KMS to encrypt your objects in S3, you must keep in mind the KMS limits. When you upload a file ,you will call GenerateDataKey in the KMS API. When you download a file, you will call Decrypt in the KMS API. Uploading/downloading will count toward the KMS quota. Region-specific, however, it;s either 5,500, 10,000 or 30,000 requests per second. Currently, you cannot requests a quota increase for KMS. Multi Uploads Recommended for files over 100 MB Required for files over 5 GB Parallelize uploads (increases efficiency) Downloads (S3 Byte-Range Fetches) Parallelize downloads by specifying byte ranges. If there’s a failure in the download, it’s only for a specific byte range. Can be used yo just download partial amounts of the file (e.g., header information). S3 Select &amp; Glacier Select S3 Select enables applications to retrieve only a subset of data from an object by using simple SQL expressions. Could achieve drastic performance increase(Up to 400% Faster and 80% cheaper). Get data by rows or columns using simple SQL expressions. AWS Organizations &amp; Consolidated Billing AWS Organizations is an account management service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage. Consolidated Billing: The more you use S3 across the entire organization the less that you pay. One bill per AWS account Very easy to track charges and allocate costs Volume pricing discount S3 Cross Account Access3 ways to share S3 buckets across accounts Using Bucket Policies &amp; IAM (applies across the entire bucket). Programmatic Access Only Using Bucket ACLs &amp; IAM (individual objects). Also Programmatic Access Only. Cross-account IAM Roles. Programmatic and Console access. Cross Region Replication Versioning must be enabled on both the source and destination. Files in an existing bucket are not replicated automatically. All subsequent updated files will be replicated automatically. Delete markers are not replicated. Deleting individual versions or delete markers will not be replicated. Understand what Cross Region Replication is at a high level. S3 Transfer Acceleration S3 Transfer Acceleration utilize the CloudFront Edge Network to accelerate your uploads to S3. Instead of uploading directly to your S3 bucket, you can use a distinct URL to upload directly to an edge location which will then transfer that file to S3. You will get a distinct URL to upload to acloudguru.s3-accelerate.amazonaws.com AWS DataSync DataSync automatically encrypts data and accelerates transfer over the WAN. DataSync performs automatic data integrity checks in-transit and at-rest. DataSync Agent is deployed as an agent on a server and connects to your NAS or file system to copy data to AWS and write data from AWS. DataSync seamlessly and securely connects to Amazon S3, Amazon EFS, or Amazon FSx for Windows File Server to copy data and meta-data to and from AWS. CloudFront A content delivery network(CDN) is a system of distributed servers(network) that deliver web pages and other web content to a user based on the geographic locations of the user, the origin of the web page, and a content delivery server. Key Terminology Edge Location This is the location where content will be cached. This is separate to an AWS Region/AZ. Edge locations are not just READ only – you can write to them too. Origin: This is the origin of all the files that the CDN will distribute. This can be an S3 bucket, an EC2 Instance, an Elastic Load Balancer, or Route53. Distribution: This is the name given the CDN which consists of a collection of Edge Locations. Web Distribution: Typically used for Web sites. RTMP: Used for Media Streaming. CloudFront Signed URL’s and CookiesUse signed URLs/cookies when you want to secure content so that only the people you authorize are able to access it. ULRs vs. Cookies 1 file = 1 URL: A signed URL is for individual files. 1 cookie = multiple files: A signed cookie is for multiple files. When we create a signed URL or signed cookie, we attach a policy. The policy can include: URL expiration IP ranges Trusted signers (which AWS accounts can create signed URLs) CloudFront Signed URL Can have different origins. Does not have to be EC2. Key-pair is account wide and managed by the root user Can utilize caching features Can filter by date, path, IP address, expiration, etc. S3 Signed URL Issues a request as the IAM user who creates the pre-signed URL Limited lifetime SnowballIt’s a big big disk. Snowball is a petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of AWS. Snowball comes in either a 50 TB or 80 TB size. AWS Snowball Edge is a 100 TB data transfer device with on-board storage and compute capacities. AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. Storage Gateway AWS Storage Gateway is a service that connects an on-premises software appliance with cloud-based storage to provide seamless and secure integration between an organization’s on-promises IT environment and AWS’s Storage infrastructure. AWS Storage Gateway’s software appliance is available for download as a virtual machine(VM) image that you install on a host in your datacenter. Three different types of Storage Gateway The volume interface presents your applications with disk volumes using the iSCSI block protocol. File Gateway (NFS &amp; SMB) Files are stored as objects in your S3 buckets, accessed through a Network File System (NFS) mount point. Volume Gateway (iSCSI) It is basically is a way of storing your virtual hard disk drives in S3, and it looks like EBS snapshots. Stored Volumes: let you store your primary data locally, while asynchronously backing up that data to AWS. Cached Volumes: let you use Amazon Simple Storage Service(S3) as your primary data storage while retaining frequently accessed data locally in your storage gateway. Tape Gateway (Gateway Virtual Tape Library) Tape Gateway offers a durable, cost-effective solution to archive your data in the AWS Cloud. Used for backup and uses popular backup applications like NetBackup. Backup Exec Veeam etc. Athena vs MacieAthena Interactive query service which enables you to analyze and query data located in S3 using standard SQL. Serverless, nothing to provision, pay per query / per TB scanned. No need to set up complex Extract/Transform/Load (ETL) processes. Works directly with data stored in S3. Can be used to query log files stored in S3 Generate business reports on data stored in S3 Analyze AWS cost and usage reports Run queries on click-stream data Macie Security service which uses Machine Learning and NLP (Natural Language Processing) to discover, classify and protect sensitive data stored in S3. PII (Personally Identifiable Information) Personal data used to establish an individual’s identity. This data could be exploited by criminals, used in identity theft and financial fraud Home address, email address, SSN Passport number, driver’s license number D.O.B, phone number, bank account, credit card number. Macie Used AI to recognize if your S3 objects contain sensitive data such as PII Dashboards, reporting and alerts Works directly with data stored in S3 Can also analyze CloudTrail logs Great for PCI-DSS and preventing ID theft. Exam Tips Every tips in “What is S3” part Not suitable to install an operating system or host a database on (box base storage needed, not object based). You can turn on MFA Delete to protect the data. Read after Write consistency for PUTS of NEW Objects. Eventual consistency for overwrite PUTS and DELETES(can take some time to propagate). Control access to buckets using either a bucket ACL or using Bucket Polices. ACL (Access Control List) allow you to set fine grained permissions all the way down to individual objects. Bucket policies (use JSON-based language) are applied to the entire bucket. Stores all versions of an objects(including all writes and even if you delete an object) Versioning cannot be disabled, only suspended once enabled Life-cycle Policies: Automates moving your objects between the different storage tiers. Can be used in conjunction with versioning. Can be applied to current versions and previous versions. Object Lock &amp; Glacier Vault Lock Use S3 Object to store objects using a write once, read many (WORM) model. Object locks can be on individual objects or applied across the bucket as a whole. Object locks come in two modes: governance mode and compliance mode. S3 Glacier Vault Lock: you can specify controls such as WORM in a Vault Lock policy and lock the policy from future edits. Performance prefixed simply is the pathway between you bucket name and filenames mybucketname/folder1/subfolder1/myfile.jpg -&gt; /folder1/subfolder1 3500 PUT/COPY/POST/DELETE and 5500 GET/HEAD requests per second per prefix. You can get better performance by spreading your reads across different prefixes. If you are using SSE-KMS to encrypt your objects in S3, you must keep in mind the KMS limits. multipart uploads &amp; S3 byte-range fetches Practices with AWS Organizations Always enable multi-factor authentication on root account. Always use a strong and complex password on root account. Paying account should be used for billing purposes only. Do not deploy resources into the paying account. Enable/Disable AWS services using Service Control Policies (SCP) either on OU or on individual accounts. 3 Different ways to share S3 Cross Region Replication AWS DataSync Used to move large amounts of data from on-promises to AWS. Used with NFS- and **SMB-**compatible file systems. Replication can be done hourly, daily, or weekly. Install the DataSync agent to start the replication. Can be used to replicate EFS to EFS. CloudFront Edge locations are not just READ only, you can write to them too.(i.e. put an object on to them.) Objects are cached for the life of the TTL (Time to Live.) You can clear cached objects, but you will be charges. That’s a really important exam topic that you can invalidate cache contents. CloudFront Signed URL’s and Cookies If your origin is EC2, then use CloudFront. If your origin is going to be S3, and you’ve only got a single file in there, then you want to use a S3 signed URL instead of a CloudFront signed URL. Think about whether or not your users can actually access S3 if they’re using OAI through CloudFront. If they can’t, you’d be using a CloudFront signed URL. If they can access the S3 bucket directly and it’s just an individual object, then you probably want an S3 signed URL. Storage Gateway File Gateway - For flat files, stored directly on S3. Volume Gateway Stored Volumes - Entire Dataset is stored on site and is asynchronously backed up to S3. Cached Volumes - Entire Dataset is stored on S3 and the most frequently accessed data is cached on site. Gateway Virtual Tape Library Athena Remember what Athena is and what it allows you to do: Athena is an interactive query service It allows you to query data located in S3 using standard SQL Serverless Commonly used to analyze log data stored in S3. Macie Remember what Macie is and what it allows you to do: Macie uses AI to analyze data in S3 and helps identify PII Can also be used to analyze CouldTrail logs for suspicious API activity Includes Dashboards, Reports and Alerting Great for PCI-DSS compliance and preventing ID theft. Summary - IAM IAM is universal. It does not apply to regions at this time. The ‘root account’ is simply the account created when first setup your AWS account. It has complete Admin access. New Users have NO permissions when first created. New users are assigned Access Key ID &amp; Secret Access Keys when first created. These are not the same as a password. You cannot use the Access key ID &amp; Secret Access Key to Login in to the console. You can use this to access AWS via the APIs and Command Line, however. You only get to view these once. If you lose them, you have to regenerate them. So, save them in a secure location. Always setup Multi-factor Authentication on tour root account. You can create and customize your own password rotation policies. Summary - S3 Remember that S3 is Object-based (allow you to upload files). Files can be from 0 to 5 TB. There is unlimited storage. Files are stored in Buckets. S3 is a universal name-space. By default, all newly created buckets are private. you can setup access control to your buckets using: Bucket Policies and Access Control Lists. The Key Fundamentals of S3 Key Value Version ID Meta-data Sub-resources Access Control Lists Read after Write consistency for PUTS of new Objects Eventually Consistency for overwrite PUTS and DELETED (can take some time to propagate) Understand how to get the best value out of S3 S3 Standard S3 - IA S3 One Zone - IA S3 - Intelligent Tiering S3 Glacier S3 Glacier Deep Archive Encryption in Transit is achieved by SSL / TLS Encryption At Rest (server side) is achieved by S3 Managed Keys - SSE - S3 AWS Key Management Service, Managed Keys - SSE - KMS Server Side Encryption With Customer Provided Keys - SSE - C Client Side Encryption Object Lock Use S3 Object Lock to store objects using a write once, read many (WORM) model. Object locks can be on individual objects or applied across the bucket as a whole. Object locks come in two modes: governance mode and compliance mode. With governance mode, users can’t overwrite or delete an object version or alter its lock setting unless they have special permissions. With compliance mode, a protected object version can’t be overwritten or deleted by any user, including the root user in your AWS account. S3 Glacier Vault Lock allows you to easily deploy and enforce compliance controls for individual S3 Glacier vaults with a Vault Lock policy. You can specify controls such as WORM in a Vault Lock policy and lock the policy from future edits. Once locked, the policy can no longer be changed. You can get better performance by spreading your reads across different prefixes. For example, if you are using two prefixes, you can achieve 11,000 requests per second. If you are using SSE-KMS to encrypt your objects in S3, you must keep in mind the KMS limits. Uploading/downloading will count toward the KMS quota. Multipart Uploads Use multipart uploads to increase performance when uploading files to S3. Should be used for any files over 100 MB and must be used for any file over 5 GB. Use S3 byte-range fetches to increase performance when downloading files to S3. S3 Select Remember that S3 Select is used to retrieve only a subset of data from an object by using simple SQL expressions. Get data by rows or columns using simple SQL expressions. Save money on data transfer and increase speed. REMEMBER TO READ FAQhttps://aws.amazon.com/s3/faqs/ https://aws.amazon.com/iam/faqs/","link":"/Blog/2020/08/13/AWS-Solution-Architect-Associate-1-Identity-Access-Management-and-S3/"},{"title":"AWS Solution Architect(Associate) - Topic 2: Elastic Compute Cloud (Amazon EC2)","text":"Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers. Amazon EC2’s simple web service interface allows you to obtain and configure capacity with minimal friction. It provides you with complete control of your computing resources and lets you run on Amazon’s proven computing environment. [toc] Elastic Compute CloudAmazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. Amazon EC2 reduces the time required to obtain and boot new server instances to minutes, allowing you to quickly scale capacity, both up and down, as your computing requirements change. EC2 Pricing ModelsOn Demand Allows you to pay a fixed rate by the hours (or by the second) with no commitment. Users want the low cost and flexibility of EC2 without any up-front payment or long-term commitment. Application with short-term, spiky, or unpredictable workloads that cannot be interrupted Applications being developed or tested on Amazon EC2 for the first time. Reserved Provides you with a capacity reservation, and offer a significant discount on the hourly charge for an instance. Applications with steady state or predicable usage Applications that require reserved capacity Users able to make up-front payments to reduce their total computing costs even further. Reserved Pricing Types Standard Reserved Instances: These offer up to 75% off on demand instances. Convertible Reserved Instances: It allows you to change between the different instance types. These offer up to 54% off on demand capacity to change the attributes of the RI as long as the exchange results in the creation of Reserved Instances of equal or greater value. Scheduled Reserved Instances: These are available to launch within the time windows you reserve. Spot Enables you to bid whatever price you want for instance capacity, providing for even greater savings if your applications have flexible start and end times. Applications that have flexible start and end times. Applications that are only feasible at very low compute prices. Users with urgent computing needs for large amounts of additional capacity. Dedicated Hosts An Amazon EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated to your use. Dedicated Hosts can help you reduce costs by allowing you to use your existing server-bound software licenses. Useful for regulatory requirements that may not support multi-tenant virtualization.(It might be the government says that you cannot support multi tenant virtualization) Grateful for licensing which does not support multi-tenancy or cloud deployment.(If you’ve got some really harsh Oracle licensing) Can be purchased On-Demand(hourly). Can be purchased as a Reservation for up to 70% off the On-demand price. Security Groups All Inbound traffic is blocked by default. All Outbound traffic is allowed. Changes to Security Groups take effect immediately. You can have any number of EC2 instances within a security group. You can have multiple security groups attached to EC2 Instances. Security Groups are STATEFUL. We don’t have to change inbound and outbound ports if you enable something on the inbound, outbound is enabled automatically for that port. You cannot block specific IP addresses using Security Groups, instead use Network Access Control Lists(VPC Section). You can specify allow rules, but not deny rules. Because by default, you deny everything. EBSAmazon Elastic Block Store (EBS) provides persistent block storage volumes for use with Amazon EC2 instances in the AWS Cloud. Each Amazon EBS volume is automatically replicated within its Availability Zone to protect you from component failure, offering high availability and durability. 5 Different Types of EBS Storage Volume Type Description Use Case API Name Volume size Max IOPS Volume General Purpose (SSD) balances price and performance for a wide variety of transaction workloads Most Work Loads gp2 1 GiB - 16 TiB 16000 Provisioned IOPS (SSD) Highest performance designed for mission-critical applications Databases io1 4 GiB - 16 TiB 64000 Throughput Optimized Hard Disk Drive(HDD) Low cost HDD for frequently accessed, throughput-intensive workloads. Big Data &amp; Data Warehouses st1 500 GiB - 16 TiB 500 Cold Hard Disk Drive(HDD) Lowest cost HDD for less frequently accessed workload File Servers sc1 500 GiB - 16 TiB 250 EBS Magnetic Precious generation HDD Workloads where data is infrequently accessed Standard 1 GiB - 1 TiB 40-200 Termination protection is turned off by default, you must turn it on. On an EBS-backed instance, the default action is for the root EBS volume to be deleted when the instance is terminated. EBS Root Volumes of your DEFAULT AMI’s CAN be encrypted. You can use a third party tool (such as bit locker etc) to encrypt the root volume. or this can be done when creating AMI’s in the AWS console or using the API. Additional volumes can be encrypted. As of Feb 2020 you can attach certain types of EBS volumes to multiple EC2 instances. EBS Volume &amp; Snapshot It’s really important to remember that your EBS volumes will always be in the same Availability Zones as your EC2 instance. Volumes exist on EBS. Think of EBS as a virtual hard disk. Snapshots exist on S3. Think of snapshots as a photograph of the disk. Snapshots are point in time copies of Volumes. Snapshots are incremental – this means that only the blocks that have changed since your last snapshot are moved to S3. If this is your first snapshot, it may take some time to create. To create a snapshot for Amazon EBS volumes that serve as root devices, you should stop the instance before taking the snapshot. However you can take a snap while the instance is running. You can create AMI’s from Snapshots. You can change EBS volume sizes on the fly, including changing the size and storage type. Migrating EBS To move an EC2 volume from one AZ to another, take a snapshot of it, create an AMI from the snapshot and then use the AMI to launch the EC2 instance in a new AZ. To move an EC2 volume from one region to another, take a snapshot of it, create an AMI from the snapshot and the copy the AMI from one region to the other. Then use the copied AMI to launch the new EC2 instance in the new region. AMI Type (EBS vs Instance Store)You can select your AMI based on: Region (see Regions and Available Zones) Operation system Architecture (32-bit or 64-bit) Launch permissions Storage for the Root Device (Root Device Volume) Instance Store (EPHEMERAL STORAGE) EBS Backed Volumes All AMIs are categorized as either backed by Amazon EBS or backed by instance store. For EBS Volumes: The root device for an instance launched from the AMI is an Amazon EBS volume created from an Amazon EBS snapshot. For Instance Store Volumes: The root device for an instance launched from the AMI is an instance store volume created from a template stored in Amazon S3. Exam Tips Instance Store Volumes are sometimes called Ephemeral Storage.(For some reason they’re stopped, you are going to lose all of your data instant.) Instance store volumes cannot be stopped. If the underlying host fails, you will lose your data. EBS backed instances can be stopped. You will not lose the data on this instance if it is stopped. You can reboot both, you will not lose your data. By default, both ROOT volumes will be deleted on termination. However, with EBS volumes, you can tell AWS to keep the root device volume. ENI vs ENA vs EFATerminology ENI For basic networking. Elastic Network Interface - essentially a virtual network card. An ENI is simply a virtual network card for your EC2 instances. EN For when you need speeds between 10 Gbps and 100 Gbps. Anywhere you need reliable, high throughput. Enhanced Networking. Uses single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities on supported instance types. Enhanced networking provides higher bandwidth, higher packet per second (PPS) performance, and consistently lower inter-instance latencies. There is no additional charge for using enhanced networking. Use where you want good network performance. Depending on your instance type, enhanced networking can be enabled using: Elastic Network Adapter(ENA), which supports network speeds of up to 100 Gbps for supported instance types. Intel 82599 Virtual Function (VF) interface, which supports network speeds of up to 10 Gbps for supported instance types. This is typically used on older instances. In any scenario question, you probably want to choose ENA over VF if given the option. EFA Fort when you need to accelerate High Performance Computing (HPC) and machine learning applications or if you need to do an OS-bypass. Elastic Fabric Adapter (EFA) is a network device that you can attach to your Amazon EC2 instance to accelerate High Performance Computing (HPC) and machine learning applications. EFA provided lower and more consistent latency and higher throughput than the TCP transport traditionally used in cloud-based HPC systems. EFA can use OS-bypass. OS-bypass enables HPC and machine learning applications to bypass the operating system kernel and to communicate directly with the EFA device. It makes it a lot faster with a lot lower latency. Not supported with Windows currently, only Linux. Encrypted Root Device Volumes &amp; SnapshotsEBS Encryption snapshots of encrypted volumes are encrypted automatically. Volumes restored from encrypted snapshots are encrypted automatically. You can share snapshots, but only if they are unencrypted. These snapshots can be shared with other AWS accounts or made public. You can now encrypt root device volumes upon creation of the EC2 instance. The process for making it encrypted: Create a Snapshot of the unencrypted root device volume Create a copy of the Snapshot and select the encrypt option Create an AMI from the encrypted Snapshot Use that AMI to launch new encrypted instance Spot Instances &amp; Spot FleetsAmazon EC2 Spot Instances let you take advantage of unused EC2 capacity in the AWS Cloud. You may also use a Spot block to stop your Spot Instances from being terminated even if the Spot price goes over your max Spot price. Spot Fleets A spot Fleet is a collection of Spot Instances and, optionally, On-Demand Instances. The Spot Fleet attempts to launch the number of Spot Instances and On-Demand Instances to meet the target capacity you specified in the Spot Fleet request. The request for Spot Instances is fulfilled if there is available capacity and the maximum price you specified in the request exceeds the current Spot price. The spot Fleet also attempts to maintain its target capacity fleet if your Spot Instances are interrupted. Exam Tips Spot Instances save up to 90% of the cost On-Demand Instances. Useful for any type of computing where you don’t need persistent storage. You can block Spot Instances from terminating by using Spot block. A Spot Fleet is a collection of Spot Instances and optionally, On-Demand Instances. EBS Behaviors ReviewedIf we stop the instance, the data is kept on the disk (with EBS) and will remain on the disk until the EC2 instance is started. If the instance is terminated, then by default the root device volume will also be terminated. EC2 Hibernate ReviewedWhen you hibernate an EC2 instance, the operating system is told to perform hibernation (suspend-to-disk). Hibernation saves the contents from the instance memory (RAM) to your Amazon EBS root volume. We persist the instance’s Amazon EBS root volume and any attached Amazon EBS data volumes. When you start your instance out of hibernation: The Amazon EBS root volume is restored to its previous sate The RAM contents are reloaded The processes that were preciously running on the instance are resumed Previously attached data volumes are reattached and the instance retains its instance ID. This is useful for: Long-running processes Services that take time to initialize Exam Tips: EC2 Hibernate preserves the in-memory RAM on persistent storage(EBS) Much faster to boot up because you do not need to reload the operating system. Instance RAM must be less than 150 GB. Available for Windows, Amazon Linux 2 AMI, and Ubuntu. Instances can’t be hibernated for more than 60 days. Cloud WatchAmazon CloudWatch is a monitoring service to monitor your AWS resources, as well as the applications that you run on AWS. CloudWatch can monitor things like: Compute Storage &amp; Content Delivery Host Level Metrics Consist of: CPU Network Disk Status Check What Can I do with CloudWatch? Dashboards - Creates awesome dashboards to see what is happening with your AWS environments. Alarms - Allows you to set Alarms that notify you when particular thresholds are hit. Events - CloudWatch Events helps you to respond to state changes in your AWS resources. Logs - CloudWatch Logs helps you to aggregate, monitor, and store logs. AWS CloudTrail AWS CloudTrail increases visibility into your user and resource activity by recording AWS Management Console actions and API calls. You can identity which users and accounts called AWS, the source IP address from which the calls were made, and when the calls occurred. CloudTrail vs CloudWatch CloudWatch monitors performance. CloudTrail monitors API calls in the AWS platform Things need to be remembered: CloudWatch is used for monitoring performance. CloudWatch can monitor most of AWS as well as your applications that run on AWS. CloudWatch with EC2 will monitor events every 5 minutes by default. You can have 1 minute intervals by turning on detailed monitoring. You can create CloudWatch alarms which trigger notifications. CloudWatch is all about performance. CloudTrail is all about auditing. Exam Tips Standard Monitoring = 5 Minutes Detailed Monitoring = 1 Minutes AWS Command Line You can interact with AWS from anywhere in the world just by using the command line (CLI). You will need to set up access in IAM. Commands themselves are not in the exam, but some basic commands will be useful to know for real life. Role: Identity Access Management Roles Roles are far more secure than storing your access key and secret access key on individual EC2 instances. Roles are easier to manage. Roles can be assigned to an EC2 instance after it is created using both the console &amp; command line. Roles are universal – you can use them in any region. BootStrap ScripsBootstrap Scripts are super powerful as you can see it’s a way of automating your infrastructure. Bootstrap scripts run when an EC2 instance first boots. Instance Meta data Used to get information about an instance (such as public IP) 1curl http://196.254.169.254/latest/meta-data/ User data simply contain the bootstrap script that you run 1curl http://196.254.169.254/latest/user-data/ EFS (Amazon Elastic File System)Amazon Elastic File System(Amazon EFS) is as file storage service for Amazon Elastic Compute Cloud (EC2) instance. Amazon EFS is easy to use and provides a simple interface that allows you to create and configure file systems quickly and easily. With Amazon EFS, storage capacity is elastic, growing and shrinking automatically as you add and remove files, so you applications have the storage they need, when they need it. Exam Tips Supports the Network File System version 4 (NFSv4) protocol. You only pay for the storage you use (no pre-provisioning required) Can scale up to the petabytes Can support thousands of concurrent NFS connections Data is stored across multiple AZ’s within a region Read After Write Consistency FSX for Windows &amp; FSX for LustreAmazon FSx for Windows Amazon FSx for Windows File Server provides a fully managed native Microsoft Windows file system so you can easily move your Windows-based applications that require file storage to AWS. Amazon FSx is built on Windows Server. FSX for Lustre Amazon FSx for Lustre is a fully managed file system that is optimized for compute-intensive workloads, such as high-performance computing, machine learning, media data processing, and electronic design automation (EDA). With Amazon FSx, you can launch and run a Lustre file system that can process massive data sets at up to hundreds of gigabytes per second of throughput, millions of IOPS, and sub-millisecond latencies. How is FSx (Windows &amp; Lustre) Different to EFS Windows FSx Lustre FSx EFS A managed Windows Server A managed file system A managed NAS filter for EC2 instances Running Windows Server Message Block (SMB)-based file services Based on Network File System (NFS) version 4 Designed for Windows and Windows applications Designed specifically for fast processing of workloads One of the first network file sharing protocols native to Unix and Linux When you need centralized Storage for Windows-based applications When you need high-speed, high-capacity distributed storage. When you need distributed, highly resilient storage for Linux-based applications EC2 Placement GroupsClustered Placement Group Grouping of instances within a single Availability Zone. Recommended for applications that need low network latency, high network throughput, or both. Only certain instances can be launched in to a Clustered Placement Group (Compute Optimized, GPU, Memory Optimized, Storage Optimized). AWS recommend homogeneous instances within clustered placement groups. Spread placement Group Group of instances that are each placed on distinct underlying hardware. Recommended for applications that have a small number of critical instances that should be kept separate from each other. Spread placement groups have a specific limitation that you can only have a maximum of 7 running instances per Availability Zone. Partitioned Amazon EC2 divides each group into logical segments called partitions. Amazon EC2 ensures that each partition within a placement group have its own set of racks. Uses cases are multiple EC2 instances HDFS, Hbase, and Cassandra. HDFS: The Hadoop Distributed File System ( HDFS ) is a distributed file system designed to run on commodity hardware. Hbase: HBase is an open-source non-relational distributed database modeled after Google’s Bigtable and written in Java. It is developed as part of Apache Software Foundation‘s Apache Hadoop project and runs on top of HDFS (Hadoop Distributed File System) or Alluxio, providing Bigtable-like capabilities for Hadoop. Cassandra: Apache Cassandra™ is a distributed NoSQL database that delivers continuous availability, high performance, and linear scalability that successful applications require. Other Exam Tips The name you specify for a placement group must be unique within your AWS account. You can’t merge placement groups. You can’t move an existing instance into a placement group. Before you move the instance, the instance must be in the stopped state. You can move or remove an instance using the AWS CLI or an AWS SDK (software development kit), you can’t do it via the console yet. HPC on AWSYou can create a large number of resources in almost no time. You only pay for the resources you use – and, once finished, you can destroy the resources. Data Transfer Snowball, snowmobile (terabytes/petabytes worth of data) AWS DataSync Direct Connect (a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS) Compute EC2 EC2 fleets (Spot Instances or Spot Fleets) Placement groups (cluster placement groups) Networking Enhanced Networking (Uses single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities on supported instance types. It provides higher I/O performance and lower CPU utilization) Elastic Network Adapter **Elastic Fabric Adapters **(Network device you can attach to your Amazon EC2 to accelerate HPC and machine learning applications.) Storage Instance-attached storage EBS(Amazon Elastic Block Store provides persistent block storage volumes for use with Amazon EC2 instances in the AWS Cloud.) Instance Store Network Storage Amazon S3 Amazon EFS (Amazon Elastic File System is as file storage service for Amazon Elastic Compute Cloud (EC2) instance) Amazon FSx for Lustre (Amazon FSx for Lustre is a fully managed file system that is optimized for compute-intensive workloads) Orchestration &amp; Automation AWS Batch AWS Batch enables developers, scientist, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. AWS Batch supports multi-node parallel jobs, which allows you to run a single job that spans multiple EC2 instance. You can easily schedule jobs and launch EC2 instances according to your needs. AWS ParallelCluster Open-source cluster management tool that makes it easy for you to deploy and manage HPC clusters on AWS. ParallelCluster uses a simple text file to model and provision all the resources needed for your HPC applications in an automated and secure manner. Automate creation of VPC, subnet, cluster type, and instance types. AWS WAF (Web Application Firewall)AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to Amazon CloudFront, an Application Load Balancer or API Gateway. AWS WAF also lets you control access to your content. You can configure conditions such as what IP addresses are allowed to make this request or what query string parameters need to be passed for the request to be allowed. Then the application load balancer or CloudFront or API Gateway will either allow this content to be received or to give a HTTP 403 Status Code. How to block malicious IP addresses: Use AWS WAF Use Network ACLs Other Exam TipsGet Hands Dirty Termination Protection is turned off by default, you must turn it on. On an EBS-backed instance, the default action is for the root EBS volume to be deleted when the instance is terminated. But any additional volumes by default won’t be deleted. EBS Root Volumes of your DEFAULT AMI’s CAN be encrypted. You can also use a third party tool (such as bit locker etc) to encrypt the root volume, or this can be done when creating AMI’s in the AWS console or using the API. Additional volumes can be encrypted. REMEMBER TO READ FAQhttps://aws.amazon.com/ec2/faqs/","link":"/Blog/2020/10/15/AWS-Solution-Architect-Associate-2-Elastic-Compute-Cloud/"},{"title":"AWS Solution Architect(Associate) - Topic 4: Advanced IAM","text":"AWS Identity and Access Management (IAM) enables you to manage access to AWS services and resources securely. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources. [toc] Advanced IAMAWS Directory Service AWS Directory Service for Microsoft Active Directory, also known as AWS Managed Microsoft Active Directory (AD), enables your directory-aware workloads and AWS resources to use managed Active Directory (AD) in AWS. https://aws.amazon.com/rds/faqs/ AWS Customer Multi-AZ Deployment Users, Groups, GPOs Patch, Monitor, Recover Standard AD Tools Instance Rotation Scale out DCs Snapshot and Restore Trusts (Resource Forest) Certificate Authorities Federation User Case Provide your on-premises AD users quick access to AWS Leverage integrations with Amazon RDS and Amazon FSx Enable single sign-on experience for AWS End User Computing services Give your on-premises AD users federated access to the AWS Management Console and AWS CLI quickly Grant your on-premises AD users single-click access to cloud business applications AD Compatible Not AD Compatible Managed Microsoft AD Cloud Directory AD Connector Cognito User Pools Simple AD Simple AD Standalone managed directory Basic AD features Small: &lt;= 500; Large &lt;= 5000 users Easier to manage EC2 Linux workloads that need LDAP Does not support trusts (can’t join on-promises AD) AD Connector Directory gateway (proxy) for on-premises AD Avoid caching information in the cloud Allow on-premises users to log in to AWS using AD Join EC2 instances to your existing AD domain Scale across multiple AD Connectors Cloud Directory Directory-based store for developers Multiple hierarchies with hundreds of millions of objects Use cases: org charts, course catalogs, device registries Full managed service Amazon Cognito User Pools Managed user directory for SaaS application Sign-ip and sign-in for web or mobile Works with social media identities IAM PoliciesAmazon Resource Name (ARN) begin with: arn:partition:service:region:account_id IAM Policies Not explicitly allowed == implicity denied JSON document that defines permissions Identity Policy &amp; Resource Policy No effect until attached List of Statements (Effect/Action/Resource) Permission Boundaries Used to delegate administration to other users Prevent privilege escalation or unnecessarily broad permissions Control maximum permissions an IAM policy can grant Use Cases Developers creating roles for Lambda functions Application owners creating roles for EC2 instances Administrator creating ad hoc users AWS Resource Access Manager (RAM)RAM eliminates the need to create duplicate resources in multiple accounts, reducing the operational overhead of managing those resources in every single account you own. Here is a detailed blog using Resource Access Manager to achieve Cross-Account Resource Sharing You can create resources centrally in a multi-account environment, and use RAM to share those resources across accounts in three simple steps: create a Resource Share, specify resources, and specify accounts. RAM is available to you at no additional charge. AWS Single Sign-OnCentrally manage access to multiple AWS accounts and business applications and provide users with single sign-on access to all their assigned accounts and applications from one place.","link":"/Blog/2021/01/08/AWS-Solution-Architect-Associate-4-Advanced-IAM/"},{"title":"AWS Solution Architect(Associate) - Topic 3: Database on AWS","text":"Choose from 15 purpose-built database engines including relational, key-value, document, in-memory, graph, time series, and ledger databases. With AWS databases, you don’t need to worry about database management tasks such as server provisioning, patching, setup, configuration, backups, or recovery. [toc] Database on AWSOverviewDatabase Types RDS (OLTP): SQL Server; Oracle; MySQL Server; PostgreSQL; Aurora: MariaDB. RDS has two key feature: Multi-AZ - For Disaster Recovery; Read Replicas - For performance. DynamoDB (NoSQL) Red Shift OLAP OLTP vs OLAP Online Transaction Processing (OLTP) differs from Online Analytics Processing (OLAP) in terms of the types of queries you will run. **Redshift for Data Warehousing ** Used for business intelligence. Tools like Congnos, Jaspersoft, SQL Server Reporting Services, Oracle Hyperion, SAP NetWeaver. Used to pull in very large and complex data sets. Usually used by management to do queries on data (such as current performance vs targets etc) Data Warehousing databases use different type of architecture both from a database perspective and infrastructure layer. Amazon’s Data Warehouse Solution is Called Redshift. ElastiCache ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory cache in the cloud. Used to speed up performance of existing databases (frequent identical queries). ElastiCache supports two open-source in-memory caching engines: Memcached Redis Remember the following points: RDS runs on virtual machines You cannot log into these operating systems however Patching of the RDS Operating System and DB is Amazon’s responsibility RDS is NOT Serverless Aurora Serverless IS Serverless RDS - Back Ups, Multi-AZ &amp; Read ReplicasBack UpsAutomated Backups Automated Backups allow you to recover your database to any point in time within a “retention period”. The retention period can be between one and 35 days. Automated Backups will take a full daily snapshot and will also store transaction logs throughout the day. When you do a recovery, AWS will first choose the most recent daily back up, and then apply transaction logs relevant to that day. This allows you to do a point in time recovery down to a second, within the retention period. Enabled by default. The backup data is stored in S3 and you get free storage space equal to the size of your database. Backups are taken within a defined window. During the backup window, storage I/O may be suspended while your data is being backed up. Database Snapshots DB Snapshots are done manually. They are stored even after you delete the origin RDS instance, unlike automated bakeups. Restoring Backups Whenever you restore wither an Automatic Backup or a manual Snapshot, the restored version of the database will be a new RDS instance with a new DNS (Domain Name System) endpoint. Encryption At Rest Encryption is done using the AWS KMS (Key Management Service). Once your RDS instance is encrypted, the data stored at rest in the underlying storage is encrypted, as are its automated backups, read replicas, and snapshot. Encryption is available for all six of the engines (MySQL, etc.) Multi-AZ Multi-AZ allows you to have an exact copy of your production database in another Availability Zone. AWS handles the replication for you, so when you production database is written to, this write will automatically be synchronized to the stand by database. In the event of planned database maintenance, DB Instance failure, or an Availability Zone failure, Amazon RDS will automatically fail-over to the standby so that database operations can resume quickly without administrative intervention. Aurora is not involved in Multi-AZ, because Aurora by its own architecture is completely fault tolerant. Used for DR (Disaster Recovery) Read Replica Read replicas allow you to have a read-only copy of your production database. This is achieved by using Asynchronous replication from the primary RDS instance to the read replica. You use read replicas primarily for very read-heavy database workloads. SQL Server is not available for the read replicas. Used for scaling, not for DR (Disaster Recovery)! Must have automatic backups turned on Each read replica will have its own DNS end point You can have read replicas that have Multi-AZ You can create read replicas of Multi-AZ source databases Read replicas can be promoted to be their own databases. This breaks the replications. So if you do promote a read replica to be its own independent database the replication will no longer work. You can have a read replica in a second region. DynamoDBFYI: I highly recommend you to watch it, it is mind-blowing. Basic DynamoDBAmazon DynamoDB is a fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale. It is a fully managed database and supports both document and key-value data models. Its flexible data model and reliable performance make it a great fit for mobile, web, gaming, ad-tech, IoT, and many other applications. The Basics of DynamoDB Stored on SSD storage Spread across 3 geographically distinct data centers Eventual Consistent Reads (Default): Consistency across all copies of data is usually reached within a second. Strongly Consistent Reads: Return a result that reflects all writes that received a successful response prior to the read. Tenets of NoSQL DATA MODELING Understand the use case Define the access patterns Read/Write workloads Data-modeling Avoid relational design patterns, use one table 1 application service = 1 table Reduce round trips Simplify access patterns Identify Primary Keys How will items be inserted and read? Overload items into partitions Define indexes for secondary access patterns Advanced DynamoDBDynamoDB Accelerator (DAX) Fully managed, highly available, in-memory cache 10x performance improvement Reduces request time from milliseconds to microseconds – even under load. No need for developers to manage caching logic DAX is completely compatible with DynamoDB API calls Transactions Multiple “all-or-nothing” operations Financial transactions / Fulfilling orders Two underlying reads or writes – prepare/commit Up to 25 items or 4 MB of Data On-Demand Capacity Pay-per-request pricing Balance cost and performance No minimum capacity No charge for read/write – only storage and backups Pay more per request than with provisioned capacity Use for new product launches On-Demand backup and Restore Full backups at any time Zero impact on table performance or available Consistent within seconds and retained until deleted operates within same region as the source table Point-in-Time Recovery (PITR) Protects against accidental writes or deletes Restore to any point in the past 35 days Incremental backups Not enabled by default Latest restorable: five minutes in the past Streams Time-ordered sequence of item-level changes in a table Stored for 24 hours Inserts, updates, and deletes Global Tables Managed Multi-Master, Multi-Region Replication Globally distributed applications Based on DynamoDB streams Multi-region redundancy for DR (Disaster Recovery) or HA (High Availability) No need to rewrite the application, DynamoDB handle it automatically for you Replication latency under one second Database Migration Service (DMS) At a high level, when using AWS DMS you do the following: Create a replication server. Create source and target endpoints that have connection information about your data stores. Create one or more migration tasks to migrate data between the source and target data stores. A task can consist of three major phases: The full load of existing data The application of cached changes Ongoing replication Security Encryption at rest using KMS Site-to-site VPN Direct Connect (DX) IAM policies and roles Fine-grained access: This is where you have an IAM policies that allows users access to only certain attributes within DynamoDB table items. CloudWatch and CloudTrail VPC endpoints: For DynamoDB to enable EC2 instances in your VPC to use their private IP addresses to access DynamoDB with no exposure to the public Internet. RedshiftAmazon Redshift is a fast, fully managed, petabyte-scale data warehouse service that makes it simple and cost-effective to efficiently analyze all your data using your existing business intelligence tools. It is optimized for datasets ranging from a few hundred gigabytes to a petabyte or more and costs less than $1,000 per terabyte per year, a tenth the cost of most traditional data warehousing solutions. Redshift can be configured as follows Single Node (160 GB) Multi-Node Leader Node (manages client connections and receives queries.) Compute Node (store data and perform queries and computations). Up to 128 Compute Nodes. Advanced Compression Amazon Redshift employs multiple compression techniques and can often achieve significant compression relative to traditional data stores. Massively Parallel Processing (MPP) Amazon Redshift automatically distributes data and query load across all nodes. Amazon Redshift makes it easy to add nodes to your data warehouse and enables you to maintain fast query performance as your data warehouse grows. Redshift Backups Enabled by default with a 1 day retention period. Maximum retention period is 35 days. Redshift always attempts to maintain at least three copies of your data. (the original and replica on the compute nodes and a backup in Amazon S3) Redshift can also asynchronously replicate your snapshots to S3 in another region for disaster recovery. Redshift is priced as follow Compute Node Hours. And you will not be charged for leader node hours, only compute nodes will incur charges. Backups Data transfer (only within a VPC, not outside it) Security Considerations Encrypted in transit using SSL Encrypted at rest using AWS-256 encryption By default RedShift takes care of key managements. Manage your own keys through HSM AWS Key Management Service Redshift Availability Currently only available in 1 AZ Can storage snapshots to new AZs in the event of an outage. Exam Tips Redshift is used for business intelligence AuroraAmazon Aurora is a MySQL and PostgreSQL-compatible relational database engine that combines the speed and availability of high-end commercial databases with the simplicity and cost-effectiveness of open source databases. Things to know about Aurora Start with 10 GB, Scales in 10 GB increments to 64 TB (Storage Autoscaling) Compute resources can scale up to 32vCPUs and 244 GB of Memory. 2 copies of your data is contained in each availability zone, with minimum of 3 availability zones. 6 copies of your data. Three Types of Aurora Replicas are available Aurora Replicas (Currently up to 15) MySQL Read Replicas (Currently up to 5) PostgresQL Read Replicas (Currently up to 1) What is Amazon Aurora Serverless Provides a relatively simple. cost-effective option for infrequent, intermittent, or unpredictable workloads. An on-demand, autoscaling configuration for the MySQL-compatible and PostgreSQL-compatible editions of Amazon Aurora. An Aurora Serverless DB cluster automatically starts up, shuts down, and scales capacity up or down based on your application’s needs. Exam Tips 2 copies of your data are contained in each availability zone, with minimum of 3 availability zones. 6 copies of your data. You can share Aurora Snapshots with other AWS accounts 3 types of replicas available. Aurora Replicas, MySQL replicas &amp; PostgresQL replicas. Automated failover is only available with Aurora Replicas. Aurora has automates backups turned on by default. You can also take snapshots with Aurora. You can share these snapshots with other AWS accounts. Use Aurora Serverless if you want a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads. ElastiCacheElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory caches, instead of relying entirely on slower disk-cased databases. ElastiCache supports two open-source in-memory caching engines: Memcached and Redis. Memcached Simple Cache to offload DB Scale horizontally multi-thread performance Redis Advanced data types Ranking/Sorting data sets Pub/Sub capabilities Persistence Multi-AZ Backup &amp; Restore Capabilities Exam Tips Use ElastiCache to increase database and web application performance Redis is Multi-AZ You can do back ups and restores of Redis Database Migration Service (DMS)DMS is a cloud service that makes it easy to migrate relational databases, data warehouses, NoSQL databases, and other types of data stores. You can use AWS DMS to migrate your data into the AWS Cloud, between on-premises instances(through an AWS Cloud Setup), or between combinations of cloud and on-premises setups. Exam Tips DMS allows you to migrate databases from one source to AWS The source can either be on-promises, or inside AWS itself or another cloud provider such as Azure. You can do homogeneous migrations(same DB engines) or heterogeneous migrations. If you do a heterogeneous migration, you will need the AWS Schema Conversion Tool (SCT). Caching ServicesCaching is a balancing act between up-to-date, accurate information and latency. We can use the following services to cache on AWS. CloudFront API Gateway ElastiCache – Memcached and Redis DynamoDB Accelerator (DAX) EMR OverviewAmazon EMR makes it easy to set up, operate, and scale your big data environments by automating time-consuming tasks like provisioning capacity and tuning clusters. Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. With EMR you can run petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. What is EMR? The central component of Amazon EMR is the cluster. A cluster is a collection of Amazon Elastic Compute Cloud (Amazon EC2) instances. Each instance in the cluster is called a node. Each node has a role within the cluster, referred to as the node type. Exam Tips EMR is used for big data processing Consists of a master node, a core node, and (optionally) a task node. By default, log data is stored on the master node You can configure replication to S3 on five-minute intervals for all log data from the master node; however , this can only be configured when creating the cluster for the first time. REMEMBER TO READ FAQhttps://aws.amazon.com/rds/faqs/","link":"/Blog/2020/10/20/AWS-Solution-Architect-Associate-3-Database-on-AWS/"},{"title":"AWS Solution Architect(Associate) - Topic 6: VPCs","text":"Virtual Private Cloud (AWS Amazon) lets you provision a logically isolated section of the Amazon Web Services Cloud where you can launch AWS resources in a virtue network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. VPCsVirtual Private Cloud lets you provision a logically isolated section of the Amazon Web Services Cloud where you can launch AWS resources in a virtue network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. VPC Overview No Translative peering When you create a VPC, a default Route Table, Network Access Control List and a default Security Group is created It won’t create any subnets, nor will it create a default Internet gateway Amazon always reserve 5 IP addresses within your subnets You can only have 1 Internet Gateway per VPC Security Groups cant span VPCs. Amazon Virtual Private Cloud features Amazon Virtual Private Cloud provides features that you can use to increase and monitor the security for your virtual private cloud (VPC): Reachability Analyzer: Reachability Analyzer is a static configuration analysis tool that enables you to analyze and debug network reachability between two resources in your VPC. VPC Flow Logs: You can monitor your VPC flow logs delivered to Amazon S3 or Amazon CloudWatch to gain operational visibility into your network dependencies and traffic patterns, detect anomalies and prevent data leakage, or troubleshoot network connectivity and configuration issues. VPC Traffic Mirroring: VPC traffic mirroring allows you to copy network traffic from an elastic network interface of Amazon EC2 instances and then send the traffic to out-of-band security and monitoring appliances for deep packet inspection. Ingress Routing: This allows you to route all incoming and outgoing traffic flowing to/from an Internet Gateway (IGW) or Virtual Private Gateway (VGW) to a specific EC2 instance’s Elastic Network Interface. With this feature, you can configure your virtual private cloud to send all traffic to an IGW, VGW or EC2 instance before the traffic reaches your business workloads. Security Groups: Security groups act as a firewall for associated Amazon EC2 instances, controlling both inbound and outbound traffic at the instance level. Network Access Control List: A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. Security groups vs Network ACLs Security group Network ACL Operates at the instance level Operates at the subnet level Supports allow rules only Supports allow rules and deny rules Is stateful: Return traffic is automatically allowed, regardless of any rules Is stateless: Return traffic must be explicitly allowed by rules We evaluate all rules before deciding whether to allow traffic We process rules in order, starting with the lowest numbered rule, when deciding whether to allow traffic Applies to an instance only if someone specifies the security group when launching the instance, or associates the security group with the instance later on Automatically applies to all instances in the subnets that it’s associated with (therefore, it provides an additional layer of defense if the security group rules are too permissive) Network Address Translation (NAT)NAT Instance When creating a NAT instance, Disable Source/Destination Check on the Instance NAT instances must be in a public subnet There must be a route out of the private subnet to the NAT instance, in order for this to work The amount of traffic that NAT instances can support depends on the instance size. If you are bottlenecking, increase the instance size. You can create high availability using auto-scaling Groups, multiple subnets in different AZs, and a script to automate fail-over. NAT Gateways If you have resources in multiple Availability Zones and they share one NAT gateway, in the event that the NAT gateway’s Availability Zone is down, resources in the other Availability Zones lose Internet access. To create an Availability Zone-independent architecture, create a NAT gateway in each Availability Zone and configure your routing to ensure that resources use the NAT gateway in the same Availability Zone. Redundant inside the Availability Zone Preferred by the enterprise Starts at 5 Gbps and scales currently to 45 Gbps No need to patch Not associated with security groups Automatically assigned a public ip address Remember to update your route tables No need to disable Source/Destination Checks Access Control Lists (ACL) VPC automatically comes with a default network ACL, and by default it allows all outbound and inbound traffic You can create custom network ACLs. By default, each custom network ACL denies all inbound and outbound traffic until you add rules. Each subnet in your VPC must be associated with a network ACL. If you don’t explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL Block IP Addresses using network ACLs not Security Groups You can associate a network ACL with multiple subnets; however, a subnet can be associated with only one network ACL at a time. When you associate a network ACL with a subnet, the previous association is removed. Network ACLs contain a numbered list of rules that is evaluated in order, starting with the lowest numbered rule. Network ACLs have separate inbound and outbound rules, and each rule can either allow or deny traffic Network ACLs are stateless; responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa.) VPC Flow Logs You cannot enable flow logs for VPCs that are peered with your VPC unless the peer VPC is in your account. You can tag flow logs After you’ve created a flow log, you cannot change its configuration; for example, you can’t associate a different IAM role with the flow log Not all IP traffic is monitored Traffic generated by instances when they contact the Amazon DNS server. If you use your own DNS server, then all traffic to that DNS server is logged Traffic generated by a Windows instance for Amazon Windows license activation Traffic to and from 169.254.169.254 for instance meta-data DHCP (Dynamic Host Configuration Protocol) traffic Traffic to the reserved IP address for the default VPC router BastionsA bastion host can be thought of as a special purpose machine, which has been configured to work against attacks. The machine contains a single application only, which it hosts. It has access to the public network, and it also known as a ‘Jump Box’. It is a powerful server, which provides high-level network security, since it is the only host that is granted permission to access the public network. A NAT Gateway or NAT Instance is used to provide Internet traffic to EC2 instances in a private subnets A Bastion is used to securely administer EC2 instances (Using SSH or RDP). Bastions are called Jump Boxes in Australia You cannot use a NAT Gateway as a Bastion host Direct ConnectBasic IdeaAWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your data-center, office, or co-location environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections. Direct Connect directly connects your data center to AWS Useful for high throughput workloads (for example, lots of network traffic) Or it you need a stable and reliable secure connection Steps to Creating a Direct Connect Connection Create a virtual interface in the Direct Connect console. This is a PUBLIC Virtual Interface Go to the VPC console and then to VPC connections. Create a Customer Gateway Create a Virtual Private Gateway Attach the Virtual Private Gateway to the desired VPC Select VPN Connections and create new VPN Connection Select the Virtual Private Gateway and the Customer Gateway Once the VPN is available, setup the VPN on the customer gateway or firewall Niches Topic of SAA-C02Global AcceleratorAWS Global Accelerator is a networking service that sends your user’s traffic through Amazon Web Service’s global network infrastructure, improving your internet user performance by up to 60%. When the internet is congested, Global Accelerator’s automatic routing optimizations will help keep your packet loss, jitter, and latency consistently low. AWS Global Accelerator is a service in which you create accelerators to improve availability and performance of your applications for local and global users. You can assigned two static IP addresses (or alternatively you can bring your own) You can control traffic using traffic dials. This is done within the endpoint group. With AWS Global Accelerator: Adding AWS Global Accelerator removes these inefficiencies. It leverages the Global AWS Network, resulting in improved performance. Without AWS Global Accelerator: It can take many networks to reach the application. Paths to and from the application may differ. Each hop impacts performance and can introduce risks. AWS Global Accelerator Components Static IP addresses By default, Global Accelerator provides you with two static IP addresses that you associate with your accelerator. 1.2.3.4 / 5.6.7.8 Accelerator An accelerator directs traffic to optimal endpoints over the AWS global network to improve the availability and performance of your Internet applications. Each accelerator includes one or more listeners DNS Name Global Accelerator assigns each accelerator a default Domain Name System(DNS) name – that points to the static IP addresses that Global Accelerator assigns to you. Depending on the use case, you can use your accelerator’s static IP addresses or DNS name to route traffic to your accelerator, or set up DNS records to route traffic using your own custom domain name. Network Zone A network zone services the static IP addresses for your accelerator from a unique IP subnet. Similar to an AWS Availability Zone, a network zone is an isolated unit with its own set of physical infrastructure. When you configure an accelerator, by default, Global Accelerator allocates two IPv4 addresses for it. If one IP address from a network zone becomes unavailable due to IP address blocking by certain client networks, or network disruptions, client applications can retry on the healthy static IP address from the other isolated network zone. Listener A listener processes inbound connections from clients to Global Accelerator, based on the port (or port range) and protocol that you configure. Global Accelerator supports both TCP and UDP protocols. Each listener has one or more endpoint groups associated with it, and traffic is forwarded to endpoints in one of the groups. You associate endpoint groups with listeners by specifying the Regions that you want to distribute traffic to. Traffic is disputed to optimal endpoints within the endpoint groups associated with a listener. Endpoint Group Each endpoint group is associated with a specific AWS Region Endpoint groups include one or more endpoints in the Region You can increase or reduce the percentage of traffic that would be otherwise directed to an endpoint group by adjusting a setting called a traffic dial The traffic dial lets you easily do performance testing or blue/green deployment testing for new releases across different AWS Regions, for example. Endpoint Endpoints can be Network Load Balancers, Application Load Balancers, EC2 instances, or Elastic IP addresses An Application Load Balancer endpoint can be an Internet-facing or internal. Traffic is routed to endpoints based on configuration options that you choose, such as endpoint wrights For each endpoint, you can configure weights, which are numbers that you can use to specify the proportion of traffic to route to each one. This can be useful, for example, to do performance testing within a Region. VPC End PointsAmazon Virtual Private Cloud (Amazon VPC) is a service that lets you launch AWS resources in a logically isolated virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. You can use both IPv4 and IPv6 for most resources in your virtual private cloud, helping to ensure secure and easy access to resources and applications. Endpoints are virtual devices. They are horizontally scaled. redundant, and highly available VPC components that allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic. There are two types of VPC endpoints: Interface Endpoints An interface endpoint is an elastic network interface with a private IP address that serves as an entry point for traffic destined to a supported service. Gateway Endpoints Amazon S3 DynamoDB VPC Private LinkIf you see a question asking about peering VPCs to tens, hundreds, or thousands of customer VPCS, think of AWS Private Link. It doesn’t require VPC peering; no route tables, NAT, IGWs, etc. Requires a Network Load Balancer on the service VPC and an ENI on the customer VPC. Transit GatewayIf you are given a scenario question where it’s talking about how you can simplify your network topology, maybe you have got hundreds of VPN connections coming in, direct connection, a whole bunch of VPC peering going on and you also need to support IP multicast. I want you to think of Transit Gateway immediately. It’s a way of simplifying your network architecture or topology, and it always uses a hub and spoke model. Allows you to have transitive peering between thousands of VPCs and on-premises data centers. Works on a hub-and-spoke model Works on a regional basis, but you can have it across multiple regions. You can use it across multiple AWS accounts using RAM (Resource Access Manager). You can use route tables to limit how VPCs talk to one another. Works with Direct Connect as well as VPN connections. Supports IP multi-cast (not supported by any other AWS service) VPN CloudHubIf you see a scenario talking about how you can essentially manage your multiple sites with VPN, please definitely consider VPN CloudHub. If you have multiple sites, each with it’s own VPN connection, you can use AWS VPN CloudHub to connect those sites together. Hub-and-spoke model. Low cost; easy to manage. It operates over the public Internet, but all traffic between the customer gateway and the AWS VPN CloudHub is encrypted. Networking Costs on AWSIf you see a scenario based question where it’s talking about cost optimization, you always want to use private IPs over public IPs Using private IP addresses will utilizes the AWS backbone network which will save on costs. If you want to cut all network costs, group your EC2 instances in the same Availability Zone and use private IP addresses. This will be cost-free, but make sure to keep in mind single point of failure issues. References AWS Storage Blog Using VPC hosted endpoints in shared VPCs with AWS Transfer Family Managing Amazon S3 access with VPC endpoints and S3 Access Points AWS Management &amp; Governance Blog Improve security by analyzing VPC flow logs with Amazon CloudWatch Contributor Insights Dive Into Exam Having just created a new VPC and launching an instance into its public subnet, you realise that you have forgotten to assign a public IP to the instance during creation. What is the simplest way to make your instance reachable from the outside world? Answer: Create an Elastic IP and new network interface. Associate the Elastic IP to the new network interface, and the new network interface to your instance. Explanation: Although creating a new NIC &amp; associating an EIP also results in your instance being accessible from the internet, it leaves your instance with 2 NICs &amp; 2 private IPs as well as the public address and is therefore not the simplest solution. By default, any user-created VPC subnet WILL NOT automatically assign public IPv4 addresses to instances – the only subnet that does this is the “default” VPC subnets automatically created by AWS in your account. Are you permitted to conduct your own vulnerability scans on your own VPC without alerting AWS first? Answer: Depends on the type of scan and the service being scanned. Some scans can be performed without alerting AWS, some require you to alert. Explanation: Until recently customers were not permitted to conduct penetration testing without AWS engagement. However that has changed. There are still conditions though. True or False: You can accelerate your application by adding a second Internet gateway to your VPC. Answer: FALSE Explanation: You may have only one Internet gateway per VPC. True or False: An application load balancer must be deployed into at least two subnets. Answer: True Which of the following allows you to SSH or RDP into an EC2 instance located in a private subnet? Answer: Bastion host Explanation: A Bastion host allows you to securely administer (via SSH or RDP) an EC2 instance located in a private subnet. Don’t confuse Bastions and NATs, which allow outside traffic to reach an instance in a private subnet. Which of the following offers the largest range of internal IP addresses? Answer: /16 Explanation: The /16 offers 65,536 possible addresses. When I create a new security group, all outbound traffic is allowed by default. Answer: True By default, how many VPCs am I allowed in each AWS region? Answer: 5 To save administration headaches, a consultant advises that you leave all security groups in web-facing subnets open on port 22 to 0.0.0.0/0 CIDR. That way, you can connect wherever you are in the world. Is this a good security design? Answer: No. Explanation: 0.0.0.0/0 would allow ANYONE from ANYWHERE to connect to your instances. This is generally a bad plan. The phrase ‘web-facing subnets’ does not mean just web servers. It would include any instances in that subnet some of which you may not strangers attacking. You would only allow 0.0.0.0/0 on port 80 or 443 to to connect to your public facing Web Servers, or preferably only to an ELB. Good security starts by limiting public access to only what the customer needs. Please see the AWS Security whitepaper for complete details.","link":"/Blog/2021/02/07/AWS-Solution-Architect-Associate-6-VPCs/"},{"title":"AWS Certified Solutions Architect - Associate 2020","text":"Content Outline Domain percentage of Examination Domain 1: Design Resilient Architectures 30% Domain 2: Design High-Performing Architectures 28% Domain 3: Design Secure Applications and Architectures 24% Domain 4: Design Cost-Optimized Architectures 18% TOTAL 100% Overview 130 Minutes in Length 60 Questions Multiple Choice Passing score of 720 (100 - 1000) Aim for 70% Qualification is valid for 2 years Scenario based questions Domain 1: Design Resilient Architectures 1.1 Design a multi-tier architecture solution 1.2 Design highly available and/or fault-tolerant architectures 1.3 Design decoupling mechanisms using AWS services 1.4 Choose appropriate resilient storage Domain 2: Design High-Performing Architectures 2.1 Identify elastic and scalable compute solutions for a workload 2.2 Select high-performing and scalable storage solutions for a workload 2.3 Select high-performing networking solutions for a workload 2.4 Choose high-performing database solutions for a workload Domain 3: Design Secure Applications and Architectures 3.1 Design secure access to AWS resources 3.2 Design secure application tiers 3.3 Select appropriate data security options Domain 4: Design Cost-Optimized Architectures 4.1 Identify cost-effective storage solutions 4.2 Identify cost-effective compute and database services 4.3 Design cost-optimized network architectures AWS - High Level ServicesGlobal Infrastructure One availability zone could be two or three data centers that are all within a couple of miles. Region is just a simply a geographic area that consists of two or more availability zones. Edge Locations are endpoints for AWS which are used for caching content. Typically this consists of CloudFront, Amazon’s Content Delivery Network(CDN). Services related to the Exam - Core Part Compute Storage Database Network &amp; Content Delivery Security, Identity &amp; Compliance Services related to the Exam - Other Part Migration &amp; Transfer Machine Learning Management &amp; Governance Analytics Desktop &amp; App Streaming Some Useful Website Jayendra’s Cloud Certification Blog - AWS Certification Catalog, a Hands On Technical &amp; Solution Architect based out of India. Braincert AWS Solutions Architect – Associate SAA-C02 Practice Exams, which are updated for SAA-C02 Stephane Maarek – AWS Certified Solutions Architect Associate Practice Exams Whizlabs - AWS Certified Solutions Architect Associate Practice test LLEI 的个人博客 - AWS Certified Solutions Architect Practice Tests SAA-C01 Exam MyTodo.vip - AWS Certification Practice Questions Jeff Zhang at Quizlet - AWS Certified Solutions Architect - Associate Practice Questions Examtopics - Amazon AWS Certified Cloud Practitioner Exam Actual Questions","link":"/Blog/2020/08/09/AWS-Certified-Solutions-Architect-Associate-2020/"},{"title":"AWS Solution Architect(Associate) - Topic 7: HA Architecture","text":"High availability protect against data center, availability zone, server, network and storage subsystem failures to keep your business running without downtime. HA ArchitectureLoad BalancersElastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, Lambda functions, and virtual appliances. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones. Load Balancer Types Application Load Balancer Application Load Balancer is best suited for load balancing of HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers. Application Load Balancer routes traffic to targets within Amazon VPC based on the content of the request. Network Load Balancer Network Load Balancer is best suited for load balancing of Transmission Control Protocol (TCP), User Datagram Protocol (UDP), and Transport Layer Security (TLS) traffic where extreme performance is required. Network Load Balancer routes traffic to targets within Amazon VPC and is capable of handling millions of requests per second while maintaining ultra-low latencies. Gateway Load Balancer Gateway Load Balancer makes it easy to deploy, scale, and run third-party virtual networking appliances. Providing load balancing and auto scaling for fleets of third-party appliances, Gateway Load Balancer is transparent to the source and destination of traffic. This capability makes it well suited for working with third-party appliances for security, network analytics, and other use cases. Classic Load Balancer Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and the connection level. Classic Load Balancer is intended for applications that were built within the EC2-Classic network. Detail need to notice 504 Error means the gateway has timed out. This means that the application not responding within the idle timeout period. Trouble shoot the application. Is it the Web Server Layer or Database Server Layer? If you need the IPv4 addresses of your end user, look for the X-Forwarded-For header. Instance monitored by ELB are reported as InService, or OutofService Health Checks check the instance health by talking to it Load Balances (Application load balancer and classic load balancer) have their own DNS name. You are never given an IP address. You can get a static IP address for network load balancer. Advanced Load Balancer TheorySticky SessionsThis feature is useful for servers that maintain state information in order to provide a continuous experience to clients. To use sticky sessions, the client must support cookies. Classic Load Balancer routes each request independently to the registered EC2 instance with the smallest load. Sticky sessions allow you to bind a user’s session to a specific EC2 instance. This ensures that all requests from the user during the session are sent to the same instance. You can enable Sticky Sessions for Application Load Balancers as well, but the traffic will be sent at the Target Group Level. Cross Zone Load BalancingCross Zone Load Balancing enables you to load balance across multiple availability zones With cross-zone load balancing, each load balancer node for your Classic Load Balancer distributes requests evenly across the registered instances in all enabled Availability Zones. If cross-zone load balancing is disabled, each load balancer node distributes requests evenly across the registered instances in its Availability Zone only. For more information, see Cross-zone load balancing in the Elastic Load Balancing User Guide. Path PatternsPath patterns allows you to direct traffic to different EC2 instances based on the URL contained in the request You can create a listener with rules to forward requests based on the URL path. This is known as path-based routing. If you are running micro-services, you can route traffic to multiple back-end services using path-based routing. For example, you can route general requests to one target group and requests to render images to another target group. Auto ScalingAWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost. Three Components Groups Logical Component Web-server group or Application group or Database group etc. Configuration Templates Groups uses a launch template or a launch configuration as a configuration template for its EC2 instance. You can specify information such as the AMI ID, instance type, key pair, security groups, and block device mapping for your instances. Scaling Options Scaling Options provides several ways for you to scale your Auto Scaling Groups. For example, you can configure a group to scale based on the occurrence of specified conditions (dynamic scaling) or on a schedule. Five Options Maintain current instance levels at all times Maintain a specified number of running instances at all times Scale manually Specify only the change in the maximum, minimum, or desired capacity of your Auto Scaling Group Scale based on a schedule Perform automatically as a function of time and date Scale based on demand Using scaling policies - lets you define parameters that control the scaling process. You can stabilize the CPU utilization of the Auto Scaling Group to stay at around 50 percent when the load on the application changes. Use predictive scaling You can also use Amazon EC2 Auto Scaling in combination with AWS Auto Scaling to scale resources across multiple services. HA ArchitectureEverything fails. Everything. You should always plan for failure. Always Design for failure Use Multiple AZ’s and Multiple Regions where ever you can Know the difference between Multi-AZ and Read Replicas for RDS. Multi-AZ is for disasters Read Replicas for performance Know the difference between scaling out and scaling up Scaling Out is using auto scaling groups to add additional instance Scaling Up is where we increase the resources inside out EC2 instance, like from t-2 micro to 6-x extra large Read the question carefully and always consider the cost element Know the different S3 storage classes Cloud Formation AWS CloudFormation gives you an easy way to model a collection of related AWS and third-party resources, provision them quickly and consistently, and manage them throughout their lifecycles, by treating infrastructure as code. CloudFormation is a way of completely scripting your cloud environment. Quick Start is a bunch of Cloud Formation templates already built by AWS Solutions Architects allowing you to create complex environments very quickly. Build serverless applications with SAMBuild serverless applications faster with the AWS Serverless Application Model (SAM), an open-source framework that provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML. During deployment, SAM transforms and expands the SAM syntax into CloudFormation syntax. Elastic BeanstalkAWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and service. With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring. Scaling: AWS Elastic Beanstalk leverages Elastic Load Balancing and Auto Scaling to automatically scale your application in and out based on your application’s specific needs. In addition, multiple availability zones give you an option to improve application reliability and availability by running in more than one zone. Customization: With AWS Elastic Beanstalk, you have the freedom to select the AWS resources, such as Amazon EC2 instance type including Spot instances, that are optimal for your application. Additionally, Elastic Beanstalk lets you “open the hood” and retain full control over the AWS resources powering your application. If you decide you want to take over some (or all) of the elements of your infrastructure, you can do so seamlessly by using Elastic Beanstalk’s management capabilities. CloudFormation vs BeanstalkElastic Beanstalk really is aimed at developers who just want to get their stuff into the AWS Cloud quickly. They don’t want to have to go and learn something like CloudFormation which is a way more powerful tool than Elastic Beanstalk. Highly Available BastionThe bastion hosts provide secure access to Linux instances located in the private and public subnets of your virtual private cloud (VPC). High Availability with Bastion Hosts Two hosts in two separate Availability Zones. Use a Network Load Balancer with static IP addresses and health checks to fail over from one host to the other. Can’t use an Application Load Balancer, as it is layer 7 and you need to use layer 4. One host in one Availability Zone behind an Auto Scaling group with health checks and a fixed EIP (Elastic IP). If the host fails, the health check will fail and the Auto Scaling group will provision a new EC2 instance in a separate Availability Zone. You can use a user data script to provision the same EIP to the new host. This is the cheapest option, but it is not 100% fault tolerant. On-Promise Services with AWSYou need to be aware of what high-level AWS services you can use on-promises for the exam: Database Migration Service (DMS) Server Migration Service (SMS) AWS Application Discovery Service VM Import/Export Download Amazon Linux 2 as an ISO Database Migration Service (DMS) Allows you to move databases to and from AWS Might have your DR environment in AWS and your on-promises environment as your primary. Works with most popular database technologies, such as Oracle, MySQL, DynamoDB, etc. Supports homogeneous (Oracle -&gt; Oracle) migrations and heterogeneous (SQL -&gt; Aurora) migrations. Server Migration Service (SMS) Supports Server Migration Service supports incremental replication of your on-promises servers in to AWS. Can be used as a backup tool, multi-site strategy (on-premises and off-premises), and a DR tool. AWS Application Discovery Service Helps enterprise customers plan migration projects by gathering information about their on-premises data centers. You can install the AWS Application Discovery Agent-less Connector as a virtual appliance on VMware vCenter. It will then build a server utilization map and dependency map of your on-premises environment. The collected data is retained in encrypted format in an AWS Application Discovery Service data store. You can export this data as a CSV filer and use it to estimate the Total Cost of Ownership (TCO) of running on AWS and to plan your migration to AWS. This data is also available in AWS Migration Hub, where you can migrate the discovered servers and track their progress as they get migrated to AWS. VM Import/Export Migrate existing applications in to EC2 Can be used to create a DR strategy on AWS as a second site. You can also use it to export your AWS VMs to your on-premises data center. Download Amazon Linux 2 as an ISO Works with all major virtualization providers, such as VMware. Hyper-V, KVM, VirtualBox (Oracle), etc. References AWS Architecture Blog Architecting for Reliable Scalability AWS Compute Blog Configuring private integrations with Amazon API Gateway HTTP APIs Networking &amp; Content Delivery Introducing AWS Gateway Load Balancer: Supported architecture patterns Scaling network traffic inspection using AWS Gateway Load Balancer AWS News Blog New AWS Auto Scaling – Unified Scaling For Your Cloud Applications New – Predictive Scaling for EC2, Powered by Machine Learning Dive Into Exam1) You have a website with three distinct services, each hosted by different web server autoscaling groups. Which AWS service should you use? Answer: Application Load Balancers (ALB) Explanation: The ALB has functionality to distinguish traffic for different targets (mysite.co/accounts vs. mysite.co/sales vs. mysite.co/support) and distribute traffic based on rules for target group, condition, and priority. 2) You have been tasked with creating a resilient website for your company. You create the Classic Load Balancer with a standard health check, a Route 53 alias pointing at the ELB, and a launch configuration based on a reliable Linux AMI. You have also checked all the security groups, NACLs, routes, gateways and NATs. You run the first test and cannot reach your web servers via the ELB or directly. What might be wrong? Answer: The launch configuration is not being triggered correctly. Explanation: In a question like this you need to evaluate if all the necessary services are in place. The glaring omission is that you have not built an autoscaling group to invoke the launch configuration you specified. The instance count and health check depend on instances being created by the autoscaling group. Finally, key pairs have no relevance to services running on the instance. 3) You work for a major news network in Europe. They have just released a new mobile app that allows users to post their photos of newsworthy events in real-time. Your organization expects this app to grow very quickly, essentially doubling its user base each month. The app uses S3 to store the images, and you are expecting sudden and sizable increases in traffic to S3 when a major news event takes place (as users will be uploading large amounts of content.) You need to keep your storage costs to a minimum, and you are happy to temporally lose access to up to 0.1% of uploads per year. With these factors in mind, which storage media should you use to keep costs as low as possible? Answer: S3 Standard-IA Explanation: The key drivers here are availability and cost, so an awareness of cost is necessary to answer this. Full S3 is quite expensive at around $0.023 per GB for the lowest band. S3 standard IA is $0.0125 per GB, S3 One Zone-IA is $0.01 per GB, and Legacy S3-RRS is around $0.024 per GB for the lowest band. Of the offered solutions S3 One Zone-IA is the cheapest suitable option. Glacier cannot be considered as it is not intended for direct access, however it comes in at around $0.004 per GB. S3 has an availability of 99.99%, S3-IA has an availability of 99.9% while S3-1 Zone-IA only has 99.5%. 4) When you have deployed an RDS database into multiple availability zones, can you use the secondary database as an independent read node? Answer: No. Explanation: The secondary database is to be thought of as a DR site, it will be active only when the primary fails, as per the documentation. You need read replicas to increase your read speed. https://aws.amazon.com/rds/details/multi-az/","link":"/Blog/2021/03/02/AWS-Solution-Architect-Associate-7-HA-Architecture/"},{"title":"AWS Solution Architect(Associate) - Topic 10: Serverless Architecture","text":"A serverless architecture is a way to build and run applications and services without having to manage infrastructure. Your application still runs on servers, but all the server management is done by AWS. You no longer have to provision, scale, and maintain servers to run your applications, databases, and storage systems. Learn more about serverless computing here. ServerlessLambdaAWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers, creating workload-aware cluster scaling logic, maintaining event integrations, or managing runtimes. Lambda scales out (not up) automatically Lambda functions are independent, 1 event = 1 function Lambda is serverless Know what services are serverless RDS is not serverless (Aurora service is a exception) even though AWS takes care of its operation system, there is still an operating system that they have to go in and you still gonna have downtime when they’re doing maintenance. DynamoDB is serverless S3 is serverless API Gateway is serverless EC2 is not serverless, because it’s obviously a virtual machine Lambda functions can trigger other lambda functions, 1 event can = x functions if functions trigger other functions Architectures can get extremely complicated, AWS X-ray allows you to debug what is happening Lambda can do things globally, you can use it to back up S3 buckets to other S3 buckets etc. Serverless Application Model (SAM)The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. CloudFormation extension optimized for serverless applications New types: functions, APIs, tables Supports anything CloudFormation supports Run serverless applications locally Package and deploy using CodeDeploy Elastic Container Service (ECS)Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service. Customers such as Duolingo, Samsung, GE, and Cookpad use ECS to run their most sensitive and mission critical applications because of its security, reliability, and scalability. What are Containers and Docker? A container is a package that contains an application, libraries, runtime, and tools required to run it Run on a container engine like Docker Provides the isolation benefits of virtualization with less overhead and faster starts than VMs Containerized applications are portable and offer a consistent environment What is ECS? Managed container orchestration service Create cluster to manage fleets of container deployments ECS manages EC2 or Fargate instances Schedules containers for optimal placement Defines rules for CPU and memory requirements Monitor resource utilization Deploy, update, roll back FREE VPC, security groups, EBS volumes ELB CloudTrail and CloudWatch (Native support for CloudWatch so that you can get alarmed on state changes in the cluster) ECS Components Cluster: Logical collection of ECS resources – either ECS EC2 instance or Fargate instances Task: Single running copy of any containers defined by a task definition. Task Definition: Defines your application Service: Allows task definitions to be scaled by adding tasks Container Definition: Inside a task definition, it defines the individual containers a task uses Registry: Storage for container images Fargate Serverless container engine Eliminates need to provision and manage servers Specify and pay fore resources per application Works with both ECS and EKS Each workload runs in its own kernel Isolation and security Choose EC2 instead if: Compliance requirements Require broader customization Require GPUs Elastic Kubernetes Service (EKS) K8s is open-source software that lets you deploy and manage containerized applications at scale Same toolset on-premises and in cloud Containers are grouped in pods Like ECS, supports both EC2 and Fargate Why use EKS? Already using K8s Want to migrate to AWS Elastic Container Registry (ECR) Managed Docker container registry Store, manage, and deploy images Integrated with ECS and EKS Works with on-premises deployments Highly available Integrated with IAM Pay for storage and data transfer ECS (Elastic Container Service) + ELB (Elastic Load Balancing) Distribute traffic evenly across tasks in your service Supports ALB (Application Load Balancer), NLB (Network Load Balancer), CLB (Classic Load Balancer) Use ALB to route HTTP/HTTPS (layer 7) traffic Use NLB or CLB to route TCP (layer 4) traffic Supported by both EC2 and Fargate launch types ALB allows: Dynamic host port mapping Path-based routing Priority rules ALB is recommended over NLB or CLB References Containers NEW – Using Amazon ECS Exec to access your containers on AWS Fargate and Amazon EC2 AWS Developer Blog Deploying AWS Step Functions using GitHub Actions AWS Compute Blog Getting started with serverless for developers: Part 1 Using container image support for AWS Lambda with AWS SAM Dive Into Exam What AWS service can be used to help resolve an issue with a lambda function? Answer: AWS X-Ray Explanation: AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices &amp; serverless architectures.","link":"/Blog/2021/04/16/AWS-Solution-Architect-Associate-10-Serverless/"},{"title":"AWS Solution Architect(Associate) - Topic 5: Route53","text":"Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost effective way to route end users to Internet applications by translating names like www.example.com into the numeric IP addresses like 192.0.2.1 that computers use to connect to each other. Route53Common DNS Types SOA Records: Start of Authority Record, it stores information about: The name of the server that supplied the data for the zone The administrator of the zone The current version of the data file The default number of seconds for the time-to-live file on resource records NS Records: NS Stands for Name Server Records They are used by Top Level Domain servers to direct traffic to the Content DNS server which contains the authoritative DNS records. A Records An A record is the fundamental type of DNS record. The “A” in A record stands for “Address” The A record is used by a computer to translate the name of the domain to an IP address. www.acloud.guru -&gt; 123.10.10.80 CNAMES A Canonical Name can be used to resolve one domain name to another. mobile.acloud.guru -&gt; m.acloud.guru Alias Records Alias records work like a CNAME record in that you can map one DNS name to another “target” DNS name. The key difference between alias records and CNAME is that, a CNAME can not be used for naked domain names. You can’t have a CNAME for http://acloud.guru, it must be either an A record or an Alias. MX Records PTR Records Exam Tips ELBS do not have pre-defined IPv4 addresses; you resolve to them using a DNS name Given the choice, always choose an Alias Record over a CNAME Health Check You can set health checks on individual record sets If a record set fails a health check it will be removed from Route53 until it passes the health check You can set SNS notifications to alert you if a health check is failed Routing PoliciesSimple Routing Policy You can only have one record with multiple IP addresses If you specify multiple values in the record, Route 53 returns all values to the user in a random order Weighted Routing Policy Allows you split your traffic based on different weights assigned For example: you can set 10% of your traffic to go to US-EAST-1 and 90% to go to EU-WEST-1 Latency Routing Policy Allows you to route your traffic based on the network latency for your end user (Which region will give them the fastest response time) To use latency-based routing, you create a latency resource record set for the Amazon EC2 resource in each region that hosts your website Failover Routing Policy Failover routing policies are used when you want to create an active/passive set up. For example. you may want to primary site to be in EU-West-2 and your secondary DR Site in AP-SOUTHEAST-2. Geolocation Routing Policy Lets you choose where your traffic will be sent based on the geographic location of your users. Geoproximity Routing Policy (Traffic Flow Only) Lets Amazon Route 53 route traffic to your resources based on the geographic location of your users and your resources. You can also optionally choose to route more traffic or less to a given resource by specifying a value, known as a bias. A bias expands or shrinks the size of the geographic region from which traffic is routed to a resource. To use Geoproximity Routing, you must use Route 53 traffic flow. Multivalue Answer Routing Multivalue answer routing lets you configure Amazon Route 53 to return multiple values, such as IP addresses for your web servers, in response to DNS queries. Route 53 responds to DNS queries with up to eight healthy records and gives different answers to different DNS resolvers. The choice of which to use is left to the requesting service effectively creating a form or randomization. Multivalue answer routing lets you configure Amazon Route 53 to return multiple values, such as IP addresses for your web servers, in response to DNS queries. You can specify multiple values for almost any record, but multivalue answer routing also lets you check the health of each resource, so Route 53 returns only values for healthy resources. This is similar to simple routing however it allows you to put health checks on each record set. References Deploying Application on S3 or ES2 Instance using Route 53/Cloudfront Gitlab","link":"/Blog/2021/02/04/AWS-Solution-Architect-Associate-5-Route53/"},{"title":"AWS Solution Architect(Associate) - Topic 8: Applications","text":"Customers are using AWS high level services(e.g. Amazon Kinesis)to collect, process, and analyze real-time data. In this way, they can react quickly to new information from their business, their infrastructure, or their customers. For example, Epic Games ingests more than 1.5 million game events per second for its popular online game, Fortnite. ApplicationsSQS (Amazon Simple Queue Service)Amazon SQS is a web services that gives you access to a message queue that can be used to store messages while waiting for a computer to process them. So it is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. So, you should notice the word “Decouple” if you ever see it in the exam. SQS is pull-based, not pushed-based Messages are 256 KB in size. Messages can be kept in the queue from 1 minute to 14 days; the default retention period is 4 days. SQS guarantees that you messages will be processed at lease once (detail explained below: visibility). Amazon SQS long polling is a way to retrieve messages from your Amazon SQS queues. While the regular short polling returns immediately (even if the message queue being polled is empty), long polling doesn’t return a response until a message arrives in he message queue, or the long poll times out. So, please bear in mind that using long pulling to reduce the cost because long polling essentially won;t turn a response until a message arrives. Using Amazon SQS with other AWS infrastructure web servicesAmazon SQS message queuing can be used with other AWS Services such as Redshift, DynamoDB, RDS, EC2, ECS, Lambda, and S3, to make distributed applications more scalable and reliable. Below are some common design patterns: Work Queues: Decouple components of a distributed application that may not all process the same amount of work simultaneously. Buffer and Batch Operations: Add scalability and reliability to your architecture, and smooth out temporary volume spikes without losing messages or increasing latency. Request Offloading: Move slow operations off of interactive request paths by enqueing the request. Fanout: Combine SQS with Simple Notification Service (SNS) to send identical copies of a message to multiple queues in parallel. Priority: Use separate queues to provide prioritization of work. Scalability: Because message queues decouple your processes, it’s easy to scale up the send or receive rate of messages - simply add another process. Resiliency: When part of your system fails, it doesn’t need to take the entire system down. Message queues decouple components of your system, so if a process that is reading messages from the queue fails, messages can still be added to the queue to be processed when the system recovers. Queue typesAmazon SQS offers two queue types for different application requirements: Standard Queues FIFO Queues Unlimited Throughput: Standard queues support a nearly unlimited number of transactions per second (TPS) per API action. High Throughput: By default, FIFO queues support up to 300 messages per second. When you batch 10 messages per operation (maximum), FIFO queues can support up to 3,000 messages per second. At-Least-Once Delivery: A message is delivered at least once, but occasionally more than one copy of a message is delivered. Exactly-Once Processing: A message is delivered once and remains available until a consumer processes and deletes it. Duplicates aren’t introduced into the queue. Best-Effort Ordering: Occasionally, messages might be delivered in an order different from which they were sent. First-In-First-Out Delivery: The order in which messages are sent and received is strictly preserved (i.e. First-In-First-Out). Standard Queues You can use standard message queues in many scenarios, as long as your application can process messages that arrive more than once and out of order, for example: Decouple live user requests from intensive background work: Let users upload media while resizing or encoding it. Allocate tasks to multiple worker nodes: Process a high number of credit card validation requests. Batch messages for future processing: Schedule multiple entries to be added to a database. FIFO Queues FIFO queues are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates can’t be tolerated, for example: FIFO queues are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates can’t be tolerated, for example: Ensure that user-entered commands are executed in the right order. Display the correct product price by sending price modifications in the right order. Prevent a student from enrolling in a course before registering for an account. SQS Visibility Visibility timeout is the amount of time that the message is invisible in the SQS queue after a reader picks up that message will then be processed before the visibility timeout expired, the message will then be deleted from the queue. If the job is not processed within that time, the message will become visible again and another reader will process it. This could result in the same message being delivered twice. Visibility timeout maximum is 12 hours. It’s a very popular exam question that asking about getting the same message being delivered twice and what could be the cause of it. SWF (Simple Workflow Service)The Amazon Simple Workflow Service (Amazon SWF) makes it easy to build applications that coordinate work across distributed components. In Amazon SWF, a task represents a logical unit of work that is performed by a component of your application. Coordinating tasks across the application involves managing intertask dependencies, scheduling, and concurrency in accordance with the logical flow of the application. SWF vs SQS SQS has a retention period of up to 14 days; with SWF, workflow executions can last up to 1 year Amazon SWF presents a task-oriented API, whereas Amazon SQS offers a message-oriented API. Amazon SWF ensures that a task is assigned only once and is never duplicated. With Amazon SQS, you need to handle duplicated messages and may also need to ensure that a message is processed only once. Amazon SWF keeps track of all the tasks and events in a application. With Amazon SQS, you need to implement your own application-level tracking, especially if your application uses multiple queues. SWF Actors Workflow Starters – An application that can initiate (start) a workflow. Could be your e-commerce website following the placement of an order, or a mobile app searching for bus times. Deciders – Control the flow of activity tasks in a workflow execution. If something has finished (or failed) in a workflow, a Decider decides what to do next. Activity Workers – Carry out the activity tasks. SNS (Simple Notification Service)Amazon Simple Notification Service (Amazon SNS) is a fully managed messaging service for both application-to-application (A2A) and application-to-person (A2P) communication. SNS allows you to group multiple recipients using topics, A topic is an ‘access point’ for allowing recipients to dynamically subscribe for identical copies of the same notification. One topic can support deliveries to multiple endpoint types – for example, you can group together iOS, Android and SMS recipients. When you publish once to a topic, SNS delivers appropriately formatted copies of your message to each subscriber. SNS Availability To prevent messages from being lost, all messages published to Amazon SNS are stored redundantly across multiple availability zones. SNS Benefits Instantaneous, push-bases delivery (no polling) Simple APIs And easy integration with applications Flexible message delivery over transport protocols Inexpensive , pay-as-you-go model with no up-front costs Web-based AWS Management Console offers the simplicity of a point-and-check interface SNS vs SQS Amazon really like to quiz you the differences between SWF &amp; SQS and then SNS &amp; SQS. Both Messaging Services in AWS SNS - Push SQS - Polls (Pulls) Elastic TranscoderAmazon Elastic Transcoder manages all aspects of the media transcoding process for you transparently and automatically. There’s no need to administer software, scale hardware, tune performance, or otherwise manage transcoding infrastructure. Media Transcoder in the cloud Convert media files from their original source format in to different formats that will play on smart phones, tablets, PCs, etc. Provides transcoding presets for popular output formats, which means that you don;t need to guess about which settings work best on particular devices. Pay based on the minutes that you transcode and the resolution at which you transcode. API GatewayAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the “front door” for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications. Remember what API Gateway is at a high level: it’s a door to your environment API Gateway has caching capabilities to increase performance API Gateway is low cost and scales automatically You can throttle API Gateway to prevent attacks You can log results to CloudWatch If you are using JavaScript/AJAX that uses multiple domains with API Gateway, ensure that you have enables CORS on API Gateway. CORS is enforced by the client, so it is enforced by your browser What Can API Gateway Do? Expose HTTPS endpoints to define a RESTful API Serverlessly connect to services like Lambda &amp; DynamoDB Send each API endpoint to a different target Run efficiently with low cost Scale effortlessly Track and control usage by API key Throttle requests to prevent attacks Connect to CloudWatch to log all requests for monitoring Maintain multiple versions of your API How DO I Configure API Gateway? Define Resources and nested Resources (URL Paths) For each Resources: Select supported HTTP methods (verbs) Set security Choose target (such as EC2, Lambda, DynamoDB, etc.) Set request and response transformation How Do I Deploy API Gateway? Deploy API to a stage Uses API Gateway domain, by default Can use custom domain Support AWS Certificate Manager: free SSL/TLS certs. API Gateway CachingYou can add caching to API calls by provisioning an API Gateway cache and specifying its size in gigabytes. The cache is provisioned for a specific stage of your APIs. This improves performance and reduces the traffic sent to your back end. Cache settings allow you to control the way the cache key is built and the time-to-live (TTL) of the data stored for each method. API Gateway also exposes management APIs that help you invalidate the cache for each stage. Caching is available for REST APIs in API Gateway. Same Origin PolicyIn computing, the same-origin policy is an important concept in the web application security model. Under the policy, a web browser permits scripts contained in a first web page to access data in a second web page, but only if both web pages have the same origin. This is done to prevent Cross-Site Scripting (XSS) attacking. Enforced by web browers Ignored by tools like PostMan and curl. CORS is one way the server at the other end (not the client code in the browser) can relax the same-origin policy. Cross-origin resource sharing (CORS) is a mechanism that allows restricted resources (e.g. fonts) on a web page to be requested from another domain outside the domain from which the first resource was served. Browser makes an HTTP OPTIONS call for a URL (OPTION is an HTTP method like GET, PUT, and POST) Server returns a response that says: “These other domains are approved to GET this URL” Error: “Origin policy cannot be read at the remote resource?” You need to enable CORS on API Gateway. Going into the exam, if you see something where it’s talking about origin policy cannot be read at the remote resource. It means the cause is not enabled on your API Gateway and API Gateway is not able to go and request that information from the other side. KinesisAmazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. Amazon Kinesis offers key capabilities to cost-effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application. 3 Different Types of Kinesis Amazon Kinesis Data Streams Amazon Kinesis Data Firehose Amazon Kinesis Data Analytics Kinesis Data StreamsIf you see shards come up in your exam, think straight away of Kinesis streams because Kinesis is the only form of Kinesis that has shards. Review the developer guide Kinesis Streams Consists Of Shards: 5 transactions per second for reads, up to a maximum total data read rate of 2 MB per second and up to 1,000 records per second for writes, up to a maximum total data write rate of 1 MB per second. The data capacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream is the sum of the capacities of its shards. Web Identity Federation - CognitoAmazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Apple, Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0 and OpenID Connect. Cognito is an Identity Broker which handles interaction between your applications and the Web ID provider. Amazon Cognito provides Web Identity Federation with the following features: Sign-up and sign-in to your Apps Access for guest users Acts as an Identity Broker between your application and Web ID providers. so you don’t need to write any additional code. Synchronizes user data for multiple devices Recommended for all mobile applications AWS services User Pool vs Identity PoolCognito User Pools User Pools are user directories used to manage sign-up and sign-in functionality for mobile and web applications. Users can sign-in directly to the User Pool, or using Facebook, Amazon, or Google. Cognito acts as an Identity Broker between the identity provider and AWS. Successful authentication generates a JSON Web token (JWTs). User Pool is user based. It handles things like user registration, authentication, and account recovery. Cognito Identity Pools Identity Pools enable provide temporary AWS credentials to access AWS services like S3 or DynamoDB. It’s all about the authorization of access to AWS resources whereas User Pools are all about your actual users. So, the difference between user pools and identity pools is that user pools are things like your email address to your password, whereas identity pools is the actual granting you access to an AWS resources. Amazon Cognito in ActionSo we’ve got a user who wants to connect into our website. She’s going to log in using her Facebook account once Facebook has authenticated her account as being genuine so her user name and password is correct. It’s going to pass back a authentication token to Cognito User Pool. Cognito User Pool then convert that to a JWT (JSON Web Token) token. She then sends that JWT token to an Identity Pool and that Identity Pool will grant her AWS credentials in a form of IAM role. Then she will be able to go on and access her AWS resources. Cognito SynchronizationCognito tracks the association between user identity and the various different devices that sign-in from. In order to provide a seamless user experience for your application, Cognito uses Push Synchronization to push updates and synchronize user data across multiple devices. Cognito uses SNS to send a notification to all the devices associated with a given user identity whenever data stored in the cloud changes. References AWS News Blog New – Amazon Kinesis Data Analytics for Java AWS Compute Blog Increasing real-time stream processing performance with Amazon Kinesis Data Streams enhanced fan-out and AWS Lambda Scale Amazon Kinesis Data Streams with AWS Application Auto Scaling https://aws.amazon.com/blogs/big-data/perform-near-real-time-analytics-on-streaming-data-with-amazon-kinesis-and-amazon-elasticsearch-service/ Dive Into ExamIn SWF, what does a “domain” refer to? Answer: A collection of related workflows. Explanation: Domains in SWF are a mechanism to scope SWF resources such as workflows, activity types, and workflow execution. All the resources are scoped to a domain. Domains isolate one set of types, executions, and task lists from other ones within an AWS account. When you work with SWF, you need to first define a domain. All the other resources are defined within a domain.","link":"/Blog/2021/03/28/AWS-Solution-Architect-Associate-8-Applications/"},{"title":"AWS Solution Architect(Associate) - Topic 9: Cloud Security","text":"AWS allows you to automate manual security tasks so you can shift your focus to scaling and innovating your business. Cloud SecurityReducing Security ThreatsIf you are operating a public web application, you should prefer WAF in these instances mentioned below, and it can be integrated into CloudFront. Network Access Control List (NACL) In AWS, a network ACL (or NACL) controls traffic to or from a subnet according to a set of inbound and outbound rules. This means it represents network level security. For example, an inbound rule might deny incoming traffic from a range of IP addresses, while an outbound rule might allow all traffic to leave the subnet. Because NACLs function at the subnet level of a VPC, each NACL can be applied to one or more subnets, but each subnet is required to be associated with one—and only one—NACL. When you create a VPC, AWS automatically creates a default NACL for it. You can add and remove rules from a default NACL, but you can’t delete the NACL itself. AWS Security Groups In AWS, a security group controls traffic to or from an EC2 instance according to a set of inbound and outbound rules. This means it represents instance-level security. For example, an inbound rule might allow traffic from a single IP address to access the instance, while an outbound rule might allow all traffic to leave the instance. Because security groups function at the instance level of a VPC, each security group can be applied to one or more instances, even across subnets. And each instance is required to be associated with one or more security groups. To be precise, a security group is associated with a network interface that is attached to an instance, but we don’t discuss that detail for simplicity. Web Application Firewall (WAF) AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits and bots that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that control bot traffic and block common attack patterns, such as SQL injection or cross-site scripting. KMS (Key Management Service) AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. Regional secure key management and encryption and decryption Manages customer master keys (CMKs) Ideal for S3 objects, database passwords and API keys stored in Systems Manager Parameter Store Encrypt and decrypt data up to 4 KB in size Integrated with most AWS services Pay per API call Audit capability using CloudTrail – logs delivered to S3 FIPS 140-2 Level 2, FIPS is a US government computer security standard used to approve cryptographic modules. Level 3 is CloudHSM Types of CMKs Type Can View Can Manage Dedicated to My Account Customer Managed √ √ √ AWS Managed CMK √ √ AWS Owned CMK √ Symmetric vs Asymmetric CMKs Symmetric Asymmetric Same key used for encryption and decryption Mathematically related public/private key pair AES-256 RSA and elliptic-curve cryptography (ECC) Never leaves AWS unencrypted Private key never leaves AWS unencrypted Must call the KMS APIs to use Must call the KMS APIs to use the private key AWS services integrated with KMS use symmetric CMKs Download the public key and use outside AWS Encrypt, decrypt, and re-encrypt data Used outside AWS by users who can’t call KMS APIs Generate data keys, data key pairs, and random byte strings AWS services integrated with KMS do not support asymmetric CMKS Import your own key material Often used in Sign messages and verify signatures CloudHSM AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud. Dedicated hardware security module (HSM) FIPS 140-2 Level 3 (Level 2 is KMS) Manage your own keys No access to the AWS-managed component Runs within a VPC in your account Single tenant , dedicated hardware, multi-AZ cluster Industry-standard APIs – no AWS APIs PKCS#11 Java Cryptography Tensions (JCE) Microsoft CryptoNG (CNG) Keep your keys safe – irretrievable if lost! Systems Manager Parameter StoreAWS Systems Manager Parameter Store (Parameter Store) provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values. Component of AWS Systems Manager (SSM) Secure serverless storage for configuration and secrets: Passwords Database connection strings License codes API keys Values can be stored encrypted (KMS) or plain-text Separate data from source control Store parameters in hierarchies Track versions Set TTL to expire values such as passwords. AWS Parameter Store vs. AWS Secrets ManagerTo implement password rotation lifecycles, use AWS Secrets Manager (Secrets Manager). Secrets Manager allows you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. For more information, see What is AWS Secrets Manager? Similarities Managed Key/Value Store Services. Both services can store values up to 4096 characters and allow the keys to have prefixes. Similar Encryption Options. Both services can leverage AWS KMS to encrypt values. Both Reference-able in CloudFormation. Writing on how SSM Parameter Store and AWS Secrets Manager interact with CloudFormation can be a whole separate article. Differences Password Generation. AWS Secrets Manager is able to generate random secrets through the AWS CLI or SDK. Secrets Rotation. Another feature unique to AWS Secrets Manger is the ability to rotate the secret value. Out of the box, AWS Secrets Manager provides full key rotation integration with RDS. Cost. There are no additional charges for using SSM Parameter Store. However, there are limit of 10,000 parameters per account. On the other hand, AWS Secrets Manager does accrue additional costs. At the time of this writing, it costs $0.40 per secret stored and additional $0.05 for 10,000 API calls. Cross Account Access. Another way AWS Secrets Manager is substantially different from SSM Parameter store, is that secrets can be shared across accounts. For example, IAM users and application resources in one development or production AWS account will be able access secrets stored in a different AWS account (e.g. Security AWS Account). Such functionality is also beneficial for use cases where a customer needs to share a particular secret with a partner. The article found HERE demonstrates how to setup a cross-account AWS Secrets Manager secret. Secrets Manager takes things several steps further and it would not be surprising to see AWS continue to build on this functionality. References AWS Security Blog Demystifying KMS keys operations, bring your own key (BYOK), custom key store, and ciphertext portability AWS Management &amp; Governance Blog The Right Way to Store Secrets using Parameter Store Third Party Blog AWS Parameter Store vs. AWS Secrets Manager","link":"/Blog/2021/04/12/AWS-Solution-Architect-Associate-9-Security/"},{"title":"Build Modern Serverless Applications with AWS Amplify and AppSync","text":"In the fast-paced field of web applications, containerization has become not only common but the preferred mode of packaging and delivering web applications. Containers allow us to package our applications and deploy them anywhere without having to reconfigure or adapt our applications to the deployment platform. Amazon Elastic Container Service (Amazon ECS) is the service Amazon provide to run Docker applications on a scalable cluster. AWS AmplifyAWS Amplify is a set of products and tools that enable mobile and front-end web developers to build and deploy secure, scalable full-stack applications, powered by AWS. Common Command Line amplify &lt;category&gt; &lt;subcommand&gt; amplify &lt;category&gt; add: Add resources of a category to the cloud. Place a CloudFormation template for the resources of this category in the category’s subdirectory amplify/backend/\\&lt;category\\&gt; Insert its reference into the above-mentioned root stack as the nested child stack. When working in teams, it is good practice to run an amplify pull before modifying the backend categories. amplify &lt;category&gt; update amplify &lt;category&gt; remove amplify &lt;category&gt; push amplify push: Once you have made your category updates, run the command amplify push to update the cloud resources. amplify pull: Operates similar to a git pull. amplify env &lt;subcommand&gt;: Control multiple environment amplify env add amplify env list amplify env checkout amplify env remove amplify console: Launches the browser directing you to your cloud project in the AWS Amplify Console. amplify delete amplify init: the root stack is created with three resources: IAM role for unauthenticated users IAM role for authenticated users S3 bucket, the deployment bucket, to support this provider’s workflow amplify publish amplify run amplify status Amplify CLIThe Amplify Command Line Interface (CLI) is a unified tool-chain to create, integrate, and manage the AWS cloud services for your app. **Authentication: **The Amplify CLI supports configuring many different Authentication and Authorization workflows, including simple and advanced configurations of the login options, triggering Lambda functions during different lifecycle events, and administrative actions which you can optionally expose to your applications. **API(GraphQL): **The GraphQL Transform provides a simple to use abstraction that helps you quickly create backends for your web and mobile applications on AWS. With the GraphQL Transform, you define your application’s data model using the GraphQL Schema Definition Language (SDL) and the library handles converting your SDL definition into a set of fully descriptive AWS CloudFormation templates that implement your data model. **Serverless Functions: **You can add a Lambda function to your project which you can use alongside a REST API or as a data source in your GraphQL API using the @function directive. **Storage: **Amplify CLI’s storage category enables you to create and manage cloud-connected file &amp; data storage. Use the storage category when you need to store: app content (images, audio, video etc.) in an public, protected or private storage bucket or app data in a NoSQL database and access it with a REST API + Lambda DirectivesThe Amplify CLI provides GraphQL directives to enhance your schema with additional capabilities, such as custom indexes, authorization rules, function triggers and more. @model: Defines a top level object type in your API that are backed by Amazon DynamoDB Allows you to easily define top level object types in your API that are backed by Amazon DynamoDB. 123456# override the names of any generated queries, mutations and subscriptions, or remove operations entirely.type Post @model(queries: { get: &quot;post&quot; }, mutations: null, subscriptions: null) { id: ID! # id: ID! is a required attribute. title: String! tags: [String!]!} @key: Configures custom index structures for @model types The @key directive makes it simple to configure custom index structures for @model types. 1directive @key(fields: [String!]!, name: String, queryField: String) on OBJECT A @key without a name specifies the key for the DynamoDB table’s primary index. You may only provide 1 @key without a name per @model type. Argument fields The first field in the list will always be the HASH key. If two fields are provided the second field will be the SORT key. If more than two fields are provided, a single composite SORT key will be created from a combination of fields[1...n]. name When provided, specifies the name of the secondary index. When omitted, specifies that the @key is defining the primary index. queryField When defining a secondary index (by specifying the name argument), this specifies that a new top level query field that queries the secondary index should be generated with the given name. Using the new ‘todosByStatus’ query you can fetch todos by ‘status’ 12345678910111213141516type Todo @model @key(name: &quot;todosByStatus&quot;, fields: [&quot;status&quot;], queryField: &quot;todosByStatus&quot;) { id: ID! name: String! status: String!}query todosByStatus { todosByStatus(status: &quot;completed&quot;) { items { id name status } }} @auth: Defines authorization rules for your @model types and fields Authorization is required for applications to interact with your GraphQL API. API Keys are best used for public APIs (or parts of your schema which you wish to be public) or prototyping, and you must specify the expiration time before deploying. When applied to a type, augments the application with owner and group-based authorization rules. 12345678910directive @auth(rules: [AuthRule!]!) on OBJECT | FIELD_DEFINITIONinput AuthRule { allow: AuthStrategy! provider: AuthProvider ownerField: String # defaults to &quot;owner&quot; when using owner auth identityClaim: String # defaults to &quot;username&quot; when using owner auth groupClaim: String # defaults to &quot;cognito:groups&quot; when using Group auth groups: [String] # Required when using Static Group auth groupsField: String # defaults to &quot;groups&quot; when using Dynamic Group auth operations: [ModelOperation] # Required for finer control only the owner of the object has the authorization to perform read (getTodo and listTodos), update (updateTodo), and delete (deleteTodo) operations on the owner created object 123456type Todo @model @auth(rules: [{ allow: owner }]) { id: ID! updatedAt: AWSDateTime! content: String!} only the owner of the object has the authorization to perform update (updateTodo) and delete (deleteTodo) operations on the owner created object, but anyone can read them (getTodo, listTodos). 123456type Todo @model @auth(rules: [{ allow: owner, operations: [create, delete, update] }]) { id: ID! updatedAt: AWSDateTime! content: String!} @connection: Defines 1:1, 1:M, and N:M relationships between @model types Has one: In the simplest case, you can define a one-to-one connection where a project has one team: 12345678910type Project @model { id: ID! name: String team: Team @connection}type Team @model { id: ID! name: String!} Has many: The following schema defines a Post that can have many comments: 123456789101112type Post @model { id: ID! title: String! comments: [Comment] @connection(keyName: &quot;byPost&quot;, fields: [&quot;id&quot;])}type Comment @model @key(name: &quot;byPost&quot;, fields: [&quot;postID&quot;, &quot;content&quot;]) { id: ID! postID: ID! content: String!} @function: Configures a Lambda function resolvers for a field The @function directive allows you to quickly &amp; easily configure AWS Lambda resolvers within your AWS AppSync API. 123456789101112131415161718directive @function(name: String!, region: String) on FIELD_DEFINITION# You can connect this function to your AppSync API deployed via Amplify using this schema:# Using this as the entry point, you can use a single function to handle many resolvers.type Query { posts: [Post] @function(name: &quot;GraphQLResolverFunction&quot;)}type Post { id: ID! title: String! comments: [Comment] @function(name: &quot;GraphQLResolverFunction&quot;)}type Comment { postId: ID! content: String} @http: Configures an HTTP resolver for a field The @http directive allows you to quickly configure HTTP resolvers within your AWS AppSync API. 123456directive @http(method: HttpMethod, url: String!, headers: [HttpHeader]) on FIELD_DEFINITIONenum HttpMethod { PUT POST GET DELETE PATCH }input HttpHeader { key: String value: String} The directive allows you to define URL path parameters, and specify a query string and/or specify a request body. 12345678910type Post { id: ID! title: String description: String views: Int}type Query { listPosts: Post @http(url: &quot;https://www.example.com/posts&quot;)} @predictions: Queries an orchestration of AI/ML services such as Amazon Rekognition, Amazon Translate, and/or Amazon Polly The @predictions directive allows you to query an orchestration of AI/ML services such as Amazon Rekognition, Amazon Translate, and/or Amazon Polly. 1234567directive @predictions(actions: [PredictionsActions!]!) on FIELD_DEFINITIONenum PredictionsActions { identifyText # uses Amazon Rekognition to detect text identifyLabels # uses Amazon Rekognition to detect labels convertTextToSpeech # uses Amazon Polly in a lambda to output a presigned url to synthesized speech translateText # uses Amazon Translate to translate text from source to target language} @searchable: Makes your data searchable by streaming it to Elasticsearch The @searchable directive handles streaming the data of an @model object type to Amazon Elasticsearch Service and configures search resolvers that search that information. 123# Streams data from DynamoDB to Elasticsearch and exposes search capabilities.directive @searchable(queries: SearchableQueryMap) on OBJECTinput SearchableQueryMap { search: String } @versioned: Defines the versioning and conflict resolution strategy for an @model type The @versioned directive adds object versioning and conflict resolution to a type. Do not use this directive when leveraging DataStore as the conflict detection and resolution features are automatically handled inside AppSync and are incompatible with the @versioned directive. 1directive @versioned(versionField: String = &quot;version&quot;, versionInput: String = &quot;expectedVersion&quot;) on OBJECT Team EnvironmentFor multiple environments, Amplify matches the standard Git workflow where you switch between different branches using the env checkout command - similar to running git checkout BRANCHNAME, run amplify env checkout ENVIRONMENT_NAME to switch between environments. Modeling Relational Data in DynamoDBData access patterns123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220type Order @model # ----------------------------------------------------------- # 6. Show all open orders within a given date range across all customers # ----------------------------------------------------------- # The @key byCustomerByStatusByDate enables you to run a query that would work for this access pattern. # In this example, a composite sort key (combination of two or more keys) with the status and date is used. # ----------------------------------------------------------- # query getCustomerWithOrdersByStatusDate($customerID: ID!) { # getCustomer(id: $customerID) { # ordersByStatusDate (statusDate: { # between: [{ status: &quot;pending&quot;, date: &quot;2018-01-22&quot;},{ status: &quot;pending&quot;, date: &quot;2020-10-11&quot;}]}) # {items {}}}} # ----------------------------------------------------------- @key(name: &quot;byCustomerByStatusByDate&quot;, fields: [&quot;customerID&quot;, &quot;status&quot;, &quot;date&quot;]) # ----------------------------------------------------------- # 5. Get orders for a given customer within a given date range # ----------------------------------------------------------- # There is a one-to-many relation that lets all the orders of a customer be queried. # This relationship is created by having the @key name `byCustomerByDate` on the Order model # that is queried by the connection on the orders field of the Customer model. # ----------------------------------------------------------- # query getCustomerWithOrdersByDate($customerID: ID!) { # getCustomer(id: $customerID) { # ordersByDate(date: {between: [ &quot;2018-01-22&quot;, &quot;2020-10-11&quot; ]}) # {items {}}}} # ----------------------------------------------------------- @key(name: &quot;byCustomerByDate&quot;, fields: [&quot;customerID&quot;, &quot;date&quot;]) # ----------------------------------------------------------- # 12. Get orders by account representative and date # ----------------------------------------------------------- # As can be seen in the AccountRepresentative model # this connection uses the byRepresentativebyDate field on the Order model to create the connection needed. # ----------------------------------------------------------- # query getOrdersForAccountRepresentative($representativeId: ID!) { # getAccountRepresentative(id: $representativeId) { # id # orders(date: {between: [&quot;2010-01-22&quot;, &quot;2020-10-11&quot;]}) # {items {}}}} # ----------------------------------------------------------- @key(name: &quot;byRepresentativebyDate&quot;, fields: [&quot;accountRepresentativeID&quot;, &quot;date&quot;]) # ----------------------------------------------------------- # 9. Get all items on order for a given product # ----------------------------------------------------------- # This access-pattern would use a one-to-many relation from products to orders # With this query we can get all orders of a given product: # ----------------------------------------------------------- # query getProductOrders($productID: ID!) { # getProduct(id: $productID) { # id # orders {items {}}}} # ----------------------------------------------------------- @key(name: &quot;byProduct&quot;, fields: [&quot;productID&quot;, &quot;id&quot;]){ id: ID! customerID: ID! accountRepresentativeID: ID! productID: ID! status: String! amount: Int! date: String!}type Customer @model # ----------------------------------------------------------- # 11. Get customers by account representative # ----------------------------------------------------------- # This uses a one-to-many connection between account representatives and customers # ----------------------------------------------------------- # query getCustomersForAccountRepresentative($representativeId: ID!) { # getAccountRepresentative(id: $representativeId) { # customers # {items {}}}} # ----------------------------------------------------------- @key(name: &quot;byRepresentative&quot;, fields: [&quot;accountRepresentativeID&quot;, &quot;id&quot;]) { id: ID! name: String! phoneNumber: String accountRepresentativeID: ID! # 5. Get orders for a given customer within a given date range ordersByDate: [Order] @connection(keyName: &quot;byCustomerByDate&quot;, fields: [&quot;id&quot;]) # 6. Show all open orders within a given date range across all customers ordersByStatusDate: [Order] @connection(keyName: &quot;byCustomerByStatusByDate&quot;, fields: [&quot;id&quot;])}type Employee @model # ----------------------------------------------------------- # 7. See all employees hired recently # ----------------------------------------------------------- # Query by whether an employee has been hired recently # ----------------------------------------------------------- # query employeesNewHire { # employeesNewHire(newHire: &quot;true&quot;) # {items {}}} # ----------------------------------------------------------- @key(name: &quot;newHire&quot;, fields: [&quot;newHire&quot;, &quot;id&quot;], queryField: &quot;employeesNewHire&quot;) # ----------------------------------------------------------- # Query and have the results returned by start date # ----------------------------------------------------------- # query employeesNewHireByDate { # employeesNewHireByStartDate(newHire: &quot;true&quot;) # {items {}}} # ----------------------------------------------------------- @key(name: &quot;newHireByStartDate&quot;, fields: [&quot;newHire&quot;, &quot;startDate&quot;], queryField: &quot;employeesNewHireByStartDate&quot;) # ----------------------------------------------------------- # 2. Query employee details by employee name # ----------------------------------------------------------- # query employeeByName($name: String!) { # employeeByName(name: $name) {items {}}} # ----------------------------------------------------------- @key(name: &quot;byName&quot;, fields: [&quot;name&quot;, &quot;id&quot;], queryField: &quot;employeeByName&quot;) # ----------------------------------------------------------- # 14. Get all employees with a given job title # ----------------------------------------------------------- # Using the byTitle @key makes this access pattern quite easy # ----------------------------------------------------------- # query employeesByJobTitle { # employeesByJobTitle(jobTitle: &quot;Manager&quot;) # {items {}}} # ----------------------------------------------------------- @key(name: &quot;byTitle&quot;, fields: [&quot;jobTitle&quot;, &quot;id&quot;], queryField: &quot;employeesByJobTitle&quot;) # ----------------------------------------------------------- # 8. Find all employees working in a given warehouse # ----------------------------------------------------------- # This needs a one to many relationship from warehouses to employees # This connection uses the byWarehouse key on the Employee model. # ----------------------------------------------------------- # query getWarehouse($warehouseID: ID!) { # getWarehouse(id: $warehouseID) { # id # employees{items {}}}} # ----------------------------------------------------------- @key(name: &quot;byWarehouse&quot;, fields: [&quot;warehouseID&quot;, &quot;id&quot;]) { id: ID! name: String! startDate: String! phoneNumber: String! warehouseID: ID! jobTitle: String! newHire: String! # We have to use String type, because Boolean types cannot be sort keys}type Warehouse @model { id: ID! # 8. Find all employees working in a given warehouse employees: [Employee] @connection(keyName: &quot;byWarehouse&quot;, fields: [&quot;id&quot;])}type AccountRepresentative @model # ----------------------------------------------------------- # 17. Get sales representatives ranked by order total and sales period # ----------------------------------------------------------- # The sales period is either a date range or maybe even a month or week. # Therefore we can set the sales period as a string and query using the combination of salesPeriod and orderTotal. # We can also set the sortDirection in order to get the return values from largest to smallest # ----------------------------------------------------------- # query repsByPeriodAndTotal { # repsByPeriodAndTotal( # sortDirection: DESC, # salesPeriod: &quot;January 2019&quot;, # orderTotal: {ge: 1000}) # {items {}}} # ----------------------------------------------------------- @key(name: &quot;bySalesPeriodByOrderTotal&quot;, fields: [&quot;salesPeriod&quot;, &quot;orderTotal&quot;], queryField: &quot;repsByPeriodAndTotal&quot;) { id: ID! # 11. Get customers by account representative customers: [Customer] @connection(keyName: &quot;byRepresentative&quot;, fields: [&quot;id&quot;]) # 12. Get orders by account representative and date orders: [Order] @connection(keyName: &quot;byRepresentativebyDate&quot;, fields: [&quot;id&quot;]) orderTotal: Int salesPeriod: String}type Inventory @model # ----------------------------------------------------------- # 15. Get inventory by product by warehouse # ----------------------------------------------------------- # We can also get all inventory from an individual warehouse # by using the itemsByWarehouseID query created by the byWarehouseID key # ----------------------------------------------------------- # query byWarehouseId($warehouseID: ID!) { # itemsByWarehouseID(warehouseID: $warehouseID) { # items {}}} # ----------------------------------------------------------- @key(name: &quot;byWarehouseID&quot;, fields: [&quot;warehouseID&quot;], queryField: &quot;itemsByWarehouseID&quot;) # ----------------------------------------------------------- # 10. Get current inventories for a product at all warehouses # ----------------------------------------------------------- # The query needed to get the inventories of a product in all warehouses # ----------------------------------------------------------- # query getProductInventoryInfo($productID: ID!) { # getProduct(id: $productID) { # id # inventories {items {}}}} # ----------------------------------------------------------- # 15. Get inventory by product by warehouse # ----------------------------------------------------------- # Here having the inventories be held in a separate model is particularly useful # since this model can have its own partition key and sort key # such that the inventories themselves can be queried as is needed for this access-pattern. # ----------------------------------------------------------- # query inventoryByProductAndWarehouse($productID: ID!, $warehouseID: ID!) { # getInventory(productID: $productID, warehouseID: $warehouseID) { # productID # warehouseID # inventoryAmount}} # ----------------------------------------------------------- @key(fields: [&quot;productID&quot;, &quot;warehouseID&quot;]) { productID: ID! warehouseID: ID! inventoryAmount: Int!}type Product @model { id: ID! name: String! # 9. Get all items on order for a given product orders: [Order] @connection(keyName: &quot;byProduct&quot;, fields: [&quot;id&quot;]) # 10. Get current inventories for a product at all warehouses inventories: [Inventory] @connection(fields: [&quot;id&quot;])} AWS Lambda in PythonLambda deployment packagesYour AWS Lambda function’s code consists of scripts or compiled programs and their dependencies. You use a deployment package to deploy your function code to Lambda. Lambda supports two types of deployment packages: container images and .zip files. Container images .zip file archives AWS Lambda layers. Using other AWS services to build a deployment package","link":"/Blog/2021/01/10/Build-Modern-Serverless-Applications-with-AWS-Amplify-and-AppSync/"},{"title":"Constructing Topical Concept Hierarchical Taxonomy of Tourist Attraction","text":"Abstract A hierarchical co-clustering module by using non-negative matrix tri-factorization for allocating attractions and things of interest to topic when splitting a coarse topic into fine-grained ones. A concept extraction module for extracting concept of every topic that maintain strong discriminative power at different levels of the taxonomy. 理论数学表达Non-negative Matrix FactorizationThe model is to approximate the input attraction-ToI matrix with three factor matrices that assign cluster labels to tourist attractions and Things of Interest (ToI) simultaneously by solving the following optimization problem:$$\\min {\\mathbf{U}, \\mathbf{H}, \\mathbf{V} \\geq 0} \\mathcal{O}=\\left|\\mathbf{X}-\\mathbf{U H V}^{T}\\right|{F}^{2} \\\\mathbf{U}^{T} \\mathbf{U}=\\mathbf{I}, \\quad \\mathbf{V}^{T} \\mathbf{V}=\\mathbf{I}$$where $X $ is the input attraction-word content matrix, and $U ∈ R^{m×c}{+}$ and $V ∈ R^{n×c}{+}$ are orthogonal nonnegative matrices indicating low-dimensional representations of attractions and things of interest, respectively. The orthogonal and nonnegative conditions of the two matrices $U$ and $V$ enforce the model to provide a hard assignment of cluster label for attractions and things of interest. $H ∈ R^{c×c}_{+}$ provides a condensed view of $X$ . The optimization problem above is not convex with respect to the three variables $U$ , $H$ and $V$ together. There is no closed-form solution for the problem. Next, we use an alternative scheme to solve the optimization problem.$$\\mathbf{H}(i, j) \\leftarrow \\mathbf{H}(i, j) \\sqrt{\\frac{\\left[\\mathbf{U}^{T} \\mathbf{X V}\\right](i, j)}{\\left[\\mathbf{U}^{T} \\mathbf{U H V}^{T} \\mathbf{V}\\right](i, j)}}$$ $$\\mathbf{U}(i, j) \\leftarrow \\mathbf{U}(i, j) \\sqrt{\\frac{\\left[\\mathbf{X} \\mathbf{V} \\mathbf{H}^{T}\\right](i, j)}{\\left[\\mathbf{U} \\mathbf{H} \\mathbf{V}^{T} \\mathbf{V} \\mathbf{H}^{T}\\right](i, j)}}$$ $$\\mathbf{V}(i, j) \\leftarrow \\mathbf{V}(i, j) \\sqrt{\\frac{\\left[\\mathbf{X}^{T} \\mathbf{U} \\mathbf{H}\\right](i, j)}{\\left[\\mathbf{V} \\mathbf{H}^{T} \\mathbf{U}^{T} \\mathbf{U} \\mathbf{H}\\right](i, j)}}$$ This is 理论推导 link. Concept Extraction based on TextRank AlgorithmPageRankGraph-based ranking algorithms are essentially a way of deciding the importance of a vertex withina graph, based on global information recursively drawn from the entire graph. Formally, let $G=(V, E)$ be a directed graph with the set of vertices $V$ and set of edges $E$, where is a subset of $V \\times V$ . For a given vertex $\\boldsymbol{V}{i}$ , let $\\operatorname{In}\\left(V{i}\\right)$ be the set of vertices that point to it (predecessors), andlet $O u t\\left(V_{i}\\right)$ be the set of vertices that vertex $V_{i}$ points to (successors). The score of a vertex $V_{i}$ is defined asfollows (Brin and Page, 1998):$$S\\left(V_{i}\\right)=(1-d)+d * \\sum_{j \\in I n\\left(V_{i}\\right)} \\frac{1}{\\left|O u t\\left(V_{j}\\right)\\right|} S\\left(V_{j}\\right)$$where $d$ is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph. The factor $d$ is usually set to $0.85$ (Brin and Page, 1998), and this is the value we are also using in our implementation. TextRankHowever, the graphs are built from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text. It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices $\\boldsymbol{V}{i}$ and $\\boldsymbol{V}{j}$ as a weight $w_{i j}$ added to the corresponding edge that connects the two vertices.$$W S\\left(V_{i}\\right)=(1-d)+d * \\sum_{V_{j} \\in I n\\left(V_{i}\\right)} \\frac{w_{j i}}{\\sum_{V_{k} \\in O u t\\left(V_{j}\\right)} w_{j k}} W S\\left(V_{j}\\right)$$Concept Extraction 在这里，由于我们的模型已经提供了一个关于景点的层级结构，也就相当于对于每一个节点（除根节点以外），其先验知识就是其父节点的知识，也就是我们在抽取关键词作为节点概念的时候，一些在父节点就已经很出众的词语不应该成为子节点的概念。所以关于一个阶段来说，其概念抽取的时候应该选择在父子节点中概念重要程度增加最多的概念：$$Importance\\left(V_{i}\\right) = W S\\left(V_{i}\\right) - W S_{parent}\\left(V_{i}\\right)\\=d * [ \\sum_{V_{j} \\in I n\\left(V_{i}\\right)} \\frac{w_{j i}}{\\sum_{V_{k} \\in O u t\\left(V_{j}\\right)} w_{j k}} W S\\left(V_{j}\\right) - \\sum_{V_{j} \\in I n_{p}\\left(V_{i}\\right)} \\frac{(w_{j i}){p}}{\\sum{V_{k} \\in O u t_{p}\\left(V_{j}\\right)} (w_{j k}){p}} W S{p}\\left(V_{j}\\right)]$$ 对树生成的约束 预剪枝 定义一个高度，当树达到该高度时就停止树的生长（Max Level） 定义一个阈值，当达到某个节点的实例个数小于阈值时就可以停止树的生长 后剪枝 定义一个目标函数，由损失项 (Loss term) 加上正则项 (Regularization term)，以最小化结构性风险(Structural risk) 每个景点由一个向量表示，向量由词袋模型生成，所以两个景点向量之间的距离可以表示成：$$\\operatorname{dist}(X, Y)=\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}}$$ 我们用景点和簇中心向量距离的和来定义这个簇的混乱程度：$$H(X)= \\sum_{i=1}^{n}dist(X_{i}-\\hat{X})$$ 因为每一个节点中景点向量的向量空间是变化的，所以直接用决策树的剪枝策略是没有意义的，这里收到决策树代价复杂度剪枝方法的启发，做了一些调整： 因为节点之间的景点向量空间是不断变化的，所以在考虑剪枝的时候仅考虑本节点是否应该继续分割而不考虑继续分割后再进行分割而产生的变化，构建如下的损失函数： $$C_{\\alpha} (T)=\\frac{1}{N} \\sum_{t=1}^{|T|} N_{t} H_{t}+\\alpha(|T|+1)$$ $|T|$：该节点生成的簇个数；$N$：样本总数；$N_t$：第 t 个簇中的样本数量；$H_t$：第 t 个簇的混乱程度；$\\alpha$：惩罚因子 对于每一个节点，聚类后：$$C_{\\alpha}(t)=H(t)+\\alpha$$若 $C_{\\alpha}(t) &lt;= C_{\\alpha} (T)$ 则此次聚类没有意义，则不需要进行这次聚类。 补充节点 X 矩阵的构建 TF-IDF 过滤 已知是属于这个节点的景点，我们将这些景点的每一条评论当做是一篇文档，对文档进行分词，然后对每一篇文档内的词计算 tf-idf 值，并且对 tf 值和 idf 值做一个上下界的阈值限制，将所有评论中剩下的词用做词袋模型。 布尔矩阵 我们尝试过直接用 tf-idf 值做 X 矩阵，也尝试了用布尔值做 X 矩阵，也尝试了用词频做 X 矩阵。最终发现 tf-idf 值做矩阵与用词频做矩阵效果都不理想，用布尔值（出现过这个词就为 1，没有出现过就为 0 ）做矩阵效果是最理想的。 节点的迭代方法因为每个景点属于 k 个类别的概率最后和都为 1，也就是矩阵分解出来的结果无法直接识别噪音（噪音型景点），所以我们采取的一个办法首先定义噪音型景点，然后迭代性的删除直到不存在噪音型景点为止。 我们认为噪音型景点的两个特征，一个是类别的归属度不高，一个是景点出现的特征词（词袋模型中的词）少。如果一个景点包含这两个特征，就判断其是噪音型的景点。 然后节点进行矩阵分解之后对结果进行分析，如果存在噪音型的景点，那么就需要将这些景点从这个节点的景点集合中删除，并且重新构建本节点的 attraction-toi 矩阵，然后重新进行矩阵分解。 重复以上的操作直到本节点不存在噪音。 AlgorithmAlgorithm 1: Hierarchical Clustering for topic splitting Input: The dataset $D$ of topic $C$ ; the number of sub-topics $K$; threshold $N$. Output: Hierarchical topics $Recursion \\ function$ &emsp;&emsp;$X \\leftarrow Prepare\\ Matrix (C, D)$ &emsp;&emsp;$S_1 , S_2 , … , S_K \\leftarrow Clustering \\ for \\ Topic \\ Splitting(X)$ &emsp;&emsp;$loss \\leftarrow Post \\ Pruning \\ Function(X, S_1 , S_2 , … , S_K)$ &emsp;&emsp;if $loss &gt; 0$ then return \\\\停止条件之后剪枝 &emsp;&emsp;for $K \\ from \\ 1 \\ to \\ K$ do &emsp;&emsp;&emsp;&emsp;$n \\leftarrow item \\ number \\ of \\ C$ &emsp;&emsp;&emsp;&emsp; if $n&lt;N$ then continue \\\\停止条件之预剪枝 &emsp;&emsp;&emsp;&emsp;$CE_k \\leftarrow Concept \\ Extraction(C, S_k)$ &emsp;&emsp;&emsp;&emsp;$Recursion \\ Function (S_k)$ &emsp;&emsp;Return $CE_1, CE_2，… , CE_k, S_1,S_2,…,S_k$ Algorithm 2: Clustering for topic splitting **Input: **A matrix $X$ of parent topic $C$ ; the number of sub-topics $K$; threshold $δ$ and $\\gamma$. Output: $K$ sub-topics of $C$ $C_{sub}$ $\\leftarrow$ $C$ ; while True do &emsp;&emsp;$ S_1, S_2, … , S_K \\leftarrow Nonnegative \\ Matrix \\ Factorization (S_{sub}, K)$ &emsp;&emsp;for $k$ from $1$ to $K$ do &emsp;&emsp;&emsp;&emsp;for t ∈ S_k do &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$r( t, S_k ) ← probability\\ of \\ term \\ t \\ for \\ S_k ;$ &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$d( t, S_k ) ← document \\ frequency \\ of \\ term \\ t \\ for \\ S_k ;$ &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;if $r(t,S k ) &lt; δ$ and $d( t, S_k ) &lt; \\gamma$ then &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$S_k ← S_k − {t};$ &emsp;&emsp;$C^{‘}_{sub} \\leftarrow S_1 ∪ S_2 ∪ … ∪ S_K;$ &emsp;&emsp;if $C^{‘}{sub} = C{sub}$ then &emsp;&emsp;&emsp;&emsp;$Break;$ &emsp;&emsp;$C^{‘}{sub} \\leftarrow C{sub}$Return $S_1 , S_2 , … , S_K;$ Algorithm 3: Non-negative Matrix Factorization Input: ${X,U_0,V_0,T}$ Output: ${U,V}$ Initialize $U = U_0 ,V = V_0 ,H ≥ 0$ while $Not convergent$ and $t ≤ T$ do &emsp;&emsp;Update$$\\mathbf{H}(i, j) \\leftarrow \\mathbf{H}(i, j) \\sqrt{\\frac{\\left[\\mathbf{U}^{T} \\mathbf{X V}\\right](i, j)}{\\left[\\mathbf{U}^{T} \\mathbf{U H V}^{T} \\mathbf{V}\\right](i, j)}}$$&emsp;&emsp;Update$$\\mathbf{U}(i, j) \\leftarrow \\mathbf{U}(i, j) \\sqrt{\\frac{\\left[\\mathbf{X} \\mathbf{V} \\mathbf{H}^{T}\\right](i, j)}{\\left[\\mathbf{U} \\mathbf{H} \\mathbf{V}^{T} \\mathbf{V} \\mathbf{H}^{T}\\right](i, j)}}$$ &emsp;&emsp;Update$$\\mathbf{V}(i, j) \\leftarrow \\mathbf{V}(i, j) \\sqrt{\\frac{\\left[\\mathbf{X}^{T} \\mathbf{U} \\mathbf{H}\\right](i, j)}{\\left[\\mathbf{V} \\mathbf{H}^{T} \\mathbf{U}^{T} \\mathbf{U} \\mathbf{H}\\right](i, j)}}$$&emsp;&emsp;$t=t+1$ end while Algorithm 4: Concept Extraction based on TextRank Algorithm Identify text units that best define the node and its parent node, and add them as vertices in the graph, respectively. Identify relations that connect such text units, and use these relations to draw edges between verticesin the graph. Iterate the graph-based ranking algorithm until convergence. The intersection of the two graph vertices is the set of concept words, and the final score of the word is the score difference between the two graphs. Sort concept words based on their final score. Use the values attached to each concept word for ranking/selection decisions.","link":"/Blog/2019/06/06/Constructing-Topical-Concept-Hierarchical-Taxonomy-of-Tourist-Attraction/"},{"title":"Apache Spark: the New ‘king’ of Big Data","text":"[TOC] AboutWhat is Hadoop?It’s a general-purpose form of distributed processing that has several components: HDFS(The Hadoop Distributed File System), which stores files in a Hadoop-native format and parallelizes them across a cluster; YARN, a schedule that coordinates application runtimes; MapReduce, the algorithm that actually processes the data in parallel. Hadoop is built in Java, and accessible through many programming languages, for writing MapReduce code, including Python, through a Thrift client. What is Apache Spark?Spark as a whole consists of various libraries, APIs, databases, etc. The main components of Apache Spark are as follows: **Spark Core: **Spare Core is the basic building block of Spark, which includes all components for job scheduling, performing various memory operations, fault tolerance, and more. Spark Core is also home to the API that consists of RDD. Moreover, Spark Core provides APIs for building and manipulating data in RDD. **Spark SQL: **Apache Spark works with the unstructured data using its ‘go to’ tool, Spark SQL. Spark SQL allows querying data via SQL, as well as via Apache Hive’s form of SQL called Hive Query Language (HQL). It also supports data from various sources like parse tables, log files, JSON, etc. Spark SQL allows programmers to combine SQL queries with programmable changes or manipulations supported by RDD in Python, Java, Scala, and R. **Spark Streaming: **Spark Streaming processes live streams of data. Data generated by various sources is processed at the very instant by Spark Streaming. Examples of this data include log files, messages containing status updates posted by users, etc. **GraphX: **GraphX is Apache Spark’s library for enhancing graphs and enabling graph-parallel computation. Apache Spark includes a number of graph algorithms which help users in simplifying graph analytics. **MLlib: **Apache Spark comes up with a library containing common Machine Learning (ML) services called MLlib. It provides various types of ML algorithms including regression, clustering, and classification, which can perform various operations on data to get meaningful insights out of it. Spark has several APIs. The original interface was written in Scala, and based on heavy usage by data scientists, Python and R endpoints were also added. Java is another option for writing Spark jobs. Apache Spark vs Hadoop vs HiveThat’s not to say Hadoop is obsolete. It does things that Spark does not, and often provides the framework upon which Spark works. The Hadoop Distributed File System enables the service to store and index files, serving as a virtual data infrastructure. Spark, on the other hand, performs distributed, high-speed compute functions on that architecture. If Hadoop is the professional kitchen with the tools and equipment to build and cook meals of data, then Spark is the expediter that rapidly assembles and distributes those meals for consumption. Because Spark was built to work with and run on the Hadoop infrastructure, the two systems work well together. Fast-growing organizations built in Hadoop can easily add Spark’s speed and functionality as needed. As for the different between hive and Spark, they are different products built for different purposes in the big data space. Hive is a distributed database, and Spark is a framework for data analytics. Hive is a pure data warehousing database which stores data in the form of tables. As a result, it can only process structured data read and written using SQL queries. Hive is not an option for unstructured data. In addition, Hive is not an ideal for OLTP or OLAP kinds of operations. Key Terminology and Concepts Spark RDDs Resilient Distributed Datasets are data structures that are the core building blocks of Spark. A RDD is an immutable, partitioned collection of records, which means that it can hold values, tuples, or other objects, these records are partitioned so as to be processed on a distributed system, and that once an RDD has been made, it is impossible to alter it. Spark DataFrame have all of the features of RDDs but also have a schema. This will make them our data structure of choice for getting started with PySpark. Spark DataSets are similar to DataFrames but are strongly-typed, meaning that the type is specified upon the creation of the DataSet and is not inferred from the type of records stored in it. This means DataSets are not used in PySpark because Python is a dynamically-typed language. Transformations Transformations are one of the things you can do to an RDD in Spark. Transformations take an RDD as an input and perform some function on them based on what Transformation is being called, and outputs one or more RDDs. Actions An Action is any RDD operation that does not produce an RDD as an output. Aan Action is the cue to the compiler to evaluate the lineage graph and return the value specified by the Action. Lineage Graph A lineage graph outlines what is called a “logical execution plan”. What that means is that the compiler begins with the earliest RDDs that aren’t dependent on any other RDDs, and follows a logical chain of Transformations until it ends with the RDD that an Action is called on. This feature is primarily what drives Spark’s fault tolerance. If a node fails for some reason, all the information about what that node was supposed to be doing is stored in the lineage graph, which can be replicated elsewhere. Application A Spark application is a user built program that consists of a driver and that driver’s associated executors. Job A Spark job is task or set of tasks to be executed with executor processes, as directed by the driver. A job is triggered by the calling of an RDD Action. Map Stage in Map Reduce The map or mapper’s job is to process the input data. Generally the input data is in the form of file or directory and is stored in the Hadoop file system (HDFS). The input file is passed to the mapper function line by line. The mapper processes the data and creates several small chunks of data. Reduce Stage in Map Reduce This stage is the combination of the Shuffle stage and the Reduce stage. The Reducer’s job is to process the data that comes from the mapper. After processing, it produces a new set of output, which will be stored in the HDFS. Apache Spark ArchitectureFeatures of Apache Spark Speed Spark runs up to 100 times faster than Hadoop MapReduce for large-scale data processing. It is also able to achieve this speed through controlled partitioning. Powerful Caching Simple programming layer provides powerful caching and disk persistence capabilities. Deployment It can be deployed through Mesos, Hadoop via YARN, or Spark’s own cluster manager. Real-TimeIt offers Real-time computation &amp; low latency because of in-memory computation. Polyglot Spark provides high-level APIs in Java, Scala, Python, and R. Spark code can be written in any of these four languages. It also provides a shell in Scala and Python. Spark Architecture OverviewApache Spark has a well-defined layered architecture where all the spark components and layers are loosely coupled. This architecture is further integrated with various extensions and libraries. Apache Spark Architecture is based on two main abstractions: Resilient Distributed Dataset (RDD) Directed Acyclic Graph (DAG) Resilient Distributed Dataset(RDD)RDDs are the building blocks of any Spark application. RDDs Stands for: Resilient: Fault tolerant and is capable of rebuilding data on failure Distributed: Distributed data among the multiple nodes in a cluster Dataset: Collection of partitioned data with values Using Hadoop and Spark togetherThere are several instances where you would want to use the two tools together. Despite some asking if Spark will replace Hadoop entirely because of the former’s processing power, they are meant to complement each other rather than compete. Below you can see a simplified version of Spark-and-Hadoop architecture: Hadoop can—at a lower price—deal with heavier operations while Spark processes the more numerous smaller jobs that need instantaneous turnaround. YARN also makes archiving and analysis of archived data possible, whereas it isn’t with Apache Spark. Thus, Hadoop and YARN in particular becomes a critical thread for tying together the real-time processing, machine learning and reiterated graph processing. Summing it upSo is it Hadoop or Spark? These systems are two of the most prominent distributed systems for processing data on the market today. Hadoop is used mainly for disk-heavy operations with the MapReduce paradigm, and Spark is a more flexible, but more costly in-memory processing architecture. Both are Apache top-level projects, are often used together, and have similarities, but it’s important to understand the features of each when deciding to implement them. So is it Spark or Hive? Hive and Spark are both immensely popular tools in the big data world. Hive is the best option for performing data analytics on large volumes of data using SQLs. Spark, on the other hand, is the best option for running big data analytics. It provides a faster, more modern alternative to MapReduce. ML PipelinesML Pipelines provide a uniform set of high-level APIs built on top of DataFrames that help users create and tune practical machine learning pipelines. Main concepts in Pipelines DataFrame: This ML API uses DataFrame from Spark SQL as an ML dataset, which can hold a variety of data types. E.g., a DataFrame could have different columns storing text, feature vectors, true labels, and predictions. Transformer: A Transformer is an algorithm which can transform one DataFrame into another DataFrame. E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions. Estimator: An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model. Pipeline: A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow. Parameter: All Transformers and Estimators now share a common API for specifying parameters. How it works The first two (Tokenizer and HashingTF) are Transformers (blue), and the third (LogisticRegression) is an Estimator (red). The bottom row represents data flowing through the pipeline, where cylinders indicate DataFrames. The Pipeline.fit() method is called on the original DataFrame, which has raw text documents and labels. The Tokenizer.transform() method splits the raw text documents into words, adding a new column with words to the DataFrame. The HashingTF.transform() method converts the words column into feature vectors, adding a new column with those vectors to the DataFrame. Now, since LogisticRegression is an Estimator, the Pipeline first calls LogisticRegression.fit() to produce a LogisticRegressionModel. If the Pipeline had more Estimators, it would call the LogisticRegressionModel’s transform() method on the DataFrame before passing the DataFrame to the next stage. A Pipeline is an Estimator. Thus, after a Pipeline’s fit() method runs, it produces a PipelineModel, which is a Transformer. This PipelineModel is used at test time; the figure below illustrates this usage. In the figure above, the PipelineModel has the same number of stages as the original Pipeline, but all Estimators in the original Pipeline have become Transformers. When the PipelineModel’s transform() method is called on a test dataset, the data are passed through the fitted pipeline in order. Each stage’s transform() method updates the dataset and passes it to the next stage. Pipelines and PipelineModels help to ensure that training and test data go through identical feature processing steps. Reference A Neanderthal’s Guide to Apache Spark in Python Apache Spark Architecture – Spark Cluster Architecture Explained How do Hadoop and Spark Stack Up? MLlib: ML Pipelines","link":"/Blog/2020/12/09/Apache-Spark-the-new-king-of-Big-Data/"},{"title":"CI&#x2F;CD Deployment with AWS SAM using Github Action","text":"**Data Science and Machine Learning are surely some fast-moving industries and somewhat need you to study at all times to stay ahead and on top in the industry. But the first step of getting into this area seems dreadfully slow due to widely involved technologies and overwhelming terminologies that scare you out of shit. ** AWS lowers the barrier to entry for companies and organizations looking for solutions of leveraging ML capabilities by offerings more than 20 services including low-level service like SageMaker, which helps build and manage infrastructure for developing environments, as well as high-level systems like Rekognition that come with pre-built Machine Learning models for image recognition. This blog will go through nearly all the Machine Learning services offered by AWS. ToolsSageMakerSageMaker is the most important microservices set in AWS. It’s a set of tools for deploying machine learning applications. it streamlines all the Machine Learning tasks that come up from preparing data and building a model to training, and deploying it. Also, The benefits of SageMaker have to do with all the details of how to stage training tasks and deploy inference tasks across a variety of infrastructures. SageMaker is so powerful that can not be just compressed into one section to introduce. So, I’m decided to use another article to illustrate it in full detail. Please stay tuned. Development ToolsThe critical blockers for traditional developers to become Cloud practitioners are all kinds of new cloud development patterns, including new Cloud IDE, new backend design patterns, and new operation standards. From that end, AWS provides several development tools to ensure the working environment, and smooth the transition experience. Machine Learning plays a vital role in this task. Amazon CodeGuru is a developer tool that provides intelligent recommendations to improve code quality and identify an application’s most expensive lines of code. Integrate CodeGuru into your existing software development workflow to automate code reviews during application development and continuously monitor application’s performance in production and provide recommendations and visual clues on how to improve code quality, application performance, and reduce overall cost. CodeGuru Reviewer uses machine learning and automated reasoning to identify critical issues, security vulnerabilities, and hard-to-find bugs during application development and provides recommendations to improve code quality. CodeGuru Profiler helps developers find an application’s most expensive lines of code by helping them understand the runtime behavior of their applications, identify and remove code inefficiencies, improve performance, and significantly decrease compute costs. Amazon DevOps Guru is a service powered by machine learning (ML) that is designed to make it easy to improve an application’s operational performance and availability. DevOps Guru helps detect behaviors that deviate from normal operating patterns so you can identify operational issues long before they impact your customers. When DevOps Guru identifies a critical issue, it automatically sends an alert and provides a summary of related anomalies, the likely root cause, and context for when and where the issue occurred. Tools for Text MiningText is important for human society and it is also more difficult and more tricky to process than other standard machine learning tasks like numerical classification. So, AWS provides a wide range of tools for NLP (natural language processing), GLU (natural language understanding), as well as NLG (natural language generation). With all kinds of text mining appliances, you can do sentiment analysis, machine translation, also speech recording. They are also accommodating if you want to build conversational user interfaces or summarize texts. Amazon Comprehend for natural language processing and text analytics helps you understand text sentiment and relate texts to each other. Amazon Lex is a service for building conversational interfaces using voice and text. With Lex, you can use the same deep learning engine that powers Alexa in your own applications. Amazon Textract extracts text and data from scanned documents. It’s not just OCR but backed by Machine Learning models that have analyzed many types of documents, and can identify the contents of fields in forms and information stored in tables. Amazon Transcribe could be used to turn any speech recording into a text. If you need to go the other way around, Amazon Polly will synthesize lifelike speech from any text. Amazon Translate caters to your multilingual needs by translating every text into the language of your choice. Amazon Polly is a service that turns text into lifelike speech. Polly lets you create applications that talk, enabling you to build entirely new categories of speech-enabled products. Polly is an Amazon artificial intelligence (AI) service that uses advanced deep learning technologies to synthesize speech that sounds like a human voice. Tools for Image and VideoThe ability to verify, organize, analysis millions and tons of images will unlock a whole new set of possibilities. Amazon Rekognition offers pre-trained and customizable computer vision (CV) capabilities to extract information and insights from your images and videos. From face detection to text extraction. AWS Panorama is a machine learning (ML) appliance and software development kit (SDK) that brings CV to on-premises internet protocol (IP) cameras. Manufacturing CapabilitiesAmazon Lookout for Vision is a machine learning (ML) service that spots defects and anomalies in visual representations using computer vision (CV). With Amazon Lookout for Vision, manufacturing companies can increase quality and reduce operational costs by quickly identifying differences in images of objects at scale. Amazon Lookout for Equipment analyzes the data from the sensors on your equipment (e.g. pressure in a generator, flow rate of a compressor, revolutions per minute of fans), to automatically train a machine learning model based on just your data, for your equipment – with no ML expertise required. Amazon Lookout for Metrics uses machine learning (ML) to automatically detect and diagnose anomalies in business and operational data, such as a sudden dip in sales revenue or customer acquisition rates. Amazon Monitron is an end-to-end system that uses machine learning (ML) to detect abnormal behavior in industrial machinery, enabling you to implement predictive maintenance and reduce unplanned downtime. Tools for Low Level Scientific EnvironmentTensorFlow is one of many deep learning frameworks available to researchers and developers to enhance their applications with machine learning. AWS provides broad support for TensorFlow, enabling customers to develop and serve their own models across computer vision, natural language processing, speech translation, and more. Amazon Elastic Inference allows you to attach low-cost GPU-powered acceleration to Amazon EC2 and Amazon SageMaker instances to reduce the cost of running deep learning inference by up to 75%. Amazon Elastic Inference supports TensorFlow, Apache MXNet, PyTorch, and ONNX models. AWS Inferentia is a machine learning inference chip designed to deliver high performance at low cost. AWS Inferentia will support the TensorFlow, Apache MXNet, and PyTorch deep learning frameworks, as well as models that use the ONNX format. Other ToolsAmazon Augmented AI (Amazon A2I) is a machine learning service that makes it easy to build the workflows required for human review. Amazon Fraud Detector is a fully managed service that uses machine learning (ML) and more than 20 years of fraud detection expertise from Amazon, to identify potentially fraudulent activity so customers can catch more online fraud faster. Amazon HealthLake is a HIPAA-eligible service that healthcare providers, health insurance companies, and pharmaceutical companies can use to store, transform, query, and analyze large-scale health data. Amazon Kendra is an intelligent search service powered by machine learning. Kendra reimagines enterprise search for your websites and applications so your employees and customers can easily find the content they are looking for, even when it’s scattered across multiple locations and content repositories within your organization. Amazon Personalize is a machine learning service that makes it easy for developers to create individualized recommendations for customers using their applications.Amazon Personalize is like having your own Amazon.com machine learning personalization team at your disposal, 24 hours a day. AWS DeepRacer is a 1/18th scale race car which gives you an interesting and fun way to get started with reinforcement learning (RL). RL is an advanced machine learning (ML) technique which takes a very different approach to training models than other machine learning methods. References Summary AWS Machine Learning Tools (2022 edition) Overview of AWS : Machine learning Services| AWS White Paper Summary AWS re:invent 2021 AI &amp; Machine Learning Launches: 7 Things You Should Know Amazon SageMaker AWS Announces Six New Amazon SageMaker Capabilities Building a customized recommender system in Amazon SageMaker Train ALBERT for natural language processing with TensorFlow on Amazon SageMaker Serverless Machine Learning with AWS Lambda 5 Must Have AWS Serverless Tools for your Starter Kit Ultimate Guide to Monitoring Serverless Applications","link":"/Blog/2022/02/16/CICD-Deployment-with-AWS-SAM-using-Github-Action/"},{"title":"Deploying AWS Lambda with Terraform","text":"Serverless is a hot topic in Cloud, so as Infrastructure as Code(IaC). Infrastructure as code (IaC) tools allow you to manage infrastructure with configuration files rather than through a graphical user interface. IaC allows you to build, change, and manage your infrastructure in a safe, consistent, and repeatable way by defining resource configurations that you can version, reuse, and share. It’s quite easy to get used to Terraform if you are familiar with CloudFormation as they all being tools to implement infrastructure as code on Cloud Provider, such as AWS. It’s a cornerstone of DevOps, designed to boost the agility, productivity and quality of work within organizations. This article will cover: the basic components of deploying lambda, and related step to set up a lambda through Terraform. A series article will be published to cover several topic including: Difference between Terraform and CloudFormation Workflow of Setting up Schedule Lambda to Sync Data Between two s3 Bucket. Automate Terraform with GitHub Actions Use PGP to Encrypt Your Terraform Secrets Deploy a Lambda Function with TerraformLet’s deploy a simple lambda function to AWS using Terraform. Talk is cheap, let’s show you the code. Prerequisite Install Terraform CLI Install it through Terraform’s website. Check the version of it to ensure it works using terraform --version Creating AWS IAM User from console Ensure IAM user has programmatic access. Giving it Administrator access permission to ensure followed resources creation. Main StepsSource Code NeededFor this project, let’s create a directory named src to store is source code. Create file hello.py ,which contain followed code: 12345678def lambda_handler(event, context): &quot;&quot;&quot; :param event: :param context: :return: &quot;&quot;&quot; print('Event receive: ', event) return &quot;Mission Completed!&quot; Define VariablesThe next step is to define all the variables in var.tf, all of them can be hard code in other resources definition, but hard to manage them and not to mention reuse them. For that end, variable is suitable to be extracted and define in separate way. 123456789101112131415161718192021222324252627282930variable &quot;function_name&quot; { default = &quot;hello&quot;}variable &quot;source_folder&quot; { default = &quot;data/&quot;}variable &quot;source_directory&quot; { type = string default = &quot;./src&quot;}variable &quot;lambda_runtime&quot; { type = string description = &quot;Lambda Parameter - Runtime. E.x. python3.6&quot; default = &quot;python3.8&quot;}variable &quot;lambda_handler&quot; { type = string description = &quot;Lambda Parameter - Handler reference, e.x. index.lambda_handler&quot; default = &quot;hello.lambda_handler&quot;}variable &quot;builds_dir&quot; { type = string description = &quot;The directory where the lambda zip should be built&quot; default = &quot;lambda_function_payload.zip&quot;} Data DefinitionFor the lambda function, it needs proper permission to perform manipulation of other sources and it may have multiple permission statement and sometimes hard to read if policies are directly put in main file. Below is a data sample to give lambda function permission to list all objects in a s3 bucket. And also, AWS allow zip file to be uploaded for lambda function, that’s why data type archive_file is needed and it point to the builds directory for zip file, and need to know which folder is the source code folder. 12345678910111213141516data &quot;aws_iam_policy_document&quot; &quot;lambda_permissions&quot; { statement { actions = [ &quot;s3:ListBucket&quot;, ] resources = [ &quot;arn:aws:s3:::${var.target_bucket}&quot;, ] }}data &quot;archive_file&quot; &quot;zip_the_python_code&quot; { type = &quot;zip&quot; source_dir = var.source_directory output_path = var.builds_dir} Lambda DefinitionFinally, we get to define the related resources in lambda.tf, it contains three resources: lambda_s3_policy: A policy to consume predefined permission in data.tf iam_role_for_lambda: An IAM role for lambda to consume role and also attach above policy to the role. schedule_lambda: Lambda function with many definitions: filename point to the zip file function_name need no explanation runtime define the version and explainer of the function, such as python3.7 handler take two input (event and context) when function is invoked. For this project hello is the file name and lambda_handler is the method name. Predefined role is assigned to the function source_code_hash is aim to detect code change and update when needed since hash will change when the source code change. environment define the variables for function. 1234567891011121314151617181920212223242526272829303132333435resource &quot;aws_iam_policy&quot; &quot;lambda_s3_policy&quot; { policy = data.aws_iam_policy_document.lambda_permissions.json}resource &quot;aws_iam_role&quot; &quot;iam_role_for_lambda&quot; { name = &quot;iam_role_for_lambda&quot; assume_role_policy = jsonencode({ &quot;Version&quot; : &quot;2012-10-17&quot;, &quot;Statement&quot; : [ { &quot;Action&quot; : &quot;sts:AssumeRole&quot;, &quot;Principal&quot; : { &quot;Service&quot; : &quot;lambda.amazonaws.com&quot; }, &quot;Effect&quot; : &quot;Allow&quot;, &quot;Sid&quot; : &quot;&quot; } ] }) managed_policy_arns = [aws_iam_policy.lambda_s3_policy.arn]}resource &quot;aws_lambda_function&quot; &quot;meta_schedule_lambda&quot; { filename = var.builds_dir function_name = var.function_name handler = var.lambda_handler runtime = var.lambda_runtime role = aws_iam_role.iam_role_for_lambda.arn source_code_hash = data.archive_file.zip_the_python_code.output_base64sha256 environment { variables = { TARGET_BUCKET = var.target_bucket } }} Provider Informationprovider.tf should be noted that the region in which S3 bucket is created, same region should be entered above. 123provider &quot;aws&quot; { region = &quot;us-east-1&quot;} Run Command LineNow we almost done all the code work. And run some command to deploy: terraform init to download all the required plugins. terraform plan to check the integrity of the code. terraform apply to deploy all the resources into AWS cloud. Also, terraform apply -auto-approve could spare you from manually input yes during the procedure to do the confirmation. Well done! And all we need to do is to check whether resources have been created through AWS Console. Terraform Lambda ResourcesThere are a lot resources in Terraform to provision lambda function and related resources such as attached policies. aws_lambda_functionA Lambda function needs code and an IAM role to run a function. Code is deployed on an S3 bucket as a deployment package (zip file). 1234567891011121314resource &quot;aws_lambda_function&quot; &quot;sample_lambda&quot; { filename = &quot;lambda_function_payload.zip&quot; function_name = &quot;lambda_terraform_function_name&quot; role = aws_iam_role.iam_role_for_lambda.arn handler = &quot;data.test&quot; source_code_hash = filebase64sha256(&quot;lambda_function_payload.zip&quot;) runtime = &quot;nodejs12.x&quot; environment { variables = { foo = &quot;bar&quot; } } }} A single resource of aws_lambda_function is not enough, because no permission is granted by default, so it has to be configured with a execution role to allow the function to assume role to get permission it need. 123456789101112131415161718resource &quot;aws_iam_role&quot; &quot;iam_role_for_lambda&quot; {name = &quot;iam_role_for_lambda&quot;assume_role_policy = &lt;&lt;EOF { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Action&quot;: &quot;sts:AssumeRole&quot;, &quot;Principal&quot;: { &quot;Service&quot;: &quot;lambda.amazonaws.com&quot; }, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Sid&quot;: &quot;&quot; } ] }EOF} aws_lambda_aliasResource aws_lambda_alias provide clients with a function identifier that you can update to invoke a different version. It also useful when a lambda function still in test stage and invocation could be split and route to different version of function. 1234567891011resource &quot;aws_lambda_alias&quot; &quot;test_lambda_alias&quot; { name = &quot;my_alias&quot; description = &quot;a sample description&quot; function_name = aws_lambda_function.lambda_function_test.arn function_version = &quot;1&quot; routing_config { additional_version_weights = { &quot;2&quot; = 0.5 } }} aws_lambda_function_event_invoke_configManages an asynchronous invocation configuration for a Lambda Function or Alias. Such as destination configuration: 1234567891011resource &quot;aws_lambda_function_event_invoke_config&quot; &quot;example&quot; { function_name = aws_lambda_alias.example.function_name destination_config { on_failure { destination = aws_sqs_queue.example.arn } on_success { destination = aws_sns_topic.example.arn } }} aws_lambda_layer_versionThis provides a Lambda Layer Version resource. Lambda Layers allow you share the reusable code through layers across multiple Lambda functions. 12345resource &quot;aws_lambda_layer_version&quot; &quot;lambda_nodejs_layer&quot; { filename = &quot;lambda_nodejs_layer_payload.zip&quot; layer_name = &quot;lambda_layer_nodejs&quot; compatible_runtimes = [&quot;nodejs12.0&quot;]} aws_lambda_permissionThis resource provides other AWS services, such as S3 and DynamoDB, access to the Lambda function. 12345678resource &quot;aws_lambda_permission&quot; &quot;allow_s3&quot; { statement_id = &quot;AllowExecutionFromS3&quot; action = &quot;lambda:InvokeFunction&quot; function_name = aws_lambda_function.s3_lambda.function_name principal = &quot;events.amazonaws.com&quot; source_arn = &quot;arn:aws:events:ap-east-2:121112424343:rule/RunDaily&quot; qualifier = aws_lambda_alias.s3_alias.name} References Terraform: Beyond the Basics with AWS","link":"/Blog/2022/04/11/Deploying-AWS-Lambda-with-Terraform/"},{"title":"Convolutional Neural Networks","text":"Attention 本文适合已经对向后传播（Backpropagation）神经网络有所了解的同学进一步学习卷积神经网络（CNN），感到困难的同学可以自行学习BP后再阅读。 This article is suitable for students who are already familiar with Backpropagation Neural Networks to further study Convolutional Neural Networks (CNN). Students who find it difficult can learn BP on their own before reading. Outline:1.卷积（Convolution） 什么是卷积 卷积为什么能够提取text，image中的特征 在NLP中直观的理解卷积 卷积的意义 2.池化（Pooling） 什么是池化 几种池化的方法 池化的意义 卷积（Convolution）什么是卷积我们现在有$f(x)和g(x)$这两个函数，卷积是发生在这两个函数之间的计算。 卷积的数学定义如下： 连续定义：$$（f*g）(n)=\\int_{-\\infty}^{\\infty}f(\\tau)g(n-\\tau)d\\tau$$离散定义：$$(f * g)(n)=\\sum_{\\tau=_\\infty}^{\\infty}f(\\tau)g(n-\\tau)$$ 如果是低年级的同学（就像我）没有接触过卷积一定会很困扰，这个看起来很复杂的东西到底是什么？$n,\\tau$有没有什么特别的意义？（实际上就是个符号，在没有情景时没有现实意义）。关于公式的来源这里不赘述，同学们也不必纠结（以后有些课程中会讲到）。我们定义出这个公式，是因为其在现实中有着广泛的应用，本文着重从现实意义解释卷积。 我们可以观察到求和时$f,g$的自变量取值是一个定值$\\tau+(n-\\tau)=n$（$n$由我们想求解何处卷积而决定），这就是理解卷积的关键。卷积实在将两个函数相乘，再使他们的自变量和为常数后求积分。下面举几个例子。 信号处理$f(t)$表示0-20时间内的离散信号输入,$g(t)$表示系统对一个输入信号响应随时间的衰减。在这个每隔一秒输入一个信号地系统，其某时刻地信号强度和之前输入地信号均有关(叠加的)。比如在$t=8$时，$t=0$输入的信号就衰减成$f(0)g(10)$其他的都以此类推，即此时的信号强度为： $$(f*g)(10)=\\sum_{\\tau=0}^{10}f(\\tau)g(10-\\tau)$$ 这正是上面所说的卷积，可以看出其在信号处理有这样的应用。如果我们把一个信号的衰减程度$g$看作它的权重，某一时刻的信号强度就可以看作之前输入信号的加权叠加。在这里我们可以说，卷积是一个函数以另一个函数为权重的加权叠加。（关于卷积还有另一种理解：将函数反转、反褶然后求和什么的，不是很直观，有兴趣的同学可以自行查看）。 至此我们说了两个关键点： **卷积是对两个数自变量和为常数的函数的积的求和（根本数学定义）。 ** 卷积中一个函数可以看作另一个函数的权，卷积可以看作加权叠加（一种理解方式）。 带着这两个观点我们再看一个例子 掷骰子同时掷两枚相同的骰子，求骰子的和为6的可能性。很明显骰子出现某个点数的概率是关于点数的常函数（函数值一直是1/6），这时两个函数自变量和为定值6，正是卷积发挥作用的场合。 概率为：​ $$(f*g)(6)=\\sum_{\\tau=1}^{5}f(\\tau)g(6-\\tau)$$ 下图为和的所有可能，均可用卷积计算。 这个例子中把一个函数理解为另一个的权解释性并不好，所以关键的是理解数学定义后运用到合适的情境中。但在NLP和computer vision的领域中用加权叠加可以很好的解释卷积的作用。 卷积如何提取text，image特征我们在 NLP 和 Computer Vision 中使用 CNN 是因为其能够提取 text ，image 中的特征，反映在出现特定特征时卷积会得到较大值（什么算较大值是规定好的）。这一机制具体如何实现请继续往下阅读，先在这里说明以免引起困惑。 NLP和Computer Vision中的卷积NLP一段文本作为词向量（非one-hot）输入神经网络，hidden-layer的神经元就会进行读取，CNN中正式以卷积操作来完成读取的。 几个重要的概念： （统一写下，之后详解） filter过滤器、kernel卷积核、neuron神经元，实际上在CNN中指的都是神经网络的神经元。 depth深度：一层卷积层（就是计算卷积的hidden-layer）有几个神经元depth就是几。 size:表示为$n\\times n$的形式，指的是receptive field-窗口的大小。 stride步长：窗口一次移动的长度。 我们现在有这样一个text：“tentative deal reached to keep government open”并用如下的词向量表示（有四列常称有4个channels）。 CNN的神经元用卷积来获取这样的text。紫色方框是刚刚提到的receptive field（窗口），size为$4\\times4$. 在文本矩阵从上到下按照设置的stride（步长）移动窗口（设置1就每次移动一格，设置2就移动两格），每次得到的矩阵都用我们的filter\\kernel（由训练得到，就是神经元中的参数）与其进行卷积运算**（矩阵中对应位置相乘再将所有位置的积相加，得到一个数就是卷积的结果），比如第一个窗口卷积为-1.0，第二个为-0.5.这样得到了右边只有一个channel的向量完成了文本的输入。把filter矩阵中的对应位置元素看作窗口中对应位置元素的权重，就符合加权叠加。** 这一步如何体现卷积的数学思想呢？ 假设窗口中的矩阵为 $$g=\\left[ \\begin{matrix} a_{1,1} &amp; a_{1,2} &amp; a_{1,3} &amp; a_{1,3} \\ a_{2,1} &amp; a_{2,2} &amp; a_{2,3} &amp; a_{2,3} \\ a_{3,1} &amp; a_{3,2} &amp; a_{3,3} &amp; a_{3,3} \\ \\end{matrix}\\right]$$ filter矩阵为： $$f=\\left[ \\begin{matrix} b_{1,1} &amp; b_{1,2} &amp; b_{1,3} &amp; b_{1,3} \\ b_{2,1} &amp; b_{2,2} &amp; b_{2,3} &amp; b_{2,3} \\ b_{3,1} &amp; b_{3,2} &amp; b_{3,3} &amp; b_{3,3} \\ \\end{matrix}\\right]=\\left[ \\begin{matrix} 3 &amp; 1&amp; 2 &amp; -3 \\ -1 &amp; 2 &amp;1 &amp; -3 \\ 1 &amp; 1 &amp; -1 &amp; 1 \\ \\end{matrix}\\right]$$ 如果把我们的filter中心对称一下， $$f=\\left[ \\begin{matrix} b_{1,1} &amp; b_{1,2} &amp; b_{1,3} &amp; b_{1,3} \\ b_{2,1} &amp; b_{2,2} &amp; b_{2,3} &amp; b_{2,3} \\ b_{3,1} &amp; b_{3,2} &amp; b_{3,3} &amp; b_{3,3} \\ \\end{matrix}\\right]=\\left[ \\begin{matrix}1 &amp; -1 &amp; 1 &amp; 1 \\ -3 &amp; 1 &amp; 2 &amp; -1 \\ -3 &amp; 2 &amp; 1 &amp; 3 \\ \\end{matrix}\\right]$$ 现在想得到和刚刚相同权重分配效果，不能让对应位置相乘，等效计算应该是 $$convolution=\\sum_{n=1}^{3}a_{n,n}b_{4-n,4-n}$$ 完美的符合了卷积的数学定义**，自变量相加等于一个定值**$（4,4）$。这样我们就了解了卷积在NLP中的运作方式了。 但是得到的向量比原先的文本短，这并不是好的结果，我们希望得到长度相同的结果，方法如下图所示： 在输入的始末添加padding（填充层）在从上到下进行相同得分步骤就能得到和text（未添加padding之前）相同长度的结果了。 但是结果只有一个channel，表示一个feature，有点少了。这就是一个卷积层拥有多个神经元的作用，每个神经元都有一个filter\\kernel（权重矩阵）来提取不同的特征： 第一个filter可能专注于polite，这段文本是不是礼貌的，如果礼貌就应该得到较大值；第二个可能是关于food，如果出现食物相关的应该得到较大值。至于得到较大值的原因， 那就是通过学习不断更新filter矩阵使其能够有这样的能力（Backpropagation）。 卷积的优势为什么这样滑动receptive field计算卷积就能获取feature呢？我们有这样的前提： 许多feature不需要看整个文本只需要看其中的一部分即可;比如，想知道有没有关于食物的phrase，只要你扫描的关于食物的部分，得到了较大的卷积值就ok，不需要整篇文章一起看. 相同的feature可能会出现在text的不同位置，这样使用同一filter扫描不同位置就能得到;比如，对于food整篇文章就能公用一组参数. 忽略我们截取的部分的语言合理性；Regardless of whether phrase is grammaticalNot very linguistically or cognitively plausible 使用卷积有这样的优势： 共享权重，使得参数大幅度减少，减轻计算压力； 一般的全连接神经网络不关心各个输入之间的相关性，CNN这样就考虑了局部的特性； 为什么要使用卷积来分配权重，直接给每个词向量的每一维设置权重就好了呀。这就是以往全连接神经网络的思想，但这样的参数量过于庞大。在上面的例子中如果使用全谅解神经网络需要$8\\times4=32$个参数才能提取text的一个feature但是CNN只需要12个。如果文本增长全连接神经网络的参数量也会上升，但卷积神经网络矩阵中仍是12个参数。 正是因为这样的特性使得CNN在ML占有一席之地，特别实在图像处理领域，全连接神经网络将每一个像素都分别赋予权重使得GPU负担过重，而卷积很好的解决了这个问题。把一部分一部分像素一起考虑，对于一个feature只使用一组参数，大大减少了图像识别的压力。比如我想知道这张图有没有手机，只需要对手机图像训练一组权重再对图像一部分一部分求卷积即可。 池化（Pooling）什么是池化相比卷积来说池化是一个比较简单的概念，其核心思想在于突出卷积计算的结果中重要的信息。 池化方法Max pooling取我们计算得到的矩阵每个channel的最大值，这样我们就能知道这段text是否含有我们想要的内容：如果第一行最大值足够大可能文本就是礼貌的。 Average Pooling用每一个channel的平均值，这样我们就知道这段text在什么程度上是polite。 二者相比较大多数情况下MAX效果会好一些。因为text中feature的某些标志是稀少的，比如用了几个敬语就可以使一段话看起来礼貌；同时大多数的词和礼貌并无关系，比如and， or ，however。 Local-max Pooling在图像处理中常用的方法，即选定一些区域，用这些区域的最大值来表示这些区域。作用为在保留主要特征的前提下压缩图片，减少数据和参数的量。","link":"/Blog/2019/04/30/Convolutional-Neural-Networks/"},{"title":"Frontiers of Natural Language Processing","text":"Natural language processing (NLP) is the technique to provide semantics to information extracted from optical character recognition engines and documents. In this article, we progress from reviewing the recent history of natural language processing towards a deeper understanding of information understanding through NLP. We will look at the history, biggest open problems and frontiers methodology. A Review of the Recent History of NLP2001 • Neural language models First neural language models: feed-forward neural networks that take into account n previous words Initial look-up layer is commonly known as word embedding matrix as each word corresponds to one vector 2013 • Word embeddings Main innovation: pretraining word embedding look-up matrix on a large unlabeled corpus Popularized by word2vec, an efficient approximation to language modeling word2vec comes in two variants: skip-gram and CBOW 2013 • Neural networks for NLP Recurrent neural networks Long-short term memory networks are the model of choice Convolutional neural networks focus on local features Can be extended with wider receptive fields (dilated convolutions) to capture wider context Convolutions can be used to speed up an LSTM Recursive neural networks Natural language is inherently hierarchical Treat input as tree rather than as a sequence Can also be extended to LSTMs 2014 • Sequence-to-sequence modelsGeneral framework for applying neural networks to tasks where output is a sequence Typically RNN-based, but other encoders and decoders can be used New architectures mainly coming out of work in Machine Translation 2015 • AttentionOne of the core innovations in Neural Machine Translation Weighted average of source sentence hidden states Mitigates bottleneck of compressing source sentence into a single vector 2018 • Pretrained language models Language models pretrained on a large corpus capture a lot of additional information Language model embeddings can be used as features in a target model or a language model can be fine-tuned on target task data Enables learning models with significantly less data Additional benefit: Language models only require unlabeled data Enables application to low-resource languages where labeled data is scarce The biggest open problems in NLPProblem 1: Natural Language Understanding and Reasoning Almost none of our current models have “real” understanding Models should incorporate common sense Problem 2: NLP for low-resource scenarios Generalization beyond the training data Domain-transfer, transfer learning, multi-task learning Learning from small amounts of data Unsupervised learning; Semi-supervised, weakly-supervised, “Wiki-ly” supervised,distantly-supervised, lightly-supervised, minimally-supervised Problem 3: Datasets, problems and evaluationPerhaps the biggest problem is to properly define the problems themselves. And by properly defining a problem, I mean building datasets and evaluation procedures that are appropriate to measure our progress towards concrete goals. Things would be easier if we could reduce everything to Kaggle style competitions! Frontiers of Natural Language ProcessingNeural networks for NLP Can be extended with wider receptive fields (dilated convolutions) to capture wider context [Kalchbrenner et al., ’17] CNNs and LSTMs can be combined and stacked [Wang et al., ACL ’16] Convolutions can be used to speed up an LSTM [Bradbury et al., ICLR ’17] CNNs over a graph (trees), e.g. graph-convolutional neural networks [Bastings et al., EMNLP ’17] Sequence-to-sequence models Deep LSTM [Wu et al., ’16] Convolutional encoders [Kalchbrenner et al., arXiv ’16; Gehring et al., arXiv ’17] Transformer [Vaswani et al.,NIPS ’17] Combination of LSTM and Transformer [Chen et al., ACL ’18] Attention Different forms of attention available [Luong et al., EMNLP ’15] Constituency parsing [Vinyals et al., NIPS ’15] Reading comprehension [Hermann et al., NIPS ’15] One-shot learning [Vinyals et al.,NIPS ’16], Image captioning [Xu et al., ICML ’15] Used in Transformer [Vaswani et al., NIPS ’17], state-of-the-art architecturefor machine translation Pretrained language models Language model embeddings can be used as features in a target model [Peters et al., NAACL ’18] Can be fine-tuned on target task data [Howard &amp; Ruder, ACL ’18]","link":"/Blog/2020/12/25/Frontiers-of-Natural-Language-Processing/"},{"title":"EKK Solution for Log Analytics Platform","text":"The EKK solution eliminates the undifferentiated heavy lifting of deploying, managing, and scaling your log aggregation and analytics solution. With the EKK stack (Elasticsearch, Amazon Kinesis and Kibana), you can fully focus on analyzing logs, improving your application, instead of managing and scaling the system to aggregate the logs. EKK Solution for Log Analytics PlatformIn this article, we explain how to use EKK stack to monitoring logs generated by your application, usually a website. Components of the EKK Solution Amazon Elasticsearch Service Distributed search and analytics engine built on Apache Lucene Send data as JSON via REST APIs Data is indexed - all fields search-able, including nested JSON. Kinesis Firehose Send data via other services or REST API Data is buffered. Transformed via Lambda. Backed up to S3 Data is delivered to Amazon ES Kibana Kinesis Firehose Data Transformation Firehose buffers up to 3 MB of ingested data When buffer is full, automatically invokes Lambda function, passing array of records to be processed Lambda function processes and returns array of transformed records, with status of each record Transformed records are saved to configured destination Writing Data to Amazon Kinesis Data Streams-","link":"/Blog/2021/04/06/EKK-Solution-for-Log-Analytics-Platform/"},{"title":"Insight into Word2Vec","text":"A comprehensive understanding of Word2Vec, including background, development, and formula derivation 自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。 自然语言处理的最最基础的部分就是要让计算机能够识别人类的语言，因此词向量也就应运而生了。词向量顾名思义就是以向量的形式表示词。 几种常见的模型（由简单到复杂）词袋模型词袋模型的基本思想就是将文档中出现的所有词用一个袋子装起来。词袋模型不考虑词出现的先后顺序，并且认为文本中每个词出现的概率都是独立的，不依靠其他词的出现。 词袋模型可以看成是用向量来表示句子。即 词1 词2 …… 词n 句子1 句子2 …… 句子m 纵坐标为选定的特征词（可以是文档中所有出现的词，也可以是选定的某一些词）。可以认为最初的空格里面全是0，当句子中出现了某个词就在相应的位置做加一的操作，最后得到的数字的组合就是相应的句子的向量。 one-hot编码one-hot编码是词向量模型的一种体现。这种编码是将文档中出现的所有的词用一个1和N-1个0表示，N是文档中出现的单词的个数。 维度1 维度2 …… 维度n 词1 1 0 0 0 词2 0 1 0 0 …… 词n 0 0 0 1 one-hot编码的好处是可以很好地表示离散数据，每一个词都可以用向量的形式表示出来。且一定程度上起到了升维的作用。在回归，分类，聚类等机器学习的算法中，特征之间距离的计算或者相似度的计算是非常重要的，而我们常用的距离或者相似度的计算都是在欧式空间的相似度计算。使用one-hot编码将离散特征的取值数字化表示到了欧式空间，离散特征的某个取值就对应与欧式空间的某个点。将离散型特征使用one-hot编码，有时确实会让特征之间的距离计算更加合理。 而坏处也比较明显，one-hot是词袋模型的体现，因此有着词袋模型的固有缺点（即忽略掉了词出现的顺序和词的上下文之间的联系）。并且one-hot编码会产生一个较大的稀疏矩阵，在用one-hot编码得到的向量计算时，如果词的维度很大可能造成维灾难而无法计算。 统计语言模型N-Gram 模型算是对上面词袋模型的补充吧。词袋模型认为文本中每个词出现的概率都是独立的，而这与我们的常规认知是不相符合的（不然完形填空该怎么做啊），而N-Gram 模型则认为可以通过计算概率的方法来获得上下文之间的关系，并且参考的上下文的词越多，预测的越准确。 词袋模型的基本思想是用概率来表示上下文之间的关系 一个经典的例子：假设我们有一个由n个词组成的句子$$\\mathrm { s } = \\left( w _ { 1 } , w _ { 2 } , w _ { 3 } , \\ldots \\ldots w _ { \\mathrm { n } } \\right)$$如何衡量它的概率呢？让我们假设，每一个单词$w_i$都要依赖于从第一个单词$w_i$到它之前一个单词$w_{i-1}$的影响:$$\\begin{array} { c }\\begin{array} { c } { p ( s ) = p \\left( w _ { 1 } , w _ { 2 } , w _ { 3 } , \\ldots , w _ { n } \\right) = p \\left( w _ { 1 } \\right) \\cdot p \\left( w _ { 2 } | w _ { 1 } \\right) \\cdot p \\left( w _ { 3 } | w _ { 2 } , w _ { 1 } \\right) \\cdot \\ldots } \\ { \\cdot p \\left( w _ { n } | w _ { n - 1 } , w _ { n - 2 } , \\ldots , w _ { 1 } \\right) } \\end{array}\\end{array}$$ 这样求一个句子出现的概率就变成了求若干个条件概率。但这样又会出现参数过多的问题。因此我们引入马尔科夫假设，即一个词出现的概率只与它之前若干个出现的单词有关。 假设一个词出现的概率只与它前面$n$个词出现的概率相关，N-Gram也就由此而来了。通常情况下$n$越大，得到的概率越准确但同时计算量也越大，因此$n$一般取2或者3。 N-Gram 思想一个先导的知识就是贝叶斯公式，这里关于贝叶斯公式不做过多的描述：$$P(B|A) = \\frac{P(A|B)P(B)}{P(A)}$$ $$P(B_i|A) = \\frac{P(A|Bi)P(Bi)}{P(A)} \\= \\frac{P(A|Bi)P(Bi)}{\\sum_{j=1}^{n} P(B_j)P(A|B_j)}$$ 从理解的角度可以认为是：$$P(规律|现象) = \\frac{P(现象|规律)P(规律)}{P(现象)}$$ 考虑$p(w_k|w_1^{k-1})$的近似计算，利用Bayes公式，有：$$p(w_k|w_1^{k-1}) = \\frac{p(w_1^k)}{p(w_1^{k-1})}$$根据大数定理，我们可以近似表示：$$p(w_k|w_1^{k-1}) \\approx \\frac{Count(w_1^k)}{Count(w_1^{k-1})}$$举个简单的例子就是：$$p(“你”|“我爱”) \\approx \\frac{Count(“我爱你”)}{Count(“我爱”)}$$从上述公式能够看出，一个词出现的概率和它之前出现的所有词都相关，如果假定一个词出现的概率只与它前面的固定书目的词相关，这就是N-Gram模型的重要思想，其中的数学逻辑就是做了一个$N-1$阶的Markov假设，认为一个词出现的概率只与它前面的$N-1$个词相关，也就是：$$p(w_k|w_1^{k-1}) \\approx \\frac{p(w_1^k)}{p(w_{k-n+1}^{k-1})}$$利用大数定律：$$p(w_k|w_1^{k-1}) \\approx \\frac{Count(w_1^k)}{Count(w_{k-n+1}^{k-1})}$$举例：假如$N=2$$$p(w_k|w_1^{k-1}) \\approx \\frac{Count(w_{k-1},w_{k})}{Count(w_{k-1})}$$ $$p(“你”|“我爱”) \\approx \\frac{Count(“爱你”)}{Count(“爱”)}$$ 所以N-Gram模型其实一个计算一个句子出现概率的模型，当我们需要计算一个句子出现的概率时，只需要计算相关的概率参数，然后将他们连乘起来就好了。 那么，我们怎么利用这样的思想，用机器学习的方法将语言进行建模呢？方法就是将我们所考虑的问题建成一个模型，然后构造出一个目标函数，然后对这个目标函数进行优化，从而求得一组最优的参数，然后利用这一组最优的参数进行预测。 对于统计语言模型，可以利用最大似然的概念，将目标函数设置为：$$\\prod_{Corpus} p(w|Context(w))$$利用前文的n-gram思想，$Context(w)=w_{i-n+1}^{i-1}$ 实际中，我们采用极大对数似然，将乘法转换为加法，那么目标函数则为：$$\\sum_{Corpus} \\log p(w|Context(w))$$目标就是最大化极大对数似然函数。 一种简单的思路来解决这个问题，就是建立一个关于$w$和它上下文$Context(w)$的函数：$$p(w|Context(w)) = F(w,Context(w),\\theta)$$这样一来就不用每次都遍历数据库了，只需要训练模型得到参数集，之后使用参数集中的参数，那么输入唯一之后输出也就唯一了。 两个重要模型词向量界中有两个很重要的模型： CBOW (Continuous Bag-of-Words Model) Skip-gram (Continuous Skip-gram Model) Word2Vec 模型中，主要有 Skip-Gram 和 CBOW 两种模型，从直观上理解，Skip-Gram 是给定 input word 来预测上下文。而 CBOW 是给定上下文，来预测 input word。 Word2Vec 模型实际上分为了两个部分，第一部分为建立模型，第二部分是通过模型获取嵌入词向量。Word2Vec 的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵 —— 后面我们将会看到这些权重在 Word2Vec 中实际上就是我们试图去学习的 “word vectors”。基于训练数据建模的过程，我们给它一个名字叫 “Fake Task”，意味着建模并不是我们最终的目的。 这两个模型是两个思想，基于这两个思想，Word2Vec给出了两个框架，一个利用了霍夫曼树（Hierarchical Softmax），一个利用了负采样（Negative Sampling）。 CBOW模型的目标函数为：$$\\sum_{w \\in Corpus} \\log p(w|Context(w))$$Skip-gram模型的目标函数为：$$\\sum_{w \\in Corpus} \\log p(Context(w)|w)$$ 模型详解Skip-gram 模型讲解Fake Task训练模型的真正目的是获得模型基于训练数据学得的隐层权重。为了得到这些权重，我们首先要构建一个完整的神经网络作为我们的 “Fake Task”，后面再返回来看通过 “Fake Task” 我们如何间接地得到这些词向量。 下面的图中给出了一些我们的训练样本的例子。我们选定句子 “The quick brown fox jumps over lazy dog”，设定我们的窗口大小为 2（window_size=2），也就是说我们仅选输入词前后各两个词和输入词进行组合。下图中，蓝色代表 input word，方框内代表位于窗口内的单词。 我们的模型将会从每对单词出现的次数中习得统计结果。例如，我们的神经网络可能会得到更多类似（“Soviet“，”Union“）这样的训练样本对，而对于（”Soviet“，”Sasquatch“）这样的组合却看到的很少。因此，当我们的模型完成训练后，给定一个单词”Soviet“作为输入，输出的结果中”Union “或者” Russia “要比”Sasquatch“被赋予更高的概率。 我们如何来表示这些单词呢？首先，我们都知道神经网络只能接受数值输入，我们不可能把一个单词字符串作为输入，因此我们得想个办法来表示这些单词。最常用的办法就是基于训练文档来构建我们自己的词汇表（vocabulary）再对单词进行 one-hot 编码。 隐层没有使用任何激活函数，但是输出层使用了 sotfmax。 我们基于成对的单词来对神经网络进行训练，训练样本是 (input word, output word) 这样的单词对，input word 和 output word 都是 one-hot 编码的向量。最终模型的输出是一个概率分布。 隐藏层部分看下面的图片，左右两张图分别从不同角度代表了输入层 - 隐层的权重矩阵。左图中每一列代表一个 10000 维的词向量和隐层单个神经元连接的权重向量。从右边的图来看，每一行实际上代表了每个单词的词向量。 所以我们最终的目标就是学习这个隐层的权重矩阵。 输出层部分经过神经网络隐层的计算，ants 这个词会从一个 1 x 10000 的向量变成 1 x 300 的向量，再被输入到输出层。输出层是一个 Softmax 回归分类器（这部分会在后面稍微展开一些），它的每个结点将会输出一个 0-1 之间的值（概率），这些所有输出层神经元结点的概率之和为 1。 下面是一个例子，训练样本的计算示意图。 input word: “ants” output word: “car” 模型理解这部分官方已经给出了非常好的解释，这里就直接上英文原文吧： Ok, are you ready for an exciting bit of insight into this network? If two different words have very similar “contexts” (that is, what words are likely to appear around them), then our model needs to output very similar results for these two words. And one way for the network to output similar context predictions for these two words is if the word vectors are similar. So, if two words have similar contexts, then our network is motivated to learn similar word vectors for these two words! Ta da! And what does it mean for two words to have similar contexts? I think you could expect that synonyms like “intelligent” and “smart” would have very similar contexts. Or that words that are related, like “engine” and “transmission”, would probably have similar contexts as well. This can also handle stemming for you – the network will likely learn similar word vectors for the words “ant” and “ants” because these should have similar contexts. Softmax 详解在机器学习尤其是深度学习中，softmax 是个非常常用而且比较重要的函数，尤其在多分类的场景中使用广泛。他把一些输入映射为 0-1 之间的实数，并且归一化保证和为 1，因此多分类的概率之和也刚好为 1。 网上有一张图很好的说明了softmax的计算方法： 假设有一个数组 $V$，$V_i$表示 $V$ 中的第$ i $个元素，那么这个元素的 softmax 值为：$$S_i = \\frac{e^i}{\\sum_j e^j}$$该元素的 softmax 值，就是该元素的指数与所有元素指数和的比值。 模型公式推导这里给出了几个模型的公式推导： 神经网络模型（One-Word模型） CBOW 模型 Skip-gram 模型 One-Word 模型One-Word 是用神经网络来实现 N-Gram（$n=2$时）。即N-Gram是思想，而One-Word模型是实现方法。即一个用one-hot编码的词作为输入，通过第一个权重矩阵得到隐藏层，再通过第二个权重矩阵的到输出层的前身，该前身再做Softmax得到输出端的向量表示。 设：词汇量的大小为$V$，隐藏层的大小为$N$。输入向量是一个one-hot编码的向量，one-hot编码的向量表示为$(x_1,x_2,…,x_v)$，其中只有一个$x_k$为1，其余的均为0。姑且认为$X(V \\times 1), h(N \\times 1), Y(V \\times 1)$都是列向量 输入层和隐藏层之间是一个$V \\times N$的矩阵$W_{V \\times N}$ $$\\left( \\begin{array}{cccc}{\\mathrm{W}{11}} &amp; {\\mathrm{W}{12}} &amp; {\\dots} &amp; {\\mathrm{W}{1 n}} \\ {\\mathrm{W}{21}} &amp; {\\mathrm{W}{22}} &amp; {\\dots} &amp; {\\mathrm{W}{2 n}} \\ {\\cdots} &amp; {\\cdots} &amp; {\\cdots} &amp; {\\cdots} \\ {\\mathrm{W}{\\mathrm{vl}}} &amp; {\\mathrm{W}{\\mathrm{v} 2}} &amp; {\\dots} &amp; {\\mathrm{W}_{\\mathrm{vn}}}\\end{array}\\right)$$ $$h=x^T W$$ 则$h$为$$\\left( \\begin{array}{llll}{\\mathrm{W}{\\mathrm{kl}}} &amp; {\\mathrm{W}{\\mathrm{k} 2}} &amp; {\\dots} &amp; {\\mathrm{W}_{\\mathrm{kn}}}\\end{array}\\right)^{\\mathrm{T}}$$即$W$矩阵的第$k$行的转置 （因为$x_k$为1，其余的均为0） 隐藏层到输出层是一个$N \\times V$的权重矩阵 形式和上面那个矩阵类似，只是矩阵的元素用$\\mathrm{w}_{\\mathrm{ij}}^{\\prime}$表示 $$\\mathrm{U}{\\mathrm{j}}=\\mathrm{W}{\\mathrm{j}}^{\\prime} \\mathrm{h}$$ 注：$\\mathrm{W}_{j}^{‘}$为$W$矩阵的第$j$行，$U_j$为输出层的第$j$个位置的前身 $U_j$经过一个Softmax回归得到$y_j$，$y_j$为正经的输出层的第$j$个位置 $$\\mathrm{P}\\left(\\mathrm{w}{j} | \\mathrm{w}{I}\\right)=y_{j}=\\frac{e^{j}}{\\sum_{i=1}^{V} e^{i}}$$ 即在输入已经确定的情况下，输出的值为$w_j$的概率为这个 更新$W$和$W^{‘}$矩阵 首先定义一个损失函数（我们希望这个损失函数最小）（至于为什么这么定义还是没太理解） $$\\mathrm{E}=-\\mathrm{P}\\left(\\mathrm{w}{\\mathrm{o}} | \\mathrm{w}{\\mathrm{i}}\\right)=\\mathrm{U}{\\mathrm{j}^{*}}-\\log \\left(\\sum{i=1}^{V} e^{i}\\right)$$ 首先更新$W^{‘}$矩阵： 这就需要用$E$来对$\\mathrm{w}^{‘}_{\\mathrm{ij}}$来求偏导，以获得最快更新$W^{‘}$的方向（即梯度方向） 在求偏导之前需要知道这样的前提 即 $$\\frac{\\partial \\mathrm{E}}{\\partial \\mathrm{U}{i}}=\\mathrm{y}{\\mathrm{i}}-\\mathrm{t}_{\\mathrm{i}}$$ 当预测准确的时候$t_i$为1，否则为0，这个计算公式可以看成是输出层的的预测误差，至于结果为什么是这样的暂时还不是很清楚 在上述前提下 $$\\frac{\\partial \\mathrm{E}}{\\partial \\mathrm{W}{\\mathrm{ij}}}=\\frac{\\partial \\mathrm{E}}{\\partial \\mathrm{U}{\\mathrm{i}}} * \\frac{\\partial \\mathrm{U}{\\mathrm{i}}}{\\partial \\mathrm{w}{\\mathrm{ij}}}=\\left(y_{i}-t_{i}\\right) * h_{i}$$ 则权重更新方程为 $$\\mathbf{W}{i j}^{\\prime}=\\mathbf{W}{i j}^{\\prime}-\\eta\\left(y_{i}-t_{i}\\right) * h_{i}$$ 其次更新$W$矩阵和上文差不多，也是需要$W$对$\\mathrm{W}_{\\mathrm{ij}}$求偏导 求之前也需要知道一个前提： $$\\frac{\\partial \\mathrm{E}}{\\partial h_{i}}=\\sum_{i=1}^{V} \\frac{\\partial \\mathrm{E}}{\\partial U_{i}} \\frac{\\partial U_{i}}{\\partial h_{i}}=\\sum_{i=1}^{V}\\left(y_{i}-t_{i}\\right) * w_{i j}^{\\prime}$$ 此式子的结果用$\\mathrm{EH}_{\\mathrm{i}}$代为表示： $$\\mathrm{EH}=\\left(\\mathrm{EH}{1} \\cdot \\mathrm{EH}{2}, \\ldots . . \\mathrm{EH}_{\\mathrm{N}}\\right)$$ 在此前提下求$E$对$\\mathrm{W}{i j}$的偏导数为：$$\\frac{\\partial \\mathrm{E}}{\\partial \\mathrm{w}{i j}}=\\frac{\\partial \\mathrm{E}}{\\partial \\mathrm{h}{j}} \\frac{\\partial \\mathrm{h}{j}}{\\partial \\mathrm{w}{i j}}=E H{j} * X_{i}$$ 由于X非1即0，所以权重更新公式为： $$\\mathrm{V}{w i}=\\mathrm{V}{w i}-\\eta E H$$其中$\\mathbf{V}_{w i}$为one-hot编码中非零行所对应的矩阵的行，其他行不用关心。 CBOW 模型（连续词袋模型） 基本概念 CBOW 的基本思想是用中心词的上下文的c个词来预测中心词。 连续词袋模型模型相当于是 One-Word 模型的补充，One-Word 是一个输入，一个输出，CBOW 是 c 个输入，1个输出。 $\\mathrm{X}_{mathrm{1}, mathrm{k}}$ 是上下文第 1 个单词的 one-hot 编码。 $\\mathrm{X}_{mathrm{c}, mathrm{k}}$ 是第 C 个单词的 one-hot 编码。 这C个 one-hot 编码通过相应位置加和求平均的方法得到一个$1 \\times \\mathrm{V}$ 的向量。 该向量再乘以我们期望得到的第一个矩阵$\\mathrm{W}{\\mathrm{V} \\times \\mathrm{N}}$来得到隐藏层的向量$h_i$，一个$1 \\times N$的向量），即$$\\mathrm{h}=\\frac{1}{C} W\\left(\\sum{i=1}^{C} x_{i}\\right)$$ 上图的$h_i$表示的意思可以与上上图的$h_i$相比较。然后$h$乘以另一个我们期望得到的矩阵：$$\\mathrm{W}_{\\mathrm{N}^{‘} \\mathrm{V}}^{*}$$得到一个$1 \\times \\mathrm{V}$的向量U，再用Softmax得到一个$1 \\times \\mathrm{V}$的向量$Y$，其中$Y_i$最大的那个值就是期待的中心词。通过不断地学习来调整 $\\mathrm{W}$ 和 $\\mathrm{W}^{‘}$ 的值。 CBOW矩阵的调整 建立一个损失函数： $$\\mathrm{E}=-\\log \\left(\\mathrm{P}\\left(\\mathrm{w}{\\mathrm{o}} | \\mathrm{W}{\\mathrm{I}, 1} \\ldots \\mathrm{W}{\\mathrm{I}, \\mathrm{c}}\\right)\\right)\\=-U{j^{*}}+\\log \\left(\\sum_{j}^{V} e^{u_{i}}\\right)\\=-\\mathrm{V}{\\mathrm{w}{\\mathrm{o}}}^{\\prime T} \\cdot h+\\log \\left(\\sum_{i=1}^{V} e^{v_{v_{i}} T} \\cdot h\\right)$$ 然后和上面one-word模型更新两个矩阵的方法类似 都是对相应的矩阵的元素求导得到梯度来更新矩阵。 $$\\mathrm{V}{\\mathrm{wj}}^{\\prime}=\\mathrm{V}{\\mathrm{wj}}^{\\prime}-\\eta\\left(\\mathrm{y}{\\mathrm{i}}-\\mathrm{t}{\\mathrm{i}}\\right) \\mathrm{h}$$ 用上述公式来更新$W^{‘}$，其中$j=1,2, \\dots \\dots ,\\mathrm{V}$ 其中$\\mathrm{V}^{\\prime}_{\\mathrm{w} j}$为隐藏层到输出等的矩阵的第$j$列 $$\\mathrm{V}{\\mathrm{w}, \\mathrm{I}, \\mathrm{c}}=\\mathrm{V}{\\mathrm{w}, \\mathrm{I}, \\mathrm{c}}-\\frac{1}{\\mathrm{C}} \\eta \\cdot \\mathrm{EH}$$ 用上述公式来更新$W$ 其中$\\mathrm{V}_{\\mathrm{w}, \\mathrm{I}, \\mathrm{c}}$是输入上下文单词的第c个单词的输入向量。其中$c=1,2, \\cdots \\cdots ,\\mathrm{C}$。 Skip Gram 模型模型概述Skip-Gram 模型可以看成是与 CBOW 模型相反的，即用一个中心词来推测其附近的 c 个上下文（注：得到的 c 个上下文不考虑与中心词之间的距离的影响）。 数学推导隐藏层$h$仍然是矩阵$W$的第$k$行（$X_k=1$的情况下） 与之前稍微有点不同的是Skip-Gram模型的输出有多个，每个输出都使用的相同的$W^{‘}$来计算 $$\\mathrm{P}\\left(\\mathrm{w}{c, j}=w{o, c} | w_{I}\\right)=y_{c, j}=\\frac{e^{u_{c, j}}}{\\sum_{i=1}^{V} e^{u_{i}}}$$ 其中 $\\mathrm{W}_{\\mathrm{c}, \\mathrm{j}}$ 是输出层第 $c$ 个输出的第 $j$ 个单词。 而 $\\mathrm{w}_{\\mathrm{o}, \\mathrm{f}}$ 是第c个输出的实际输出的单词。 $\\mathrm{U}_{\\mathrm{c}, \\mathrm{j}}$ 是 h 与 $\\mathrm{W}^{‘}$ 的第 $j$ 行向量相乘的结果，由于乘的都是同一个第 $j$行。 所以：$$\\mathrm{U}{\\mathrm{c}, \\mathrm{j}}=\\mathrm{U}{\\mathrm{j}}=V_{w_{j}}^{\\prime T} h$$ 再次构造损失函数： $$\\mathrm{E}=-\\log \\left(\\mathrm{P}\\left(w_{0,1}, w_{0,2}, \\ldots w_{0, \\mathrm{C}} | w_{\\mathrm{I}}\\right)\\right) \\=-\\sum_{i=1}^{C} U_{j_{i}^{*}}+\\operatorname{Clog}\\left(\\sum_{i=1}^{V} U_{i^{\\prime}}\\right) \\$$ $$\\frac{\\partial \\mathrm{E}}{\\partial \\mathrm{U}{c, j}}=y{c, j}-t_{c, j}$$ 用:$$\\mathrm{EI}=\\left{\\mathrm{EI}{1}, \\mathrm{EI}{2}, \\ldots . \\mathrm{EI}{\\mathrm{v}}\\right}$$来表示所有上下文单词的预测误差之和，即：$$\\mathrm{EI}{\\mathrm{j}}=\\sum_{\\mathrm{i}=1}^{\\mathrm{C}} \\mathrm{y}{\\mathrm{i}, \\mathrm{j}}-\\mathrm{t}{\\mathrm{i}, \\mathrm{j}}$$ $\\mathrm{W}^{\\prime}$的权重矩阵更新公式为： $$w_{i, j}^{\\prime}=w_{i, j}^{\\prime}-\\eta^{} E I_{j}^{} h_{i}$$ $V$的权重矩阵更新公式为： $$\\mathrm{V}{\\mathrm{w}{1}}=\\mathrm{V}{\\mathrm{w}{1}}-\\eta \\mathrm{EH}$$ 其中 $$\\mathrm{EH}=\\sum_{j=1}^{V} E I_{j} * w_{i, j}^{\\prime}$$ 参考来源 Chris McCormick · Machine Learning Tutorials and Insights 刘建平 Pinard · CBOW 与 Skip-Gram 模型基础 peghoty · word2vec 中的数学原理详解 算法原理以及公式推导 理解Word2Vec之Skip-Gram模型","link":"/Blog/2019/04/21/Insight-into-Word2Vec/"},{"title":"Docker, Django, React, AWS: Deploying Containerized Application to Cloud","text":"In the fast-paced field of web applications, containerization has become not only common but the preferred mode of packaging and delivering web applications. Containers allow us to package our applications and deploy them anywhere without having to reconfigure or adapt our applications to the deployment platform. Amazon Elastic Container Service (Amazon ECS) is the service Amazon provide to run Docker applications on a scalable cluster. [TOC] Build an Application Using Django and ReactFor this application, React serves as the front-end or client side framework, handling UI and getting and setting data via requests to the Django back-end, which is an API built using the Django REST framework (DRF). React is a JS framework that is great for developing SPAs (single page applications) and it has solid documentation and a vibrant ecosystem around it. Django is a Python web framework that simplifies common practices in web development. Django has been around for a while, meaning most gotcha’s and problems have been solved, and there’s a set of stable libraries supporting common development needs. Work-flow Setting up the Back-end Creating an Application with Docker ComposeKey benefit: if it works locally, it works in production. Setup DockerAdd NGINX ProxyModifying Settings Files Deploy Docker Containers on Amazon ECSReference Creating an app with Docker Compose, Django, and Create React App Docker, Django, React: Building Assets and Deploying to Heroku Deploying Django Applications to AWS EC2 with Docker Deploying Django to AWS with Docker and Let’s Encrypt Deploy Docker Containers on Amazon Elastic Container Service (Amazon ECS)","link":"/Blog/2020/12/08/Docker-Django-React-AWS-Deploying-Containerized-Application-to-Cloud/"},{"title":"Mastering AWS SAM: The AWS Serverless Application Model","text":"SAM stands for Serverless Application Model. The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. AWS SAM OverviewAWS SAM comes in two parts SAM Templates: Using shorthand syntax to express resources and event source mappings, it provides infrastructure as code (IaC) for serverless application. SAM CLI: Provides tooling for local development, debugging, build, packaging, and deployment for serverless applications. Sam Serverless Resources AWS:Serverless::Function AWS:Serverless::Api AWS:Serverless::HttpApi AWS:Serverless::SimpleTable AWS:Serverless::LayerVersion AWS:Serverless::StateMachine AWS SAM GlobalsGlobals help you make your SAM templates even smaller by enabling you to re-use attributes throughout. Also, using the same template to build the same infrastructure ensures consistency across multiple environments. AWS SAＭ CLIReference Documentation Tutorial: Deploying a Hello World application AWS News Blog New – AWS SAM Local (Beta) – Build and Test Serverless Applications Locally","link":"/Blog/2021/07/03/Mastering-AWS-SAM/"},{"title":"Natural Language Processing(NLP) for Machine Learning","text":"Machine learning with natural language is faced with one major hurdle – its algorithms usually deal with numbers, and natural language is, well, text. So we need to transform that text into numbers, otherwise known as text vectorization. [toc] Form the DatasetPositive Samples Twitter Information Operations: Insights into attempts to manipulate Twitter by state-backed entities. User Dataset: followers count, following count, account creation date, etc. Tweets Dataset: tweets content, hash-tag, etc. Both of these two dataset have user_id which can tell us which tweets is belong to who. Base on this information, we could use all the tweets of a account as a feature of the user, and convert this feature into a numeric value which could directly used by machine learning model. Negative Samples User Dataset – “Tweets Loud and Quiet” Tweets Dataset – sentiment140 Those two dataset have no connection, but we could know the distribution of users information and tweets content separately. That’s also why we could use these two separate dataset to form the negative sample by simply sampling the tweets from tweets dataset to be the users tweets feature. the total number of tweets posted by user is told by the users dataset, as well as the frequency of user’s tweet behavior(calculated by total number divided by time horizon). Sample of Natural Language Dataset user_id(index) follower_count following_count tweet_content state-back label 1 32 1 @DiazCanelB: Campaign by MEPs against Cuba rejected in Belgium. Another instance of the Empire’s vulgar and interfering policy of subver… RT 1 2 23 45 @DiazCanelB: Fidel: “I keep in mind..that Bolivar was the man that José Martí most admired. 1 3 2245 3332 #Style used to be an #interaction between the #human #soul and tools that were limiting. 0 4 4 0 #AI RT @couponfree01: #udemy Free Discount - The Complete Node.js Developer Course (3rd Edition) 0 Sample of Numeric Dataset user_id(index) follower_count following_count against campaign … Developer mind state-back label 1 32 1 0.63 0.77 … 0.65 0 1 2 23 45 0 0 … 0 1 1 3 2245 3332 0 0 … 0 0 0 4 4 0 0 0 … 0.64 0 0 Note: the numeric value isn’t the number of time that word appear in the sample, it’s the TF-IDF value of the words. That’s why the values are decimal instead of integer. TF-IDF value will be introduced in the Vectorizing Data section, please find it below. The mean reason to do so is the reduce the dimension and also measure the feature of samples in a more scientific way Pre-processing DataRemove punctuationPunctuation can provide grammatical context to a sentence which supports our understanding. But for our vectorizer which counts the number of words and not the context, it does not add value, so we remove all special characters. e.g.: How are you?-&gt;How are you TokenizationIs the process of segmenting running text into sentences and words. In essence, it’s the task of cutting a text into pieces called tokens, and at the same time throwing away certain characters, such as punctuation. Remove stopwordsStopwords are common words that will likely appear in any text. They don’t tell us much about our data so we remove them. e.g.: silver or lead is fine for me-&gt; silver, lead, fine. we are passing two parameters to CountVectorizer, max_df and stop_words. The first is just to say ignore all words that have appeared in 85% of the documents, since those may be unimportant. The later, is a custom stop words list. You can also use stop words that are native to sklearn by setting stop_words='english',. LemmatizingFor grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set It is better than stemming as it uses a dictionary-based approach i.e a morphological analysis to the root word. e.g.: entitling, entitled -&gt; entitle Vectorizing DataVectorizing is the process of encoding text as integers i.e. numeric form to create feature vectors so that machine learning algorithms can understand our data. Bag-Of-WordsIt gives a result of 1 if present in the sentence and 0 if not present. It, therefore, creates a bag of words with a document-matrix count in each text document. Extracting features with TF-IDFWhy is TF-IDF used in Machine Learning ?Machine learning with natural language is faced with one major hurdle – its algorithms usually deal with numbers, and natural language is, well, text. So we need to transform that text into numbers, otherwise known as text vectorization. It’s a fundamental step in the process of machine learning for analyzing data, and different vectorization algorithms will drastically affect end results, so you need to choose one that will deliver the results you’re hoping for. What is TF-IDF ?TF-IDF which stands for Term Frequency – Inverse Document Frequency. It is one of the most important techniques used for information retrieval to represent how important a specific word or phrase is to a given document. The TF-IDF value increases in proportion to the number of times a word appears in the document but is often offset by the frequency of the word in the corpus, which helps to adjust with respect to the fact that some words appear more frequently in general. The term frequency of a word in a document. There are several ways of calculating this frequency, with the simplest being a raw count of instances a word appears in a document. Then, there are ways to adjust the frequency, by length of a document, or by the raw frequency of the most frequent word in a document. The inverse document frequency of the word across a set of documents. This means, how common or rare a word is in the entire document set. The closer it is to 0, the more common a word is. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm. So, if the word is very common and appears in many documents, this number will approach 0. Otherwise, it will approach 1. Multiplying these two numbers results in the TF-IDF score of a word in a document. The higher the score, the more relevant that word is in that particular document. Mathematical TermTo put it in more formal mathematical terms, the TF-IDF score for the word t in the document d from the document set D is calculated as follows:$$\\begin{equation}\\text { tf-idf }(t, d, D)=t f(t, d) \\cdot \\text { idf }(t, D)\\end{equation}$$ where $$\\begin{equation}\\begin{array}{c}t f(t, d)=\\log (1+\\text { freq }(t, d)) \\i d f(t, D)=\\log \\left(\\frac{N}{\\operatorname{count}(d \\in D: t \\in d)}\\right)\\end{array}\\end{equation}$$ How can one reduce the TFIDF model size? The most effortless way is by filtering out infrequent words. You can achieve this by setting input arguments as follows: to use min_df to ignore terms that have a document frequency lower than the min_df. If float, the parameter represents a proportion of documents, integer absolute counts. When dealing with a relatively large corpus, using min_df of 5, 10, or 50 reduces the size of the vocabulary significantly while maintaining (or often improving) the accuracy. max_features To consider only the top max_features ordered by term frequency across the corpus. This is useful if you have strict limit on the size of TF-IDF transformed features (e.g. up to 100,000 TF-IDF features). Feature EngineeringFeature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. I haven’t tried this work in our project. We could discuss how to do this part if we want the higher performance or want to dive deeply into the natural language processing. Some Basic Idea of Constructing Features: The average length of tweets posted by user. The average length of sentence(base on the intuition that the provocative sentence tend to have few words in a sentence to make a clear slogan). Metric Accuracy can be a misleading metric for imbalanced data sets. Consider a sample with 95 negative and 5 positive values. Classifying all values as negative in this case gives 0.95 accuracy score. Precision: In the field of information retrieval, precision is the fraction of retrieved documents that are relevant to the query. Recall: recall is the fraction of the relevant documents that are successfully retrieved. Precision and recall are then defined as: F-measure: he traditional F-measure or balanced F-score (F1 score) is the geometric(harmonic) mean of precision and recall: Reference Natural Language Processing(NLP) for Machine Learning Sklearn | Feature Extraction with TF-IDF Feature Extraction using TF-IDF algorithm Extracting Keywords with TF-IDF and Python’s Scikit-Learn Twitter sentiment analysis using Python and NLTK What is TF-IDF Other Useful Dataset A list of Twitter datasets and related resources","link":"/Blog/2020/11/13/Natural-Language-Processing(NLP)-for-Machine-Learning/"},{"title":"How to Make the Best Pour Over Coffee Like a Pro","text":"For a coffee enthusiast, nothing gives as much gratification as participating in every step of the coffee-making process and making the most out of the precious, well-loved beans. From choosing the roast to getting the perfect grind and to choosing the approach of extracting that liquid gold, it’s as much a ritual as it is an adventure into the pristine world of caffeine. While there are many ways of making your own cup of coffee, nothing can be compared to pour-over coffee for a “big coffee lover who knows his cup”, and the golden line always is how to make a consistently amazing cup of one. [TOC] GET GOOD COFFEE BEANSDifferent Shades of Coffee RoastsThere is a real distinction between light, medium and dark roast coffee, and we are talking about more than just the color of the coffee beans when they are removed from the roaster. Light Roast Coffee If you find yourself enjoying coffee with a sweeter, more tangy taste, light roast coffee is your go-to order. Light roast coffee beans are roasted between 175-200°C to either just before or right at the first crack. Word has it that coffee roasters in the 80s realised when high-quality beans are roasted for a shorter time, more complex flavours are unlocked. Many like light roasts for its milder taste — it is less bitter, though more acidic to the tongue. Having been exposed to heat for a shorter time, the beans offer some delicately nuanced flavours, retaining much of the original taste imbued from the soil they have been grown in. A cup of light roast coffee reveals traces of sweetness and fruity undertones, often with a subtle floral aroma. Medium Roast Coffee Next, the medium roast coffee, an all-rounder in the coffee roasting realm. Coffee beans are roasted a little longer than the light roast until the colour turns a slightly darker shade of brown. Medium roasts are typically exposed to temperatures between 200-220°C, roasted to the end of the first crack or the beginning of the second. These coffee beans offer a multilayered complexity in taste. While many of the coffee’s original flavours are preserved, the beans are also roasted until they begin to reach a deep caramel sweetness. As a result, your cup of medium roast coffee is most likely to be sweet in flavour with prominent notes of fruit, chocolate and caramel, highly aromatic and less acidic. For those who prefer a more balanced flavour profile, you can’t go wrong with a medium roast. Dark Roast Coffee Traditionally, dark roast is used to mask defective or lower grade coffee beans. They are roasted to a point where one is no longer able to taste any of the discerning qualities. The dark roast happens roughly at the end of the second crack or slightly beyond, reaching a little over 230°C. At this stage, the coffee’s original flavours (bright tones) are typically overshadowed by the roasting qualities, which are bold and rich in body and texture as well as a hallmark aroma familiar to most. Lately though, coffee roasters are no longer roasting away the bad flavours, but creating dark roasts to bring out the deeper and darker yet pleasant notes in coffee beans. The right dark roast sometimes reveals a decadent dark chocolatey flavour or toasted pine. It’s hard to miss dark roast; the dark shiny appearance from the oily beans will give away the roast. Freshness MattersCheck the Roast Date On each bag of specialty coffee, you’ll notice a roast date. Check for this when buying your coffee beans. Restock your coffee every one or two weeks. Only buy as much as you and your household need. Degassing Valve Near the top of your bag of coffee, you may have noticed a small round piece of plastic with a couple of holes. This is a degassing valve. It lets the CO2 gases out while keeping the oxygen at bay, keeping your coffee fresher for longer. Storage Sunlight and moisture are the coffee beans worst enemy. Keep them out of the sun and do not put them in your fridge or freezer. Changing your beans climate from cold to warm will cause condensation that will do more damage than good. Keep your beans in an air tight or sealed container. If your bag of beans doesn’t have a zip lock, store them in a glass or ceramic jar, and put them away in a kitchen cupboard. Easy! HOW TO MAKE AMAZING POUR OVER COFFE Step 1. Boil water. Measure out at least 600g (20 oz) of water and boil it. The ideal water temperature for pour over coffee is somewhere between 195F and 205F. So when our water has started boiling, turn off the heat and let it sit for 30 seconds to one minute. Step 2. Grind the coffee. It’s best to use freshly ground coffee of your choice, so grind just enough coffee beans you need for serving your pour over coffee. Professional baristas and home coffee enthusiasts alike swear by investing in a quality burr grinder for a consistent grind that also results in even coffee extraction. A good ratio to go by for pour over method is 60 grams of beans for every 1 liter of water, or you can try a coffee to water ratio of something between 1:12 and 1:17. Still, you can make your adjustments based on your own preference. Step 3. Pre-wetting the filter. Rinse your paper filter to ensure that your coffee doesn’t have any paper taste. It also ensures that your filter sticks to the sides of your dripper for a better fit. Place the filter in the dripper over your cup or carafe, and then for about five seconds, carefully pour hot water all over the filter in a circular motion. Then, discard the water that runs through the filter and into the cup. Step 4. Make the coffee bloom. Pour the coffee grounds into the filter and gently tap it to make sure that the grounds settle evenly. Make sure that your pour-over dripper is placed snugly on top of your cup. Then, add just enough of the hot water to ensure that all of the grounds get wet. This process of blooming the coffee releases carbon dioxide while making the grounds swell and expand. This also releases the beans’ wonderful smell and flavor, priming you for the delicious cup ahead. Step 5. Make the first pour. Pouring over coffee may be straightforward, but it still requires some form of finesse. When you finally pour your hot water onto your beans, remember to take your time instead of just dumping all the water. The right way to pour over coffee is by pouring water slowly over the grounds, in a circular motion. Start at the outer edge of the coffee, the one that hits the filter, and slowly move towards the center, saturating all the grounds evenly. This should take roughly 15 seconds. Then stop and allow your coffee to drip before making the second pour. Step 6. The second pour. Once you see that there aren’t a lot of extracted coffee dripping, or that the coffee grounds are not saturated with much water anymore, it’s time to make your second pour. The interval from the end of the first pour to the beginning of the second should be around 30 seconds. Starting from the center of the filter, slowly pour in a steady stream of hot water, again in a circular motion, toward the outer edge and then back at the center. Make sure that you’re not missing the outer edges of the beans. This even and circular motion that goes in and out and in again helps prevent the grounds from bunching up irregularly around the filter, missing the extraction. It also creates something of a turbulence that stirs the coffee in the filter so that the water comes in contact throughout all the coffee grounds. This should take roughly between 45 to 65 seconds, depending on how much coffee and water you have. Step 7. The third pour. As the coffee mixture goes through the filter to extract your precious brew, pour in additional water using the same slow and steady motions as the second pour. This will take 15 to 20 seconds. Step 8. The fourth and final pour. As the mixture falls down into the bottom of the filter and into your cup, make your fourth and final pour. This should take 15 to 20 seconds as well. Then, remove the dripper and serve your coffee. And there you have it—a delicious, full-flavored pour over coffee that will make caffeine gods and baristas proud. Enjoy! FLAVOURED COFFEE AND HOW IT IS MADEIt wasn’t until a couple of hundred years ago when the Middle Easterners started blending nuts and spices with coffee. Using natural ingredients that were readily sourced, they created their own rendition of flavoured coffee. Infusing with Flavouring Oils In most cases, flavoured coffee is made by infusing one or more flavouring oils into the beans. These oils can either be made from natural oils, synthetic flavour chemicals, or a mixture of both. Natural oils are extracted from plants or spices such as vanilla pods, cocoa beans, nuts and berries. However, other flavours are mimicked in a laboratory. Flavour components from the natural oils are isolated and mixed to reproduce the desired flavour, therefore deriving its name – natural and artificial flavouring. Adding Fresh Spices If you enjoy the aroma and taste notes that spices bring, you can easily make your own flavoured coffee without buying questionable ones that are laden with chemicals and preservatives. For a quick solution, simply mix the grounded spices with coffee powder, add hot water, and the coffee will pick up the flavouring as it brews. However, be wary of fine powders such as cocoa and cinnamon as they tend to clump up and clog coffee-making gadgets. As a precaution, ensure that the mixture is always well-mixed and proportioned. For more intense flavouring, leave a whole piece of spice that isn’t broken up into your airtight coffee bean storage container. Although it may take a few days to get a significant spice taste into your beans, your coffee will not have an overbearing spice kick. Coffee Flavouring Syrups Rather than infusing beans with vanilla pods and cloves, most coffeehouse chains use packaged coffee syrups that are manufactured in an industrial laboratory for time and cost-efficiency. However, these syrups provide consistent quality and flavour, offering a sense of tradition with flavours such as toffee, caramel and hazelnut. Beware though, as these syrups are high in fructose corn syrup, food additives and preservatives. Here are six recommended liqueurs that bring a delicate yet complex set of flavours when added to coffee. SINGLE ORIGIN COFFEE: WHAT IS IT, AND IS IT WORTH THE PREMIUM?You’ve seen it emblazoned on bags of coffee beans and touted on cafe menus everywhere: the words “single origin”. But what does it mean and what’s the big deal, really? Is it just another meaningless marketing buzzword? What is “single origin”?“Single origin” means that the coffee beans were sourced from a single location, usually a region or country. Nowadays, however, it even goes as far as to mean that the beans were sourced from a single farm, estate or co-operative, which does make a difference to the end product that you drink. There are three prominent industry bodies that were set up to assess the quality of single origin coffee, namely Cup of Excellence, Coffee Review and Coffee Quality Institute. This helps to keep a standard of quality so that consumers know what they’re getting. Where does single origin coffee come from?There are four main coffee bean-growing regions around the world: Central America, South America, Africa and Indonesia. These regions are collectively known as the Coffee Belt or the Bean Belt. Each region produces coffee beans with their own distinctive flavour profile, so of course, every coffee drinker develops a regional preference when it comes to the coffee they like best. Central America (e.g. Costa Rica, Guatemala): Bright and acidic South America (e.g. Brazil, Colombia): Smooth and sweet Africa (e.g. Kenya, Ethiopia): Light and fruity, often citrusy Indonesia: Full-bodied and earthy HOW TO GRIND COFFEE BEANSWhen we brew a cup of coffee, we are extracting soluble flavors from coffee beans with hot water. This is called coffee extraction. Coffee doesn’t dissolve completely into water. In fact, only 30% of coffee is actually soluble. That’s why there are always grounds left over when we brew. When brewing a cup of coffee, we are aiming for the sweet spot of between 18 – 22% coffee extraction. This is the window where coffee tastes delicious. There are many variables that can alter the coffee extraction and the taste of your cup. The grind being one of them. Consistent Grind SizeIf your grounds are uneven sizes, then the extraction of coffee will be inconsistent. The smaller grounds will extract quicker. This means the coffee will over extract, tasting bitter. The larger grounds will take longer to extract. Under extracting coffee tastes sour. **This inconsistent extraction will make your cup of coffee unbalanced and it just won’t taste right. **So to get a better tasting cup of coffee, we must make sure that we grind coffee beans consistently and that they are the same size. A uniform and consistent grind will produce an even coffee extraction, making your final cup of coffee taste balanced. If you’ve brewed your cup of coffee correctly, you’ll have created a sweet and tasty cup of coffee. Not too sour, not too bitter. Grind Size MattersEach brewing device requires a unique grind size. For example, an espresso shot calls for a finer grind, while a French Press requires a courser grind. Grind coffee beans – brew, taste and tweak your grind size accordingly. Burr or Blade Grinder?Go burr! They produce a more consistent grind size than a blade grinder, and your coffee will taste better for it. Burr Grinder Most coffee lovers will tell you that a burr grinder is far superior when it comes to grind size and flavor. While more expensive than a blade grinder, burr mills are widely recognized for their consistency, quality, and overall uniformity. A burr grinder, also called a burr mill, is made up of two revolving burrs in between which the coffee is ground. The beans are crushed between a moving grinder wheel and a non-moving surface. Conical burr grinders are the industry standard when it comes to burr grinders. They use a cone-shaped center burr with an outer serrated burr that helps produce well-ground coffee time and time again. And, its design is naturally energy-efficient and heat resistant, making it a great option for professional and home baristas. But, conical burrs don’t produce evenly ground coffee and if put under a microscope, you’d notice different sizes of bean in the mix. While this won’t impact the overall taste of your cup of coffee, some people do prefer a flat burr grinder for espresso. In fact, a flat burr became popular after it was introduced during the 2013 World Barista Championship. Flat burr grinders feature two donut-shaped burrs that face one another with very sharp edges. This design allows the beans to stay between the burrs until they are perfectly (and symmetrically) ground up, as opposed to conical burrs which can allow the beans to shoot out and stay somewhat intact. When all the coffee or espresso grounds are the same size, the flavor is very one-note, which can give baristas more room for creativity. But, flat burrs are louder than conical burrs and utilize more energy and heat during the grinding process, which makes them less ideal for commercial or even at-home use. But when precision is required, flat burrs are the better option. Blade Grinder A blade grinder is a machine that chops coffee beans and spices while mixing it. There is a blade in the center of the grinder that looks like a propeller, similar to a blade in a blender or a food processor. This grinder offers more power for faster grinding, but coffee grounds can be uneven in size. The bottom line is that you should get the best quality grinder you can afford. Whether that’s a $20 blade grinder or a $100 burr grinder. A bad grinder can turn delicious coffee beans into a watery, sad cup of coffee. PRE-GROUND COFFEE: WHAT’S THE DEAL?Coffee is a perishable food type. When your coffee beans come into contact with oxygen, they start to go stale. The oxygen reacts with the freshly roasted beans and they start to lose their original aroma and flavors. When you grind your coffee, it speeds up this aging process and flavor is lost even quicker. We advise you to buy whole bean coffee and get a burr grinder. Grind your coffee just before you brew so your coffee will taste fresh and full of flavor. And the smell of freshly ground coffee in the morning… oh my!","link":"/Blog/2020/12/20/How-to-Make-the-Best-Pour-Over-Coffee-Like-a-Pro/"},{"title":"Modern Android Architecture via MVVM + JetPack","text":"The first Android Development SDK was released in 2007–14 years ago as of this writing. The Android SDK has evolved–significantly–in that time, yet the basic paradigm of loosely coupled layout (usually in XML files) with code in Java (more recently, Kotlin), has largely remained the same. Now, two years after the launch of Jetpack, we’ve seen tremendous adoption by apps, from large developer teams to those just getting started. App BasicsApp Manifest OverviewEvery app project must have an AndroidManifest.xml file (with precisely that name) at the root of the project source set. The manifest file describes essential information about your app to the Android build tools, the Android operating system, and Google Play. ActivityAn activity is a single, focused thing that the user can do. Almost all activities interact with the user, so the Activity class takes care of creating a window for you in which you can place your UI with #setContentView. To be of use with android.content.Context #startActivity, all activity classes must have a corresponding &lt;activity&gt; declaration in their package’s AndroidManifest.xml. MainActivity.kt: What the app does activity_main.xml: What the app looks like Toasts overviewA toast provides simple feedback about an operation in a small popup. It only fills the amount of space required for the message and the current activity remains visible and interactive. Toasts automatically disappear after a timeout. 1Toast.makeText(context, text, duration).show() Late-initialized properties and variablesNormally, properties declared as having a non-null type must be initialized in the constructor. However, it is often the case that doing so is not convenient. To handle such cases, you can mark the property with the lateinit modifier: 1234567891011public class MyTest { lateinit var subject: TestSubject @SetUp fun setup() { subject = TestSubject() } @Test fun test() { subject.method() // dereference directly }} Tools attributes referenceWhen you build your app, the build tools remove these attributes so there is no effect on your APK size or runtime behavior. Android Studio supports a variety of XML attributes in the tools namespace that enable design-time features (such as which layout to show in a fragment) or compile-time behaviors (such as which shrinking mode to apply to your XML resources). Design-time view attributes The following attributes define layout characteristics that are visible only in the Android Studio layout preview. tools: instead of android: Intended for: &lt;View&gt; LayoutA layout defines the structure for a user interface in your app, such as in an activity. All elements in the layout are built using a hierarchy of View and ViewGroup objects. A View usually draws something the user can see and interact with. Whereas a ViewGroup is an invisible container that defines the layout structure for View and other ViewGroup objects, as shown in figure 1. Load the XML ResourceWhen you compile your app, each XML layout file is compiled into a View resource. You should load the layout resource from your app code, in your Activity.onCreate() callback implementation. 1234fun onCreate(savedInstanceState: Bundle) { super.onCreate(savedInstanceState) setContentView(R.layout.main_layout)} Android styling: themes vs stylesBoth themes and styles use the same &lt;style&gt; syntax but serve very different purposes. You can think of both as key-value stores where the keys are attributes and the values are resources. Let’s take a look at each. What’s in a style? tyles are a collection of view attributes; specific to a single type of widget 1234567&lt;!-- Copyright 2019 Google LLC. SPDX-License-Identifier: Apache-2.0 --&gt;&lt;style name=&quot;Widget.Plaid.Button.InlineAction&quot; parent=&quot;…&quot;&gt; &lt;item name=&quot;android:gravity&quot;&gt;center_horizontal&lt;/item&gt; &lt;item name=&quot;android:textAppearance&quot;&gt;@style/TextAppearance.CommentAuthor&lt;/item&gt; &lt;item name=&quot;android:drawablePadding&quot;&gt;@dimen/spacing_micro&lt;/item&gt;&lt;/style&gt; What’s a theme? A theme is a collection of named resources which can be referenced later by styles, layouts etc. 1234567&lt;!-- Copyright 2019 Google LLC. SPDX-License-Identifier: Apache-2.0 --&gt;&lt;style name=&quot;Theme.Plaid&quot; parent=&quot;…&quot;&gt; &lt;item name=&quot;colorPrimary&quot;&gt;@color/teal_500&lt;/item&gt; &lt;item name=&quot;colorSecondary&quot;&gt;@color/pink_200&lt;/item&gt; &lt;item name=&quot;android:windowBackground&quot;&gt;@color/white&lt;/item&gt;&lt;/style&gt; Reference Android Styling: Themes vs Styles Android Styling: Common Theme Attributes Android Styling: Prefer Theme Attributes Data BindingThe Data Binding Library is a support library that allows you to bind UI components in your layouts to data sources in your app using a declarative format rather than programmatically. Layouts are often defined in activities with code that calls UI framework methods. For example, the code below calls findViewById() to find a TextView widget and bind it to the userName property of the viewModel variable: 123findViewById&lt;TextView&gt;(R.id.sample_text).apply { text = viewModel.userName} The following example shows how to use the Data Binding Library to assign text to the widget directly in the layout file. This removes the need to call any of the Java code shown above. Note the use of @{} syntax in the assignment expression: 12&lt;TextView android:text=&quot;@{viewmodel.userName}&quot; /&gt; Binding components in the layout file lets you remove many UI framework calls in your activities, making them simpler and easier to maintain. This can also improve your app’s performance and help prevent memory leaks and null pointer exceptions. Constraint LayoutConstraintLayout allows you to create large and complex layouts with a flat view hierarchy (no nested view groups). It’s similar to RelativeLayout in that all views are laid out according to relationships between sibling views and the parent layout, but it’s more flexible than RelativeLayout and easier to use with Android Studio’s Layout Editor. reference Build a Responsive UI with ConstraintLayout ConstraintLayout NavigationAndroid Jetpack’s Navigation component helps you implement navigation, from simple button clicks to more complex patterns, such as app bars and the navigation drawer. The Navigation component also ensures a consistent and predictable user experience by adhering to an established set of principles. The Navigation component consists of three key parts that are described below: Navigation graph: An XML resource that contains all navigation-related information in one centralized location. This includes all of the individual content areas within your app, called destinations, as well as the possible paths that a user can take through your app. NavHost: An empty container that displays destinations from your navigation graph. The Navigation component contains a default NavHost implementation, NavHostFragment, that displays fragment destinations. NavController: An object that manages app navigation within a NavHost. The NavController orchestrates the swapping of destination content in the NavHost as users move throughout your app. As you navigate through your app, you tell the NavController that you want to navigate either along a specific path in your navigation graph or directly to a specific destination. The NavController then shows the appropriate destination in the NavHost. Intents and Intent FiltersAn Intent is a messaging object you can use to request an action from another app component. Although intents facilitate communication between components in several ways, there are three fundamental use cases: Starting an activity An Activity represents a single screen in an app. Starting a service A Service is a component that performs operations in the background without a user interface. Delivering a broadcast A broadcast is a message that any app can receive. The rest of this page explains how intents work and how to use them. For related information, see Interacting with Other Apps and Sharing Content. Intent types Explicit intents specify which application will satisfy the intent, by supplying either the target app’s package name or a fully-qualified component class name. Implicit intents do not name a specific component, but instead declare a general action to perform, which allows a component from another app to handle it. Jetpack Compose OverviewJetpack Compose represents a major shift of the Android UI development paradigm–a direction which will be easier, faster and ultimately result in less expensive, higher quality software that better meets the needs of companies field software in a competitive mobile software marketplace. Jetpack combines the existing Android support libraries and components and wraps them into a new set of components (including a couple of new ones) for managing things like background tasks, navigation, paging, and life-cycle management, as well as UI features like emoji and layout controls for various platforms like Android Wear, Auto and TV, as well as some more foundation features like AppCompact and Test. Here’s a round up of the latest updates in Jetpack — an extended version of our What’s new in Jetpack talk! Online Courses and CertificationUdacity - Developing Android Apps with Kotlin Learn to architect and develop Android apps in the Kotlin programming language using industry-proven tools and libraries. Create apps in less time, writing less code, with fewer errors. Google - Jetpack Compose Learn about Compose, a modern toolkit for building native Android UI. Associate Android Developer Certification Accelerate your move toward a career in mobile app development. Learn to build simple Android apps with our Android Basics in Kotlin training — no programming experience necessary. Then, take the Associate Android Developer Certification exam to gain recognition for your skills as a developer.","link":"/Blog/2021/08/04/Modern-Android-Architecture-via-MVVM-JetPack/"},{"title":"Not Enough Data? Deep Learning to the Rescue!","text":"What a data scientist to do if they lack sufficient data or suffer from extreme imbalanced dataset to train a deep learning model? The answer definitely is using IBM’s Lambada AI generates training data for text classifiers. Here is an full implementation of the paper ‘Not Enough Data? Deep Learning to the Rescue!‘ with code. LAMBADA AI method overviewAn interesting approach to generate training utterances called LAMBADA (language-model-based data augmentation) has been published by IBM Research AI. The underlying idea is to take a language model, which has been pretrained on large corpora such as Wikipedia and books, that is able to generate textual output of good quality. This language model is then fine-tuned on the available domain specific data. After fine-tuning, the model can then be used to generate additional utterances. These utterances in turn improve the training of Intent Classification models. Reference Scientific Paper [1] Anaby-Tavor, Ateret, et al. “Do not have enough data? Deep learning to the rescue!.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 05. 2020. Blog IBM’s Lambada AI generates training data for text classifiers LAMBADA AI Method: Mastering Conversational Systems with Data Augmentation LAMBADA Method: How to use Data Augmentation in NLU?","link":"/Blog/2021/10/26/Not-Enough-Data-Deep-Learning-to-the-Rescue/"},{"title":"A Comprehensive Look at The Empirical Performance of Equity Premium Prediction","text":"A comprehensive interpretation of paper A Comprehensive Look at The Empirical Performance of Equity Premium Prediction 数据被解释变量就是股票超额收益（equity premium） Our dependent variable is always the equity premium, that is, the total rate of return on the stock market minus the prevailing short-term interest rate. 解释变量有以下三种：与股票特征相关的变量，与利率相关的变量以及市场上的宏观变量： 股票特征相关变量 stock characteristics 股息 Dividends ： d/p (dividend price ratio) &amp; d/y (dividend yield) Dividends are 12-month moving sums of dividends paid on the S&amp;P 500 index. 收入 Earnings ：e/p (earnings price ratio) &amp; d/e (dividend payout ratio) Earnings are 12-month moving sums of earnings on the S&amp;P 500 index. 股票方差 Stock Variance (svar)： 股票方差是以标准普尔500指数日回报的平方之和计算的 Stock Variance is computed as sum of squared daily returns on the S&amp;P 500. 横截面溢价 Cross-Sectional Premium (csp)：测度了高 beta 股票和低 beta 股票的相对估值。 账面价值 BookValue 公司发行活动 Corporate Issuing Activity： Net Equity Expansion (ntis) is the ratio of 12-month moving sums of net issues by NYSE listed stocks divided by the total end-of-year market capitalization of NYSE stocks. Percent Equity Issuing (eqis), is the ratio of equity issuing activity as a fraction of total issuing activity. 利率相关变量 国库券 Treasury Bills (tbl) 长期收益率 Long Term Yield (lty) 公司债券收益率 Corporate Bond Returns 企业债券收益率 Corporate Bond Yields 默认收益率差 Default Yield Spread (dfy) 违约收益率 Default Return Spread (dfr) 通胀率 Inflation (infl) 宏观变量 投资对资本的比率 Investment to Capital Ratio (i/k) i/k = 总投资 / 整个经济体的总资本 方法 Simple univariate model ‘‘Kitchen Sink’’ Regression (all) 包括上述所以的变量（它不包括cay,部分原因是数据的可用性有限）。 Consumption, wealth, income ratio (cay)$$c_{t}=\\alpha+\\beta_{a} \\cdot a_{t}+\\beta_{y} \\cdot y_{t}+\\sum_{i=-k} b_{a, i} \\cdot \\Delta a_{t-i} +\\sum_{i=-k}^{k} b_{y, i} \\cdot \\Delta y_{t-i}+\\epsilon_{t} \\t=k+1, \\ldots, T-k$$where c is the aggregate consumption, a is the aggregate wealth, and y is the aggregate income. Because the cay is constructed using look-ahead (in-sample) estimation regression coefficients, we also created an equivalent measure that excludes advance knowledge from the estimation equation and thus uses only prevailing data. In other words, if the current time period is ‘s’, then we estimated using only the data up to ‘s’ through$$c_{t}=\\alpha+\\beta_{a}^{s} \\cdot a_{t}+\\beta_{y}^{s} \\cdot y_{t}+\\sum_{i=-k}^{k} b_{a, i}^{s} \\cdot \\Delta a_{t-i} +\\sum_{i=-k}^{k} b_{y, i}^{s} \\cdot \\Delta y_{t-i}+\\epsilon_{t} \\t=k+1, \\ldots, s-k$$This measure is called caya (‘‘ante’’) to distinguish it from the traditional variable cayp constructed with look-ahead bias (‘‘post’’). model selection (ms) 如果有 $k$ 个变量，就会有 $2^k$ 个随机的模型组合方式。在每个时期 $t$，选出一个最好的模型 —— 标准是 OOS 的预测误差最小（minimum OOS prediction errors）。 The latter two models, cay and ms, are revised every period, which render IS regressions problematic. This is also why we did not include caya in the kitchen sink specification. 预测 首先，将 T 个样本分为 m 个样本内数据和 p 个样本外数据； 其次，为了预测第 m+1 期的值，我们要用前 m 期共 m-1 个有效数据回归，得到系数 α 和 β； 最后，代入第 m 期的解释变量求第 m+1 期的 r; 然后，预测第 m+2 期的 r, 此时第 m+ 1 期的是所有真实数据都已知了，用前 m+1 期共 m 个有效数据回归，再得到系数 α 和 β（与第一次的可能不同）；代入第 m+1 期的解释变量求第 m+2 期的 r; 重复以上过程，直到把 q 个样本外预测做完。 实证分析 Empirical ProcedureOOS StatisticeN 表示 OOS 与历史均值（无条件预测）之间的误差；eA 表示 OOS 与 OLS 回归模型（条件预测）之间的误差。 $$R^{2}=1-\\frac{\\mathrm{MSE}{A}}{\\mathrm{MSE}{N}}$$ $$\\overline{R}^{2}=R^{2}-\\left(1-R^{2}\\right) \\times\\left(\\frac{T-k}{T-1}\\right)$$ $$\\Delta \\mathrm{RMSE}=\\sqrt{\\mathrm{MSE}{N}}-\\sqrt{\\mathrm{MSE}{A}}$$ $$\\operatorname{MSE}-\\mathrm{F}=(T-h+1) \\times\\left(\\frac{\\mathrm{MSE}{N}-\\mathrm{MSE}{A}}{\\mathrm{MSE}_{A}}\\right)$$ For our encompassing tests in Section 6, we compute $$\\mathrm{ENC}=\\frac{T-h+1}{T} \\times \\frac{\\sum_{t=1}^{T}\\left(e_{N t}^{2}-e_{N t} \\cdot e_{A t}\\right)}{\\operatorname{MSE}_{A}}$$ 重抽样 Bootstrap论文We then generate 10,000 bootstrapped time series by drawing with replacement from the residuals. The initial observation—preceding the sample of data used to estimate the models—is selected by picking one date from the actual data at random. This bootstrap procedure not only preserves the autocorrelation structure of the predictor variable, thereby being valid under the Stambaugh (1999) specification, but also preserves the cross-correlation structure of the two residuals. 我们 Moving Block BootstrapBootstrap **它的核心思想是通过使用数据本身，从而估计从该数据中计算出来的统计数据的变化。**现代计算机强大的计算能力使得该方法的实现非常简单。 放到参数估计的上下文中，Bootstrap 意味着我们仅仅通过使用手头上的样本数据（样本数据 “自力更生”）而不对总体的分布做任何假设（比如传统方法中的正态分布假设），来计算样本统计量在估计总体统计量时的误差。 Bootstrap 原则指出：Bootstrap 样本统计量 u* 围绕原始样本统计量 u 的变化（简称为 u* 的变化）是 原始样本统计量 u 围绕总体统计量 v 的变化（简称为 u 的变化） 的一个很好的近似。 为了计算 u* 的变化，我们只需要对原始样本数据进行大量的可置换重采样。 Block Bootstrap The block bootstrap is used when the data, or the errors in a model, are correlated. In this case, a simple case or residual resampling will fail, as it is not able to replicate the correlation in the data. The block bootstrap tries to replicate the correlation by resampling instead blocks of data. **由于时间序列存在自相关性，因此在重采样的时候应使用 Block Bootstrap。**顾名思义，Block Bootstrap 就是每次从序列中有放回的抽取一个由连续 n 个相邻数据点构成的 block（大小由 block size 决定）。主流的 Block Bootstrap 算法包括以下三种： Moving Block Bootstrap（Kunsch 1989, Liu and Singh 1992）； Circular Block Bootstrap（Politis and Romano 1992）; Stationary Bootstrap（Politis and Romano 1994）。 下图说明了 Moving Block Bootstrap（MBB）的原理： 从上图的原理可知，MBB 最大的问题是对于原始序列首尾两端样本采样不足。为了规避这个问题，Circular Block Bootstrap（CBB）被提出。顾名思义，它是将原始数据的首尾相连，构成一个圆圈（Circular 一词的出处），然后再按照给定的 block size 进行重采样，避免首尾两端采样不足。 最后一种方法是 Stationary Bootstrap（SB），它和前两者最大的区别是使用非固定的 block size。SB 中的 block size 满足几何分布；作为输入而给定的 block size 是它的期望。该方法得到的 bootstrapped 样本可以更好的满足平稳性的要求，因此当原始时间序列难以满足平稳性时有更好的效果。 Statistical PowerOur article entertains both IS and OOS tests. Inoue and Kilian (2004) show that the OOS tests used in this paper are less powerful than IS tests. We believe this is the wrong way to look at the issue of power for two reasons: In our forecasting regression context, OOS performance just happens to be one natural and especially useful diagnostic statistic. It can help determine whether a model is stable and wellspecified, or changing over time, either suddenly or gradually. It is unreasonable to propose a model if the IS performance is insignificant, regardless of its OOS performance. All of the OOS tests in our paper do not fail in the way the critics suggest. Low-power OOS tests would produce relatively poor predictions early and relatively good predictions late in the sample. Instead,allofourmodelsshowthe opposite behavior—good OOS performance early, bad OOS performance late. Estimation Period The first begins OOS forecasts 20 years after data are available; The second begins OOS forecast in 1965 (or 20 years after data are available, whichever comes later); The third ignores all data prior to 1927 even in the estimation. 结论 大多数模型是不稳定的、甚至是虚假的。即使单个变量模型在某段时间内具有良好的样本外预测能力，这种预测能力也很难持续，比如经济结构不稳定或结构变化。 到 2005 年末为止，大多数模型无论是在 IS 还是在 OOS 中都丧失了统计显著性。在 OOS 中，大多数模型不仅不能在统计意义上或经济意义上打败无条件基准水平（历史均值），而且表现的还不如它。如果我们把目光集中在 1975 年以后的时间里，我们会发现，没有哪一个模型在 OOS 中有突出的表现，而且也几乎没有可接受的 IS 显著水平。 当我们把视角从研究者转向为投资者时，我们相信有证据表明这些模型并不能给今天的投资提供支持或建议。 参考用 Bootstrap 进行参数估计大有可为","link":"/Blog/2019/05/01/A-Comprehensive-Look-at-The-Empirical-Performance-of-Equity-Premium-Prediction/"},{"title":"OpenRouter 的 100 万亿 Tokens 实证研究","text":"2025年12月，OpenRouter发布了基于其平台100万亿Tokens使用数据的实证研究报告，全面揭示了当前真实的AI交互模式。这些发现极具启发性，为数据驱动的LLM系统设计与优化提供了重要参考：报告深入分析了开发者和终端用户在不同任务中调用模型的情况、模型与任务的双向匹配关系、使用模式随地理区域和时间的变化规律，以及定价和新模型发布等外部因素对用户行为的影响。 本文重点分析开源生态、智能体发展趋势和用户留存机制，省略了OpenRouter关于地理区域（因缺少中国样本）和成本定价的分析（后续单独讨论）。 [TOC] Summary基于OpenRouter平台处理的100万亿Tokens数据分析，本研究揭示了大语言模型生态系统的关键发展趋势： 第一章分析开源生态演进。随着DeepSeek的崛起，LLM生态系统呈现出稳定的双重结构：开源与闭源模型形成30% vs 70%的平衡格局。闭源系统继续定义性能上限，而开源模型凭借成本效益和可定制性优势，成为特定工作负载的首选。中国开源模型从2024年末几乎为零的基数稳步增长，周占比最高接近30%，平均占比13.0%。模型市场明显分化：小型模型从60%降至12%，逐渐被市场淘汰；中型模型从0%增长至30%，成为重要市场；大型模型从40%提升至50%，成为主流选择。这种分化反映了市场的成熟，用户不再需要在极端之间权衡，而是根据需求选择中型模型（平衡成本效率）或大型模型（追求最佳效果）。 第二章探讨智能体推理的兴起。推理模型占比从低位稳步上升至超过50%，成为实际工作负载的默认选择。工具调用逐步上升至15%，主要集中在针对智能体推理优化的模型中。提示词长度从1.5K增长至6K以上（增长4倍），补全长度从150增长至400（增长3倍），编程是提示词增长的主要驱动力，编程提示词平均长度是通用提示词的3-4倍。模型正越来越多地扮演分析引擎角色，处理大量材料并生成高价值见解。 第三章分析模型在不同业务中的不均匀发展。编程是主导且持续增长的类别，从11%增长至50%以上，LLM已深度融入开发者工作流程。Claude持续占据60%编程市场份额，但近期出现下滑迹象，首次跌破60%阈值；OpenAI在编程领域从2%快速扩大至8%，Google稳定在15%。现实世界中LLM使用高度集中在少数可重复、高频次任务上，而非均匀探索。 第四章揭示了顶级实验室的野心：You Get What You Trained, and You Train What You What。各大顶尖公司的战略意图在模型的能力上体现的淋漓尽致：Anthropic 的 Claude 主要用于编程和技术任务，占比超过80%；Google 的模型用途多样化，涵盖法律、科学、技术以及常识性查询；DeepSeek 的使用主要体现在角色扮演和日常互动上，占比超过66%；Qwen 在编程任务上专注度较高，占比40%-60%。一个多模型的生态系统正在形成，无单一模型能覆盖所有使用场景。 第五章重点分析用户留存机制。基础用户群代表工作负载与模型已实现深度适配，一旦适配确立就会产生经济和认知惯性。模型与工作负载完美匹配的”灰姑娘时刻”转瞬即逝，只出现在模型被视为”前沿”的那一刻。DeepSeek展现出”回旋镖效应”，部分流失用户在尝试其他方案后回归，确认其专业性能和成本效益优势。基础用户群体是技术进步的真正标志，标志着AI模型从新奇事物转变为必需品的转折点。 一、开源生态：双重结构，DeepSeek，中型模型与角色扮演1.1 持久的双重结构：70% vs 30% 总结：大语言模型生态系统中存在一种持久的双重结构：开源模型与闭源模型。目前的平衡点约为30%。 专有系统继续定义着可靠性和性能的上限，特别是在受监管或企业工作负载方面。相比之下，开源模型具有成本效益、透明度和可定制性，使其成为某些工作负载的有吸引力的选择。这些模型并非相互排斥，相反，它们在开发者和基础设施提供商日益青睐的多模型堆栈中相互补充。 开源模型使用量的增长与主要开源模型的发布时间高度吻合，这表明像DeepSeek这样有竞争力的开源项目，能够在发布后迅速获得市场认可并保持增长势头。 从2024年末几乎可以忽略不计的基数（周占比低至1.2%）开始，中国开源模型稳步获得关注，某些周内占所有模型总使用量的比例接近30%。这一年中，中国开源模型的周Tokens量平均占比约13.0%，强劲增长主要集中在2025年下半年。相比之下，其他地区开源模型平均占13.7%，而专有模型保持最大份额（平均70%）。中国开源模型的扩张不仅体现了其竞争力，还反映了快速迭代和密集发布周期。像Qwen和DeepSeek这样的模型保持定期发布，能够快速适应新兴工作负载。这种模式极大重塑了开源领域，推动了全球大语言模型领域的竞争。 1.2 主要玩家：DeepSeek vs Others 总结：DeepSeek 一骑绝尘，但主导地位有所下降，整个开源生态在朝着多元化的方向发展。 下表按模型划分的总 Tokens 量（2024年11月–2025年11月），反映了 OpenRouter 上所有模型的总使用量。 Model Author Total Tokens (Trillions) 总令牌数（万亿） DeepSeek 14.37 Qwen 5.59 Meta LLaMA 3.96 Mistral AI 2.92 OpenAI 1.65 Minimax 1.26 Z-AI 1.18 TNGTech 1.13 MoonshotAI 0.92 Google 0.82 **这种近乎垄断的格局在”夏季拐点”（2025年年中）后被打破。**此后，市场变得更加复杂，用途也大幅多样化。通义千问、Minimax的M2、MoonshotAI的Kimi K2以及OpenAI的GPT-OSS系列等新进入者迅速发展，承接大量需求，往往在发布后几周内就实现生产级别应用。这表明，开源社区和AI初创企业通过推出具备新颖功能或更高效率的模型，能够快速获得市场认可。 如今，没有任何单一开源模型的Tokens消耗超过整个生态的25%，分布更加均衡。这说明一个重要事实：用户正在从多样化选择中发现价值——无论是风格还是能力——而非直接默认选择一个最佳选项。 **顶级多样性：**曾经由DeepSeek主导开源生态，现在各模型保持可观份额。没有任何开源模型能持续占据超过20%-25%的市场份额。 **新进者的快速扩张：**性能出众新型开放模型能在几周内获得大量使用。例如，MoonshotAI模型迅速发展，可与老牌开源领军者抗衡，甚至像MiniMax这样的新入局者在一个季度内就从零做到可观流量。这表明用户转换成本低，且用户群体乐于尝试新事物。 **迭代优势：**DeepSeek长期位居榜首，凸显了持续改进的重要性。其连续发布（Chat-V3、R1等）使其在挑战者涌现时仍保持竞争力。那些停滞不前的开源模型，其市场份额往往被频繁更新或针对特定领域微调的模型抢占。 1.3 模型规模的趋势：Medium vs Large/Small 总结：小模型已经成为过去式，中型的模型占据重要市场，而大模型成为绝对意义上的主流。 市场变得成熟的一个标志就是中型模型的产生和发展，用户无需再两个极端之间权衡。市场的分化主要在于目标的选择，要么倾向于使用中型模型（要在成本和效率之间权衡），要么只用大型模型（将工作负载整合到能力最强的模型上，获得最佳的效果和智慧）。 OpenRouter 根据参数数量对模型进行如下分类： **小型：**参数少于 150 亿的模型。 **中型：**参数规模在 150 亿到 700 亿之间的模型。 **大型：**具有 700 亿或更多参数的模型。 深入研究推动这些趋势的模型，可以发现不同的市场动态： 可以看出，小模型市场从最初的60%逐步下降至12%，且趋势没有回弹迹象，说明小模型正被市场淘汰。中型模型经过一年发展，已占据约30%市场份额且非常稳定，说明这个市场需求长期可持续。大模型占比也从40%提升至50%，说明大参数量模型仍是用户和企业的首选。 “小型”模型的市场：整体使用率下降。 尽管新模型不断涌现，但小型模型类别整体的使用份额正在下降。 这一类别具有高度碎片化的特点。没有任何单一模型能长期占据主导地位，而且来自Meta、谷歌、Mistral和深度求索等各类提供商的新进入者不断更迭。例如，Google Gemma 3.12B（2025年8月发布）获得了快速采用，但它所处的领域竞争激烈，用户会不断寻找下一个更优的替代方案。 “中型”模型的市场：寻找“模型-市场契合点”。 中等规模模型类别清晰地讲述了一个市场创造的故事：直到2024年11月Qwen2.5 Coder 32B发布后，这一细分领域才算是确定出现，在此之前，这个市场可以说几乎微不足道。 这一领域表明，用户正在寻求能力与效率之间的平衡。 随着Mistral Small 3（2025年1月）和GPT-OSS 20B（2025年8月）等其他强劲竞争者的出现，这一领域逐渐发展成为一个竞争激烈的生态系统，这些模型也赢得了用户的关注。 “大型”模型领域：多元化格局。 “追求质量”并未导致市场整合，反而促进了多样化发展。如今，大型模型类别中涌现出一系列高性能的竞争者，从Qwen3 235B A22B Instruct（2025年7月发布）和Z.AI GLM 4.5 Air，再到OpenAI: GPT-OSS-120B（8月5日发布），每一款都拥有可观且持续的使用率。 这种多元化说明：用户正积极地在多个开源大型模型之间进行比较与采纳，而非集中采用单一的标准来评价模型。 1.4 开源模型的业务匹配：角色扮演+编程 vs OthersOpenRouter通过非专有模块GoogleTagClassifier，对占所有提示词约0.25%的随机样本进行内部分类。虽然仅占总活动的一小部分，但考虑到OpenRouter处理的整体查询量，基础数据集仍然相当庞大。GoogleTagClassifier与谷歌云自然语言的classifyText内容分类API相连接。分类细节放在附录中。 1.4.1 全球趋势：角色扮演和编程 总结：虽然专有模型在结构化的商业任务中仍占主导地位，但开源模型已在两个特定领域确立了领先地位：创意角色扮演和编程辅助。这两个类别共同占据了开源模型 Tokens 使用量的大部分。 数据：角色扮演占据 52% 的市场份额，而编程大致为 20%（编程与科技总计为 33%）。 上图清晰显示，超过一半的开源模型使用属于角色扮演，而编程是第二大类别。 这表明用户转向开放模型主要是为了创造性交互式对话（如讲故事、角色扮演和游戏场景）和编码相关任务。 角色扮演的主导地位（达到50%及以上）说明开源模型的持续优势：可用于创造力，且通常不受内容过滤器限制，对幻想或娱乐应用极具吸引力。 角色扮演任务需要灵活响应、上下文保留和情感细微差别——开放模型能有效提供这些属性，不受商业安全或审核层严重限制。这使得它们对角色驱动体验、粉丝向小说撰写、交互式游戏和模拟社区特别有吸引力。 1.4.2 中国开源趋势：强编程，技术与生产力 总结：中国模型的主要任务在于编程与技术，而非主要创意。 数据：角色扮演占 33%（低于全球平均值 52%），编程占 39%（高于全球平均值 33%） 如果只聚焦中国开源模型随时间的细分情况，可以看出：这些模型不再主要用于创意任务。角色扮演仍是最大类别，占比约33%，但编程和技术领域使用量合计已占多数（39%）。 这种转变表明，像Qwen和DeepSeek这样的模型正越来越多地用于代码生成和基础设施相关工作负载。虽然大量企业用户可能影响特定领域，但总体趋势表明，中国开源模型正在技术和生产力领域直接竞争。 1.4.3 编程开源生态：依旧闭源主导，但中国开源生态蓬勃 总结：闭源模型依旧主导市场，中国开源生态起步较早（Qwen 3 Coder），但西方模型比例有所上升，并观察到市场竞争十分激烈，比例变化大，说明用户粘性低，新模型凭借优异表现可迅速抢占市场。 如果只聚焦编程类别，会发现闭源模型仍处理大部分编码辅助工作（灰色区域），反映出像Anthropic的Claude等强大产品表现。 然而在开源部分，出现显著转变：2025年中期，中国开源模型（蓝色）提供大部分开源编码帮助（得益于Qwen 3 Coder等早期成功案例）。到2025年第四季度，西方开源模型（橙色），如Meta的LLaMA-2 Code和OpenAI的GPT-OSS系列，出现激增，但最近几周总体占比有所下降。这种波动表明市场竞争非常激烈。 **实际结论是，开源代码助手使用情况高度动态变化，对新模型质量反应强烈：开发者愿意接受任何当前能提供最佳编码支持的开源模型。**需要说明，该图表未显示绝对数量：开源编码使用量整体在增长，因此蓝色部分占比缩小不意味着中国开源模型失去用户，只是相对份额有所变化。 1.4.4 角色扮演开源生态：与闭源生态评分秋色，中国开源也占有开源生态的一席之地 总结：在角色扮演生态中，开源占比非常高，达到60%，且开源生态占比逐步上升。可以确认开源生态在该领域具有天然优势，也可预见未来开源生态很可能主导该市场。到2025年底，中国和世界其他开源模型流量大致平分。 数据：开源模型占角色扮演领域60%市场，开源生态中，中国开源生态占20%-30%，其他开源生态占30%-40%。 如果只考察角色扮演流量，会发现其目前几乎由世界其他地区开源模型（橙色，近几周占43%）和闭源模型（灰色，最近约占42%）平分秋色。这与2025年初相比发生显著变化，当时该类别由专有模型（灰色）主导，约占70%市场份额。彼时（2025年5月），西方开源模型仅占约22%流量，中国开源模型（蓝色）占比更小，约8%。全年中，闭源模型份额稳步下降。到2025年10月底，随着西方和中国开源模型均取得显著进展，这一趋势进一步加速。 可以得出结论，角色扮演领域存在良性竞争；用户在创意聊天和故事讲述方面，既有开源产品也有闭源产品可供选择，且选择都切实可行。这反映出开发者意识到角色扮演/聊天模型需求，并为此对发布模型进行针对性调整（如在对话上进行微调，为角色一致性添加对齐机制）。需注意，”角色扮演”涵盖一系列子类型（从休闲聊天到复杂游戏场景）。 从宏观角度看，开源模型在这个创意领域显然具有优势。 二、智能体推理的兴起序：语言模型在生产环境中的使用方式正发生根本性转变：从单轮文本补全转向多步骤、工具集成且推理密集型的工作流。OpenRouter将这种转变称为智能体推理的兴起，即模型部署不仅为生成文本，还通过规划、调用工具或在扩展语境中交互来采取行动。本节通过五个指标追踪这一转变：推理模型兴起、工具调用行为扩展、序列长度分布变化，以及编程用途如何推动复杂性。 下面分析的趋势（推理占比上升、工具使用范围扩大、序列变长以及编程的极大复杂性）共同表明，大语言模型（LLM）使用重心已发生转移。典型LLM请求不再是简单问题或孤立指令，而是结构化、类智能体循环的一部分，会调用外部工具、基于状态推理，并在更长语境中持续运行。 对模型提供商而言，这提高了对默认能力要求。延迟、工具处理、上下文支持及鲁棒性变得愈发关键。对基础设施运营商来说，推理平台现在不仅要管理无状态请求，还要处理长时间运行对话、执行轨迹及权限敏感工具集成。 即便现在还未实现，也很快，智能体推理将占据大部分推理工作。 2.1 Reasoning调用量稳步上升至一半市场 总结：自2025年初以来，通过推理优化模型处理的所有Tokens占比稳步上升，以推理为导向的模型正成为实际工作负载的默认路径。 该指标反映推理模型处理的所有Tokens比例，而非模型输出中”推理Tokens”占比。 第一季度初，这一使用占比很低，现在已超过50%。这种转变反映市场两个方面： 供给端，GPT-5、Claude 4.5和Gemini 3等更高能力模型系统发布，提升用户对逐步推理期望。 需求端，用户越来越倾向于能够管理任务状态、遵循多步骤逻辑并支持智能体式工作流程模型，而仅仅是简单生成文本模型。 在推理模型中，xAI的Grok Code Fast 1目前处理推理相关流量占比最大，其次是谷歌的Gemini 2.5 Pro和Gemini 2.5 Flash。xAI的Grok 4 Fast和OpenAI的gpt-oss-120b也跻身顶级行列。 在最新数据中，xAI的Grok Code Fast 1目前在推理流量中占据最大份额（不包括免费发布版访问），领先于谷歌的Gemini 2.5 Pro和Gemini 2.5 Flash。这与几周前情况相比有显著变化，当时Gemini 2.5 Pro在该类别中处于领先地位，DeepSeek R1和Qwen3也位居顶级行列。借助xAI积极推出策略、具有竞争力定价以及开发者对其代码导向型变体关注，Grok Code Fast 1和Grok 4 Fast迅速获得市场份额。与此同时，像OpenAI的gpt-oss-120b这样开源模型持续存在，凸显开发者在可能情况下仍会选择开源模型。 数据指向明确结论：以推理为导向的模型正成为实际工作负载的默认路径，而流经这些模型的Tokens所占市场份额，现已成为AI系统交互的主要指标。 2.2 工具调用逐步上升至15% 总结：在高价值工作流中，启用工具使用的趋势正在上升。无法提供可靠工具调用的模型在企业应用中可能会落后。 数据：来自Tool Call的请求占总请求的15%。 上图中，OpenRouter报告了来自完成原因为Tool Call的请求的总Tokens占比。该指标经过标准化且仅包含实际调用工具的那些交互。 OpenRouter解释上图中5月份显著峰值主要归因于一个大型账户，其活动短暂提升整体交易量。除这一异常情况外，工具采用率在全年呈现持续上升趋势。 工具调用集中在针对Agent Inference明确优化的模型中，例如Claude Sonnet、Gemini Flash。 可以看出，工具调用最初集中在几个模型中：OpenAI的gpt-4o-mini和Anthropic的Claude 3.5和3.7系列，它们在2025年初占了大多数支持工具调用的市场。 然而到年中，更广泛模型开始支持工具调用，从9月底开始，较新的Claude 4.5 Sonnet模型迅速获得份额。与此同时，Grok Code Fast和GLM 4.5等较新模型取得明显进展。这也说明在工具调用领域，多元化格局正在形成。 含义显而易见：在高价值工作流中，启用工具使用的趋势正在上升。没有提供可靠工具格式的模型在企业应用中可能会落后。 2.3 序列长度增长主要驱动力为编程总结：提示词长度、补全长度均大幅提升，主要推动因素是编程需求。模型正越来越多扮演分析引擎角色，而非创意生成器。 数据：从2024年初，Prompt长度增长四倍（从约1.5K增至6K以上），Completion长度增长三倍（约150增至400），涉及代码理解、调试和代码生成的请求通常超过20K输入tokens。 自2024年初以来，平均提示词Token长度增长近四倍，反映出工作负载上下文越来越复杂。 **每次生成的平均令牌数（提示词+补全内容）增长近三倍，从2000增长至6000 Tokens。**序列长度是任务复杂度和交互深度的代表指标。上图显示，过去20个月里，平均序列长度增加两倍多。这种增长反映结构性转变，即朝着更长上下文窗口、更深任务历史及更详尽补全内容方向发展。 输出长度也有所增加，尽管起点较低，这表明更丰富、更详细的响应主要源于推理Tokens。 自2025年春季开始提供标签以来，与编程相关的任务始终需要最大输入上下文。且迅速拉开与其他领域需求差距。 **编程提示词通常更长，且增长速度更快。**与编程相关的提示词现在平均token长度是通用提示词的3-4倍。这种差异表明，软件开发工作流是更长交互的主要驱动因素。较长序列不仅是用户冗长表达：它们是嵌入的、更复杂智能体工作流的标志。 这种增长的相对幅度凸显了向更复杂、上下文更丰富工作负载的决定性转变。 这种模式还反映模型使用的新平衡：如今，典型请求不再那么侧重开放式生成（如”给我写一篇文章”），而更多是对用户提供的大量材料（如代码库、文档、文字记录或冗长对话）进行推理，并生成简洁、高价值见解。模型正越来越多扮演分析引擎角色，而非创意生成器。 Category级数据呈现更细致图景：编程工作负载是提示词token增长的主要驱动力。涉及代码理解、调试和代码生成的请求通常超过20K输入Tokens，而所有其他类别请求则相对平稳且数量较少。这种不对称贡献表明：最近提示词长度增长并非所有任务的普遍趋势，而是与软件开发和技术推理用例相关的集中式增长。 三、通过产业标签分析人类交互行为序：了解用户使用大语言模型执行的任务分布，对评估实际需求和模型与市场契合度至关重要。 3.1 毫无疑问的主导：编程 总结：编程是主导且不断增长的类别。LLM已融入开发者工作流程，实现常态化。这个领域持续吸引各大顶级实验室关注。 数据：Claude持续占据60%市场份额，但最近有下滑迹象。谷歌稳定在15%，OpenAI份额在最近几周从约2%扩大至约8%。 编程是主导且不断增长的类别。 被归类到编程类别的所有大语言模型查询占比稳步上升，反映人工智能辅助开发工作流的兴起。 编程已成为所有模型中扩张最稳定的类别。2025年期间，与编程相关的请求占比稳步上升，与大语言模型辅助的开发环境及工具集成相呼应。如上图所示，2025年初，编程查询约占总Tokens量的11%，而最近几周已超过50%。这一趋势反映用户使用从探索性或对话性用途转向代码生成、调试和数据脚本编写等应用型任务。 随着大语言模型融入开发者工作流程，它们作为编程工具的角色正逐渐常态化。这一演变对模型开发具有重要意义，包括： 更加强调以代码为中心的训练数据 提升多步骤编程任务的推理深度 加强模型与集成开发环境之间的反馈循环 对编程支持需求的不断增长正在重塑各模型提供商之间的竞争格局。 Anthropic的Claude系列在这一领域始终占据主导地位，在大部分时间里，其在编程相关市场中占比超过60%。尽管如此，这一格局仍发生显著变化。11月17日那一周，Anthropic份额首次跌破60%阈值。自7月以来，OpenAI份额在最近几周从约2%扩大至约8%，这可能反映其重新强调以开发者为中心。 同一时期，谷歌份额稳定在约15%。中端市场也在发生变化。包括Z.AI、Qwen和Mistral AI在内的开源提供商正稳步获得更多关注。尤其是MiniMax，作为快速崛起的参与者，在最近几周取得显著增长。 总体而言，编程已成为竞争最激烈且具有重要战略意义的模型类别之一。它持续吸引顶尖实验室关注，即使是模型质量或延迟方面的微小变化，也可能在每周改变市场份额。对基础设施提供商和开发者来说，这凸显持续进行基准测试和评估的必要性，尤其是在技术前沿不断发展的情况下。 3.2 产业探索的非均匀分布 总结：大多数Category并非均匀分布，它们由一两种反复出现的使用模式主导（如角色扮演、科学和编程），这往往反映集中的用户意图或与大语言模型优势的契合。 也有些领域反映使用的分散性：如金融、学术和法律，这种分散性可能反映这些领域的复杂性，或仅是与编码和聊天等更成熟类别相比，它们缺乏针对性的大模型工作流程。 分散性一方面反映领域复杂性，另一方面也说明大模型在现实生活中的探索是非均匀的。 每个条形图显示该Category中主要子标签的细分情况。标签表示在该类别中占比至少7%的子标签。 上图按十二个最常见的内容类别细分大语言模型使用情况，揭示每个类别的内部子主题结构。 一个关键发现是，大多数类别并非均匀分布：它们由一两种反复出现的使用模式主导，这往往反映集中的用户意图或与大语言模型优势的契合。 【角色扮演】 在Tokens量最大的类别角色扮演中，近60%的角色扮演标记属于游戏/角色扮演游戏，这表明用户不将大语言模型视为随意的聊天机器人，而更多将其视为结构化的角色扮演或角色生成引擎。作家资源（15.6%）和成人内容（15.4%）的存在进一步印证这一点，它们体现互动小说、场景生成和个人幻想的融合。 与认为角色扮演主要是非正式对话的假设相反，数据显示这是一种定义明确且可复制的基于类型的使用场景。 【编程】 编程的情况也类似: 超过三分之二的流量被标记为编程/其他。这表明与代码相关的提示具有广泛和通用的性质：用户并非狭隘地关注特定工具或语言，而是向大语言模型提出从逻辑调试到脚本起草等各种需求。 开发工具（26.4%）以及来自脚本语言的少量占比表明出现专业化趋势。 【其他】 除角色扮演和编程这两个主要类别外，其余领域代表大语言模型使用中多样化但体量较小的部分。虽然这些领域各自规模较小，但它们揭示用户在专门任务和新兴任务中与模型交互的重要模式。 翻译、科学和健康领域呈现相对平稳的内部结构。 翻译领域，使用量几乎平均分配在外语资源（51.1%）和其他之间，这表明存在分散的需求：多语言查询、重新措辞、简单的语码转换，而非持续的文档级翻译。 科学领域由单一标签机器学习与人工智能主导（80.4%），这表明大多数科学查询是关于元人工智能的问题，而非像物理或生物学这样的一般STEM主题。这反映用户兴趣或模型优势偏向于自我指涉性探究。 健康是Top类别中最分散的，没有任何子标签的占比超过25%。标记分布在医学研究、咨询服务、治疗指导和诊断查询等多个方面。这种多样性凸显该领域的复杂性，也带来安全建模的挑战：大语言模型必须涵盖差异极大的用户意图，且这些意图往往出现在敏感场景中，却没有集中在单一用例上。 长尾类别的共同之处在于它们的广泛性：用户借助大语言模型进行探索性、结构松散或寻求帮助的交互，但没有编程或个人助理领域中那种专注的工作流程。总体而言，这些次要类别可能在数量上不占主导，但它们暗示潜在的需求。 这也表明大语言模型正被应用于从翻译到医疗指导再到人工智能内省等众多领域的边缘地带，而且随着模型在领域稳健性和工具集成方面的改进，我们可能会看到这些分散的意图汇聚成更清晰、数量更多的应用。 相比之下，金融、学术和法律领域的分布则要分散得多。 金融领域的内容量分布在外汇、社会责任投资以及审计/会计等多个方面：没有任何一个标签的占比超过20%。 法律领域也呈现类似的分散性，其使用量分布在政府/其他（43.0%）和法律/其他（17.8%）之间。这种分散性可能反映这些领域的复杂性，或仅是与编码和聊天等更成熟的类别相比，它们缺乏针对性的大模型工作流程。 或者说，现实世界中，大语言模型的使用并非均匀地具有探索性：其使用高度集中在一小部分可重复、高频率的任务上。角色扮演、编程和个人助理这三类任务均呈现清晰的结构和主导性标签。相比之下，科学、健康和法律领域的使用则更为分散，且可能未得到充分优化。 这些内在分布规律可为模型设计、特定领域的微调以及应用层面的界面设计提供指导，尤其在使大语言模型贴合用户目标方面。 四、模型和领域的结合：八仙过海，各显神通 总结：每个提供商都展现与其战略重点相符的独特特征。这些差异凸显为何没有单一模型或提供商能最佳地覆盖所有使用场景，同时也强调多模型生态系统的潜在优势。 Anthropic: 主要用于编程和技术任务（占比超过80%），角色扮演用途极少。 Google: 一个广泛使用的组合，涵盖法律、科学、技术以及一些常识性查询。 xAI: 使用主要集中在编程领域，而技术、角色扮演和学术领域在11月下旬占比更为突出。 OpenAI: 随时间推移，逐渐转向编程和技术任务，角色扮演和随意聊天显著减少。 DeepSeek: 其使用主要体现在角色扮演和日常互动上。 Qwen: 在编程任务上专注度较高，角色扮演和科学类别的专注度则随时间波动。 Anthropic: 主要用于编程和技术任务（占比超过80%），角色扮演用途极少。 Anthropic的Claude在编程 + 技术方面的应用占比极高，两者合计超过其使用量的80%。角色扮演和一般问答仅占很小一部分。这证实Claude的定位是一款针对复杂推理、编码和结构化任务进行优化的模型；开发者和企业似乎主要将Claude用作编码助手和问题解决工具。 Google: 一个广泛使用的组合，涵盖法律、科学、技术以及一些常识性查询。 谷歌的模型用途更为多样化。在翻译、科学、技术以及一些常识领域有显著的应用部分。例如，谷歌约5%的使用量涉及法律或政策内容，另有约10%与科学相关。这可能暗示Gemini广泛的训练重点。与其他公司相比，到2025年底，谷歌在编码方面的占比相对较低，实际上还在下降（降至约18%），且应用类别范围更广。这表明谷歌的模型更多地被用作通用信息引擎。 xAI: 使用主要集中在编程领域，而技术、角色扮演和学术领域在11月下旬占比更为突出。 xAI的使用情况与其他提供商截然不同。在大部分时间里，其使用量绝大多数集中在编程领域，往往超过所有Tokens的80%。直到11月下旬，这一分布才有所扩大，在技术、角色扮演和学术领域有了显著增长。 这种急剧变化与xAI模型通过特定消费者应用免费发布的时间相吻合，这很可能带来大量非开发者流量。其结果是，使用构成融合早期以开发者为主的核心群体和突然涌现的通用型用户参与，这表明xAI的采用路径既受技术用户的影响，也与促销活动带来的阶段性流量激增有关。 OpenAI: 随时间推移，逐渐转向编程和技术任务，角色扮演和随意聊天显著减少。 2025年，OpenAI的使用情况发生显著变化。今年早些时候，科学类任务占OpenAI所有Tokens的一半以上；到2025年末，这一比例已降至15%以下。 与此同时，编程和技术相关的使用量现在占总量的一半以上（各占29%），这反映其与开发者工作流、生产力工具和专业应用的整合更加深入。 OpenAI的使用构成目前介于Anthropic高度集中的情况和谷歌更分散的分布之间，这表明其应用基础广泛，且正越来越倾向于高价值、结构化的任务。 DeepSeek: 其使用主要体现在角色扮演和日常互动上。 深度求索和通义千问的使用模式与前文讨论的其他模型家族存在显著差异。DeepSeek的Tokens分布以角色扮演、休闲聊天和娱乐导向的互动为主，这类使用通常占其总使用量的三分之二以上。只有一小部分活动属于编程或科学等结构化任务。这种模式反映深度求索强烈的消费者导向及其作为高参与度对话模型的定位。 值得注意的是，到夏末时，深度求索在编程相关使用方面呈现适度但稳定的增长，这表明它在轻量级开发工作流中的采用率正逐步提升。 Qwen: 在编程任务上专注度较高，角色扮演和科学类别的专注度则随时间波动。 相比之下，通义千问呈现几乎相反的情况。在所显示的整个时间段内，编程内容始终占所有标记的40%-60%，这表明其明显侧重于技术和开发者任务。与Anthropic更稳定的、以工程为主的构成相比，通义千问在科学、技术和角色扮演等相邻类别中的波动性更大。 这种每周的变化意味着其用户群体具有多样性，且应用场景在快速迭代。9月和10月角色扮演使用量显著上升，随后11月有所下降，这暗示用户行为在不断演变，或下游应用的路径规划在进行调整。 五、用户留存：灰姑娘的”水晶鞋”现象5.1 灰姑娘的”水晶鞋”现象OpenRouter提出群留存率概念来揭示模型的用户留存现象。 Cohort Retention Rates. Retention is measured as activity retention, where users are counted if they return in subsequent months, even after periods of inactivity; as a result, curves may exhibit small non-monotonic bumps. 同期群留存率。留存率以活动留存来衡量，即只要用户在后续月份返回，即使中间有不活跃的时期也会被统计在内；因此，曲线可能会出现小的非单调波动。 更多图片放在引用后面，供参考。 这份留存率图表集捕捉领先大语言模型用户的市场动态。乍一看，数据的主要特征是高流失率和用户群体的快速衰减。然而，在这种波动性之下，隐藏一个更微妙且更重要的信号：一小部分早期用户群体随时间推移表现出持久的留存率。我们将这些群体称为基础用户群。 这些群体不仅仅是早期采用者；他们代表那些工作负载已实现深度且持久的工作负载-模型适配的用户。一旦这种适配确立，就会产生经济和认知上的惯性，即便有更新的模型出现，也会抵制替代。 我们将”灰姑娘水晶鞋效应”作为一个框架来描述这种现象。该假说认为，在快速发展的人工智能生态系统中，存在一种潜在的高价值工作负载的分布，这些工作负载在连续的模型迭代中一直未得到解决。每个新的前沿模型都相当于被”试穿”以应对这些未解决的问题。当一个新发布的模型恰好满足之前未被满足的技术和经济约束时，它就实现精准匹配——也就是比喻中的”玻璃鞋”。 对于那些工作负载最终”适配”的开发者或组织而言，这种契合会产生强烈的锁定效应。他们的系统、数据管道和用户体验会锚定在最先解决其问题的模型上。随着成本下降和可靠性提高，重新搭建平台的动力会大幅减弱。相反，那些未能找到这种适配的工作负载仍处于探索阶段，会从一个模型迁移到另一个模型，以寻找适合自己的解决方案。 从经验来看，这种模式在2025年6月的Gemini 2.5 Pro用户群和2025年5月的Claude 4 Sonnet用户群中可见，这两个用户群在第5个月仍保留约40%的用户，显著高于后续用户群。这些用户群似乎与特定的技术突破（例如推理保真度或工具使用稳定性）相对应，这些突破最终使之前不可能实现的工作负载成为可能。 率先解决问题会产生持久优势。 当一个模型率先解决关键工作负载时，经典的先发优势便具有重要意义。早期采用者会将该模型嵌入到各种管道、基础设施和用户行为中，从而产生很高的转换成本。这就形成一种稳定的平衡状态，即便出现更新的替代方案，该模型仍能保留其核心用户群体。 留存率作为能力拐点的指标。 同期群组层面的留存模式是模型差异化的实证信号。一个或多个早期群组中的持续留存表明存在有意义的能力拐点——即从不可行变为可行的工作负载类别。缺乏此类模式则表明能力相当，差异化深度有限。 **前沿窗口的时间限制。**竞争格局带来一个狭窄的时间窗口，模型可在其中获取基础用户。随着后续模型缩小能力差距，形成新基础用户群体的概率急剧下降。因此，模型与工作负载完美匹配的”灰姑娘”时刻虽转瞬即逝，却对长期采用动态起决定性作用。 每一代新模型都会带来一个短暂的机会，以解决先前未满足的工作负载。当这种契合出现时，受影响的用户会形成基础用户群：即使后续有新模型推出，其留存轨迹仍保持稳定的用户群体。 5.2 主导性发布异常 OpenAI GPT-4o Mini的图表极度直观地展现这一现象。一个单一的基础群体（2024年7月，橙线）在发布时就确立主导性的、稳定的工作负载-模型适配。所有后续群体——在这种适配确立且市场已经向前发展之后出现的群体——表现都如出一辙：它们不断流失并聚集在底部。 这表明，建立这种基础适配的窗口期是唯一的，且只出现在模型被视为”前沿”的那一刻。 5.3 适配失败的后果Gemini 2.0 Flash和Llama 4 Maverick的图表展示一个警示故事，说明当初始适配从未建立时会发生什么。 与其他模型不同，它们没有表现出色的基础用户群体。每个用户群体的表现都同样糟糕。这表明，这些模型从未被视为高价值、粘性工作负载的”前沿”。它们直接进入足够好的市场，因此未能锁定任何用户群。 同样，尽管DeepSeek总体上取得巨大成功，但其混乱的图表显示，它难以建立一个稳定的基础用户群体。 5.4 DeepSeek 的回旋镖效应DeepSeek 模型呈现一种更为复杂的模式。 它们的留存曲线显示一种极不寻常的异常现象：复苏式跃升。 与典型的单调下降留存率不同，多个深度求索用户群体在经历初期用户流失后，留存率出现明显上升（例如，R1的2025年4月用户群体在第3个月左右，以及Chat V3-0324的2025年7月用户群体在第2个月左右）。 这表明部分流失的用户正在回归该模型。这种”回旋镖效应”意味着，这些用户在尝试其他替代方案后，通过竞争性测试确认深度求索凭借其专业技术性能、成本效益或其他独特功能的卓越组合，能够为其特定工作负载提供最优且更适配的解决方案，因此选择回归深度求索。 水晶鞋现象将留存率重新定义为理解能力突破的视角，而非一种结果。 **基础用户群体是真正技术进步的印记：它们标志着人工智能模型从新奇事物转变为必需品的转折点。**对于开发者和投资者而言，尽早识别这些用户群体或许是预测模型在市场中能否保持持久优势的最有效信号。 Discussion这项实证研究从数据驱动的角度探讨了大型语言模型（LLMs）的实际使用情况，体现了几个能让人们人工智能部署的传统认知更加细致的主题： 【多模型生态系统】 没有任何单一模型能在所有使用场景中占据主导地位。相反，一个丰富的多模型生态系统正在形成，封闭模型和开放模型都占据了相当大的份额。 例如，尽管OpenAI和Anthropic的模型在许多编程和知识任务中处于领先地位，但DeepSeek和Qwen等开源模型合计处理了总token的很大一部分（有时超过30%）。这表明，大语言模型（LLM）使用的未来可能是与模型无关且多样化的。 对于开发者而言，这意味着要保持灵活性，整合多个模型并为每项任务选择最合适的模型，而不是将所有赌注都押在某一个模型的优势上。 对于模型提供商来说，竞争可能来自意想不到的地方（例如，除非持续改进和差异化，否则其他模型可能会侵蚀你的部分市场）。 【超越生产力的使用多样性】 一个令人惊讶的发现是，角色扮演和以娱乐为导向的使用量非常大。超过一半的开源模型使用是为了角色扮演和讲故事。 即使在闭源平台上，早期 ChatGPT 的使用中也有相当一部分是休闲和创造性的。这与大语言模型主要用于编写代码、电子邮件或摘要的假设相悖。实际上，许多用户使用这些模型是为了获得陪伴或进行探索。这具有重要意义。 它凸显了面向消费者的应用程序存在巨大机遇，这些应用程序将叙事设计、情感参与和交互性融合在一起。它为个性化开辟了新领域——智能体可以发展个性、记住偏好或维持长篇互动。 它还重新定义了模型评估指标：成功可能更少依赖于事实准确性，而更多地取决于一致性、连贯性以及维持引人入胜的对话的能力。 最后，它为人工智能与娱乐知识产权之间的交叉提供了途径，在互动叙事、游戏和创作者驱动的虚拟角色方面具有潜力。 【智能体与人类：智能体推理的兴起】 大型语言模型的使用正从单轮交互转向智能体推理，即模型通过多个步骤进行规划、推理和执行。它们不再生成一次性的响应，而是协调工具调用、访问外部数据，并迭代优化输出以实现目标。 早期证据表明，多步骤查询和链式工具使用呈上升趋势，我们将其视为智能体使用的代表。 随着这种模式的扩展，评估将从语言质量转向任务完成度和效率。 下一个竞争前沿在于模型能够多么有效地进行持续推理，这一转变可能最终会重新定义大规模智能体推理在实践中的意义。 【留存率与灰姑娘水晶鞋现象】 随着基础模型的跨越式（而非渐进式）发展，留存率已成为衡量防御能力的真正标准。每一次突破都会创造一个短暂的启动窗口，在这个窗口中，模型可以完美地“适配”高价值工作负载（即灰姑娘水晶鞋时刻），一旦用户找到这种适配，他们就会留下来。 在这种模式下，产品与市场的契合度等同于工作负载与模型的契合度：率先解决实际痛点会推动深度且稳定的采用，因为用户会围绕该能力构建工作流程和习惯。届时，无论是从技术上还是行为习惯上，转换模型的成本都会很高。 对于开发者和投资者而言，需要关注的信号并非增长，而是留存曲线，即那些在模型更新过程中依然留存的核心用户群体的形成。 在一个节奏日益加快的市场中，尽早抓住这些重要的未被满足的需求，将决定谁能在下次能力飞跃后屹立不倒。 Reference[1] (OpenRouter) State of AI AppendixOpenRouter 的分类与谷歌标签的对应关系 Programming / 编程： /Computers &amp; Electronics/Programming /Science/Computer Science/* Roleplay / 角色扮演： /Games/Roleplaying Games /Arts &amp; Entertainment/* 下的创意对话 Translation / 翻译： /Reference/Language Resources/* General Q&amp;A / Knowledge / 一般问答 / 知识： /Reference/General Reference/* /News/* 下的事实查询 Productivity / Writing / 生产力 / 写作： /Computers &amp; Electronics/Software/Business &amp; Productivity Software /Business &amp; Industrial/Business Services/Writing &amp; Editing Services Education / 教育： /Jobs &amp; Education/Education/* Literature / Creative Writing / 文学 / 创意写作： /Books &amp; Literature/* ``/Arts &amp; Entertainment/*`下的叙事内容 Adult / 成人： /Adult Gemini Retention Rates","link":"/Blog/2025/12/10/OpenRouter-%E7%9A%84-100-%E4%B8%87%E4%BA%BF-Tokens-%E5%AE%9E%E8%AF%81%E7%A0%94%E7%A9%B6/"},{"title":"State of AI 2025","text":"关于硅谷投资人 Nathan Benaich 和他创办的 Air Street Capital 所撰写的报告 State of AI 2025 中的一些观点的深度解读。其中包含了一些技术工作，产业实证结论，以及 GW 数据中心的相关盈利研究。 重要结论关于研究：推理的 Scaling Law 成为新焦点 推理计算的重要性被严重低估：传统上重视训练阶段的投入（Training Time），但研究表明，推理阶段（Inference Time）的扩展是释放模型潜力的另一个关键维度，可视为“横向扩展”。 “小模型 + 复杂推理”可能是更优策略：在固定计算预算下，使用较小的模型并为其配备更复杂的推理策略（如思维链、树搜索等）生成更多Tokens，其性能往往优于单纯使用更大的模型。这为成本效益权衡提供了新思路，即在某些场景下部署中型模型并投入更多推理计算可能更经济高效。 存在收益递减规律：所有推理优化策略都受制于强烈的收益递减规律，投入的边际效益会逐渐降低。 关于技术演进：DeepSeek 的战略转向效率与生态 从性能追赶到综合优势构建：DeepSeek的演进路线（V3 → V3.1 → V3.2）表明其战略重心已从单纯追求模型性能，转向构建 “效率-成本-生态”三位一体的综合优势。 核心技术是稀疏注意力（DSA）：V3.2通过自研的 Lightning Indexer 和 DSA 稀疏注意力机制，实现了长上下文推理速度的倍增和处理成本的大幅下降，使推理速度能随输入长度呈线性增长。 关于产业：市场加速，格局初定 英伟达的市值突破 4 万亿美元，在人工智能研究论文的相关领域占据了 90% 的份额，与此同时，定制芯片和新型云服务也在崛起。循环式的巨额交易为大规模扩张提供了资金支持。 AI-First 公司增长进入“火箭模式”；OpenAI 在应用层断档领先；多模态生成应用收入疯涨；大模型答案引擎呈现“风格化。 随着 GW 集群的规划，电力成为了新的瓶颈，而电网限制也开始影响路线图和利润空间。 关于政策算力基础设施：军备竞赛与盈利模型 算力集群建设进入“吉瓦（GW）时代”：主要AI实验室（xAI, Meta, OpenAI, Anthropic）正在建设或计划在2026年投入使用的算力集群规模均达到约1吉瓦级别，集群规模成为实力和招聘的象征。 推理计算是为训练买单的关键：商业模型的核心在于，如何将模型生命周期中更多的计算分配给能产生收入的推理工作，并追求最高的利润率。推理利润率和计算分配策略直接影响投资回报。 AI数据中心是资本密集型高毛利业务：以1GW数据中心为例： 资本支出（Capex）巨大：约500亿美元，其中计算硬件（GPU等）占比最高（60%）。 折旧与摊销（D&amp;A）是主要成本：占总年度成本的 75% - 80%。 具备强大的盈利能力：尽管投入巨大，但精细化的财务模型显示，此类业务能够产生可观的运营利润（Operating Profit）和税后净利润（Contribution Profit），运营利润率可达 40% 左右。甲骨文与 OpenAI 的合作案例预测了其持续的盈利潜力。 基础前沿针对推理的 Scaling Law Sam Altman: The intelligence of an AI model roughly equals the log of the resources used to train and run it. The world will not change all at once; it never does. Life will go on mostly the same in the short run, and people in 2025 will mostly spend their time in the same way they did in 2024. We will still fall in love, create families, get in fights online, hike in nature, etc. 世界不会一蹴而就地发生改变；它从来都不是这样。短期内，生活大多会照旧进行，2025年的人们大多会以2024年的方式度过他们的时光。我们仍会坠入爱河，组建家庭，在网上争吵，在大自然中徒步等等。 But the future will be coming at us in a way that is impossible to ignore, and the long-term changes to our society and economy will be huge. We will find new things to do, new ways to be useful to each other, and new ways to compete, but they may not look very much like the jobs of today. 但未来将以一种无法忽视的方式向我们袭来，我们的社会和经济将发生巨大的长期变革。我们会找到新的事情去做，找到新的方式来彼此帮助，以及新的竞争方式，但它们可能与今天的工作大不相同。 CMU 的论文 《Inference Scaling Laws: An Empirical Analysis of Compute-optimal Inference for Problem-solving with Language Models》 给出了一个结论：**通过推理策略来扩展推理计算，可能比扩展模型参数在计算上更高效。此外，较小的模型与先进的推理算法相结合，在成本和性能方面呈现出帕累托最优的权衡。**例如，在 MATH 基准测试中，Llemma-7B 模型与树搜索算法配对后，在所有测试的推理策略上均持续优于 Llemma-34B 模型。 We find that using a smaller model and generating more tokens in an inference strategy often outperforms using a larger model at a fixed compute budget. This has implications for models deployed in the real world, where inference compute is constrained in various ways. Specifically, it is potentially beneficial to deploy smaller models with more sophisticated inference strategies for better cost-performance trade-off. 我们发现，在固定的计算预算下，使用较小的模型并在推理策略中生成更多的 Tokens，其性能往往优于使用较大的模型。这对于在现实世界中部署的模型具有重要意义，因为推理计算在多种情况下都受到限制。具体而言，部署较小的模型并采用更复杂的推理策略，可能有助于实现更优的成本效益权衡。 论文评估，随着推理计算量的增加，每种模型大小的错误率稳步下降，并在最后趋于收敛。以及最佳模型大小（对于 2⁴¹、2⁴⁴和 2⁴⁷次浮点运算，以星号显示）会根据推理时间的计算预算而变化。这有效的说明了 Inference Time 在实践中的重要程度，在目前是被远远低估了（相对于 Training Time），同时通过合理的组合（比如较小的模型＋复杂的推理过程）的确能过获得更大的收益。 给定一个固定的总计算预算（训练+推理），如何在模型规模和推理计算之间进行分配，是未来系统优化的重要课题。有研究表明，对于某些任务，训练一个稍小的模型但为其配备大量的推理时计算（如采样），可能比直接训练一个巨大的模型但只做单次采样更高效。 所以在 Inference Time 的 Scaling Law 问题上可以总结如下： “大力出奇迹”在推理时也适用：即使不改变模型，通过投入更多推理计算（采样、思考），也能显著提升效果，尤其在不确定性高、需要创造力的任务上。 收益递减：所有推理时优化策略都受制于强烈的收益递减规律。 模型规模是基础：推理时计算的有效性高度依赖于模型本身的能力。一个能力不足的模型，即使给它再多的“思考时间”，也无法产生质的飞跃。 新的成本权衡：这引入了一种新的工程和成本权衡：是部署一个超大模型（高固定成本）进行简单推理，还是部署一个中型模型（低固定成本）但为其配备复杂的推理策略（高可变成本）？ 总而言之，推理时间的 Scaling Law 揭示了模型能力释放的另一个维度：横向扩展。它不再仅仅追求模型的“大脑”更大，而是追求在解决问题时给予更长的“思考时间”，从而激发现有模型参数的潜力。 DeepSeek 的发展与演进DeepSeek V3.2 技术细节DeepSeek 发布了 V3.2，其通过 Lightning Indexer 来大幅提升效率，与之前的 V3.1 模型相比，长上下文推理速度提升了 2-3 倍，处理成本降低了 6-7 倍(Source: DeepLearning.AI)。本质上，V3.2 通过稀疏注意力机制使推理速度能随输入长度呈线性增长。 在预训练过程中，DeepSeek 通过 Lightning Indexer 的加权相似性函数从 21 亿个 Tokens 中训练以预测 DeepSeek-V3.1-Terminus 的稠密注意力机制会关注哪些 Tokens。随后在约 1000 亿个 Tokens 上对所有参数进行了微调，使其能与 Indexer 协同。 训练环节分为两个部分：Dense Warm-up Stage 和 Sparse Training Stage。 Dense Warm-up Stage 主要作用是为 Lightning Indexer 提供初始参数，实现 Indexer 和主注意力分布对齐，为后面的稀疏矩阵的训练做基础。 Sparse Training Stage 引入细粒度 Token 选择机制，也就是 Fine-grained Token Selection Mechanism，这一步的训练目的是让模型和 Indexer 共同适配 DSA 的稀疏注意力模式，并保证在这个模式下，模型的语言建模能力不显著变化或者下降。 在后训练过程中，目的是在“稀疏架构下补全多任务能力，验证 DSA 对性能的影响”，后训练也分为两个大部分：专家蒸馏 Specialist Distillation 和混合 RL 训练 Mixed RL Training。 专家蒸馏：研究团队通过将五个专业模型（经过预训练的 DeepSeek-V3.2 基础模型的不同版本，分别针对推理、数学、编程、智能体编程和智能体搜索进行了微调）蒸馏到 DeepSeek-V3.2-Exp 中，进一步对该模型进行了微调。 混合 RL 训练：研究团队依然应用了 GRPO (Group Relative Policy Optimization)，将推理、智能体和人类对齐训练合并到一个阶段。这种方法避免了灾难性遗忘问题，即新学到的知识会取代旧知识，而这一问题通常会困扰多阶段强化学习。 在推理时，Indexer 会对每个历史 Tokens 与正在生成的 Tokens 的相关性进行评分。使用 FP8 精度（8 位浮点数，精度相对较低，但处理时所需的计算量更少）来快速计算这些分数。 基于这些分数，模型不再计算当前输入上下文中所有 Tokens 的注意力，而是选择并计算得分最高的 2048 个 Tokens 的注意力，显著降低了计算成本。 DeepSeek 的发展演进思路总的来说，DeepSeek的演进路线图清晰地描绘了其战略重心从单纯追求模型性能，转向构建“效率-成本-生态”三位一体的综合优势。 V3/R1 回答了“如何快速达到一线水平”的问题：重点是通过“后训练”优化，快速弥补与国际顶尖模型的能力差距 以基础模型 DeepSeek-V3-Base 为基座，通过后训练技术来激发模型潜力，使其在推理、编程等特定任务上表现更出色。引入了“深度思考”模式，进行更复杂的推理，在长对话中的上下文记忆也更加稳定。在数学、代码和通用推理等基准测试中，性能迅速逼近当时的国际顶尖模型，其中代码能力被认为可媲美Claude 4。 V3.1 回答了“如何为未来硬件与复杂应用布局”的问题，并开始在混合推理架构和软硬件协同优化上进行关键布局 引入了混合推理架构，使得一个模型能同时支持需要快速响应的“即时模式”和需要深度思考的“思考模式”，更具适应性。采用FP8精度训练，能显著提升计算速度并降低存储需求。通过对模型进行后训练优化，其使用外部工具和执行复杂任务的能力（即Agent能力）获得了显著提升。 V3.2 则回答了“如何在保持能力的同时，让AI变得真正便宜、好用且自主可控”的问题 引入了自研的DSA（DeepSeek Sparse Attention）稀疏注意力机制。该机制让模型在处理长文本时，能够智能地聚焦于最关键的信息，极大地提升了长文本的训练和推理效率。编程语言选用TileLang这个新兴AI编程语言，可以实现对不同硬件平台的支撑，极大地改善了国产卡目前所面对的CUDA带来的生态壁垒问题，为国产大模型软硬件生态建立起到了极大的推动作用。 Leaderboard [Update to Sep] Artificial Analysis GDPval OpenAI 设计的榜单，涵盖了从对美国 GDP 贡献最大的 9 个行业中选出的 44 个职业，GDPval 任务并非简单的文本提示。它们附带参考文件和上下文，预期交付成果涵盖文档、幻灯片、图表、电子表格和多媒体。这种现实性使得 GDPval 能够更真实地测试模型如何支持专业人士。 LLM-Stats LiveBench ScaleAI WebDev SuperCLUE 中文语言理解测评基准CLUE（The Chinese Language Understanding Evaluation）是致力于科学、客观、中立的语言模型评测基准，发起于2019年。陆续推出CLUE、FewCLUE、KgCLUE、DataCLUE等广为引用的测评基准。 产业实证实证结论AI- First 公司与 SaaS 公司的早期增长：营收增长进入 “火箭” 模式AI 企业达成关键营收里程碑的速度远超预期。Stripe 平台百强 AI 企业实现 100 万美元年化营收的中位用时仅为 11.5 个月，比营收增长最快的 SaaS 企业还快整整 4 个月。在达到 500 万美元年化营收时，AI 企业的中位用时为 24 个月，而 SaaS 企业则需 37 个月，AI 公司在此项上快了近一年。(Source: Stripe) TODO#AI-First 公司和 SaaS 公司的详细调研 OpenAI 在应用的使用率上依旧断档领先Ramp 的人工智能指数（来自 45,000 多家美国企业的信用卡 / 账单支付数据）显示，科技行业在付费人工智能采用率方面处于领先地位（73%），金融业紧随其后（58%）。总体而言，2025 年第一季度的采用率大幅上升。此外，Ramp 的客户对 OpenAI 模型表现出强烈的偏好（35.6%），其次是 Anthropic（12.2%）。与此同时，谷歌、深度求索（DeepSeek）和 xAI 的使用率非常低。 音频、虚拟形象和图像生成公司的收入出现了疯狂增长市场领导者 ElevenLabs、Synthesia 和黑森林实验室（Black Forest Labs）的年收入都已轻松达到数亿美元。此外，由于收入来自企业客户以及超过 10 万名且不断增长的长尾客户，其收入质量正日益提高。 ElevenLabs 在 9 个月内将年收入增长了一倍，达到 2 亿美元，并宣布其估值为 66 亿美元，与此同时还提出了 1 亿美元的员工股权收购要约。到 2026 年，客户已创建超过 200 万个智能体，这些智能体已处理超过 3300 万次对话。 Source (ElevenLabs): https://elevenlabs.io/blog/introducing-elevenlabs-agents Synthesia 在 2025 年 4 月的年度经常性收入突破 1 亿美元，财富 100 强企业中有 70% 是其客户。自 2021 年推出以来（右侧图表），客户生成的虚拟形象视频时长已超过 3000 万分钟。 据悉，黑森林实验室（Black Forest Labs）的年度经常性收入约为 1 亿美元（同比增长 3.5 倍），毛利率为 78%，其中包括与 Meta 达成的一项为期两年、价值 1.4 亿美元的大额交易。此外，Midjourney 也与 Meta 达成了一项授权协议，但其条款尚未公开。 Source (Black Forest Labs): https://x.com/ArfurRock/status/1965426792191439012 大模型作为答案引擎的风格化这部分的研究结论主要来源于 Profound Data，大模型回答问题的风格，对搜索引擎的应用都会极大的影响用户体验，而这部分总体可以归结为问答引擎的风格化，风格会产生用户粘性。 ChatGPT 用户平均每个会话有 5.6 轮对话，而 Gemini 和 Perplexity 约为 4 轮，DeepSeek 约为 3.9 轮。这要么意味着更多的轮次代表对话更具吸引力，要么意味着更少的轮次代表回答更高效 对话风格各不相同：DeepSeek 的用户会写出最长的提示词，并得到最冗长的回答，而 Perplexity 则会给出更简短、引用密集的回应。 ChatGPT 通常会从人类通常不会点击的排名较低的页面中提取信息，这扩大了非顶级结果网站的曝光度。 各模型引用的顶级域名包括：Reddit（3.5%）、维基百科（1.7%）、YouTube（1.5%）和《福布斯》（1.0%）。 不同模型呈现出不同的信息来源风格：Gemini 和 Perplexity 倾向于主流的简洁信息来源，而 DeepSeek 则往往会从长篇内容的域名中获取信息 这意味着，针对答案引擎优化（AEO）进行优化与针对搜索引擎优化（SEO）同样重要，因为可见性不仅取决于排名，还取决于模型的引用模式。 推理和训练的盈亏平衡分析推理为训练买单：实验室努力将模型生命周期计算中更多的部分分配给能带来收入的推理工作，且要尽可能追求最高的利润率。我们下方的表格 * 展示了在不同的推理利润率和计算分配情况下，计算成本的预期回报率。 *Simplified sensitivity analysis: neglects people costs and assumes all inference generates revenue. Can also be interpreted in terms of token count between inference &amp; training (2DN vs. 6DN, MFU: ~15% vs. ~45%). 算力集群的建设军备竞赛计划中约 1 吉瓦规模的集群将于 2026 年投入使用：在美国的实验室中，集群规模日益成为一个标志性特征，在招聘时尤其有用。如果估值依据的是集群规模而非采用率或财务指标，那么可能会形成一个更大的泡沫。 Code Name IT Power at YE 2026 Number of Chips Chip Type Total TFLOPS Provider xAI - Colossus 1,200 MW GB200/300 550,000 3,488,148,649 xAI Meta - Promethus 1,020 MW GB200/300 500,000 3,171,044,226 Meta OpenAI - Stargate 880 MW GB200/300 400,000 2,469,594,595 Oracle Anthropic - Project Rainer 780MW Tranium 2 800,000 1,040,000,000 AWS * 谷歌 DeepMind 也在爱荷华州、内布拉斯加州和俄亥俄州建立了许多值得关注的集群。但是谷歌的项目可获得的信息不足，并且是分布式的，所以并未列在上述表格中。 1GW 的 AI 数据中心盈利水平分析资本支出，折旧与摊销，成本结构NewStreet Research 给出了一个关于 1GW 数据中心的财务模型：500亿 CAPEX，110亿年总成本。 资本支出是建设数据中心的总投入，1GW AI 数据中心总 Capex 为 500 亿美元，具体构成如下： 计算与存储（Compute and storage）：300 亿美元，占总 Capex 的 60% GPU：210 亿美元（占总 Capex 的 42%），是核心硬件成本。每 GW 需要 60 万块芯片，单块 GPU 平均售价（ASP）3.5 万美元，单 GPU 平均功耗 1.7kW。 CPU：10 亿美元（占 2%）。 其他服务器和存储：80 亿美元（占 16%）。 网络（Networking）：60 亿美元，占总 Capex 的 12%。 建筑、电力与冷却（Building, power &amp; cooling）：140 亿美元，占总 Capex 的 28%，是支撑算力运行的基础设施成本。 D&amp;A 是将资本支出在资产使用寿命内逐年分摊的费用，直接影响年度成本结构： 不同资产的使用寿命决定了 D&amp;A 的年限：GPU、CPU、其他服务器存储、网络设备：使用寿命 5 年。建筑、电力与冷却设施：使用寿命 10 年。 年度 D&amp;A 总额为 86 亿美元（$8.6bn p.a.），具体拆分： GPU：42 亿美元 / 年（210 亿 ÷ 5 年），占总 D&amp;A 的 50%。 CPU：2 亿美元 / 年（10 亿 ÷ 5 年），占 2%。 其他服务器和存储：16 亿美元 / 年（80 亿 ÷ 5 年），占 18%。 网络：12 亿美元 / 年（60 亿 ÷ 5 年），占 14%。 建筑、电力与冷却：14 亿美元 / 年（140 亿 ÷10 年），占 16%。 年度全部成本为110 亿美元（$11bn p.a.），由 “D&amp;A” 和 “现金成本（Cash Costs）” 组成： D&amp;A 占比 75 - 80%：86 亿美元 / 年，是最主要的年度成本，反映了资产折旧对利润的持续压力。 现金成本占比 20 - 25%：24 亿美元 / 年，具体拆分： 电力：12 亿美元 / 年（占总成本的 11%）。年均能耗 8TWh，电价 $0.15/kWh（计算：8TWh×$0.15/kWh = $1.2bn）。 维护、软件及其他：12 亿美元 / 年（占总成本的 11%）。 数据中心盈利水平分析（以 Oracle &amp; OpenAI 的合作为例）虽然目前公开信息还无法精确计算出甲骨文在此笔交易中的最终盈利，但我们可以根据现有数据，对其盈利水平和财务模型进行一次深入的推演分析。下面这个表格梳理了与本次交易相关的一些关键已知数据和合理的估算参数，可以作为我们分析的基础。 项目 数据/估算 合同规模 4.5 GW (总计) 年度费用 300亿美元 硬件投资估算 ~400亿美元 (以阿比林1.2GW园区为例，部署约40万GPU) 甲骨文官方毛利率指引 30% - 40% (AI基础设施，扣除土地、数据中心、电力和计算设备成本后) 收入端：主要来自OpenAI支付的300亿美元/年的巨额租金。 成本端：主要包含以下几大块： 硬件折旧：这是最大头的成本。根据的分析，一个类似的GPU数据中心项目中，服务器折旧是成本中绝对的大头。如果4.5GW的总投资按数百亿美元计算，其每年的折旧费用将非常惊人。 电力成本：1GW的数据中心年耗电量约为8 TWh，电费约12亿美元。4.5GW的规模，年电费成本预计超过50亿美元。 托管与运维成本：包括场地租金、网络、冷却和维护等。在的模型中，这项与电费成本相加，年支出约20亿美元（针对较小规模）。 融资成本：如此大规模的投资，甲骨文很可能通过借款进行，由此产生的利息费用也是一笔不小的开支。 SemiAnalysis 针对这份交易也给出了一个盈利分析，以 40W 块 GB200 的数据中心来进行预测： 基础指标 Chips in Service（在用芯片数量）：每年稳定在 400,000 块（GB200 芯片）。 Compute Rental（算力租赁单价）：2.60 USD/hr/GPU，是算力服务的单位定价，为收入核算的基础。 收入端：算力租赁业务的规模与稳定性 Revenue（营业收入）：年营收在86.93 亿–91.60 亿美元区间，整体保持高位且小幅波动。 说明 “算力租赁” 是核心收入来源，市场需求稳定，具备较强的营收持续性。 成本端：构成与变化逻辑 （1）直接成本（影响毛利） Hosting Cost（托管成本）：年支出10.01 亿 – 12.27 亿美元，逐年上升。 反映算力集群的托管运维复杂度增加（如场地、基础服务外包成本上升）。 Electricity Cost（电力成本）：年支出7.55 亿 – 8.13 亿美元，逐年上升。 是算力运行的核心可变成本，与芯片规模、电价波动或能效优化节奏有关（按芯片数量比例换算，与前 1GW 模型的电力成本逻辑完全匹配）。 （2）固定成本（影响运营利润） Server Depreciation（服务器折旧）：年支出32.86 亿 – 32.95 亿美元，几乎无波动。 源于服务器类资产的 “年限平均法” 折旧（资产使用寿命固定），是核心固定成本。 Amortization of Installation/Fit Out cost（安装 / 装修成本摊销）：前 3 年每年 2 亿美元，第 4-5 年为 0。 此类资产（如机房装修、专项安装工程）摊销年限为 3 年，到期后不再产生摊销成本。 Repair and Maintenance（维修维护成本）：每年 2 亿美元，固定支出。 保障服务器、设施的正常运行，属于常规运维成本。 Sales and Marketing Cost（销售与营销成本）：前 4 年每年 4.6 亿美元，第 5 年 4.3 亿美元。 前期为拓展市场投入营销资源，后期业务成熟后小幅缩减，属于合理的费用优化。 Annual Maintenance Cost（年度维护成本）：第 2-5 年每年 1 亿美元（第 1 年无）。 可能是新增长期维护合同或设备老化后专项维护的支出，体现运维策略的阶段性调整。 四、盈利端：高毛利与持续盈利性 Gross Profit（毛利）：67.53 亿 – 74.04 亿美元，毛利规模大且支撑力强。 毛利 = 收入 - 托管成本 - 电力成本，反映 “算力租赁” 业务的核心盈利能力。 Operating Profit（运营利润）：34.21 亿 – 40.59 亿美元，运营效率突出。 运营利润 = 毛利 - 各类运营成本（折旧、摊销、维修、营销、维护），体现扣除所有运营成本后的盈利水平。 Operating Margin（运营利润率）：39% – 44%，属于高毛利行业的典型表现。 说明业务模式的盈利能力极强，成本管控与收入规模的协同效应显著。 Interest Expense（利息支出）：前 4 年每年8.87 亿 – 8.90 亿美元，第 5 年降至 4.45 亿美元。 前期因资本投入产生较高债务利息，第 5 年或因债务偿还、利率调整而大幅下降。 Profit Before Tax（税前利润）：29.76 亿 – 31.69 亿美元，是运营利润扣除利息后的盈利。 Income Tax Expense（所得税费用）：5.95 亿 – 6.34 亿美元，税率约 20%（所得税 / 税前利润），符合企业所得税常规水平。 Contribution Profit（税后利润，实际为净利润）：23.81 亿 – 25.35 亿美元，每年稳定创造 20 多亿美元税后利润。 反映业务在覆盖所有成本（运营 + 财务 + 税务）后，具备持续的盈利产出能力。 ReferenceWu, Yangzhen, et al. “Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models.” arXiv preprint arXiv:2408.00724 (2024). Pilz, Konstantin F., et al. “Trends in AI supercomputers.” arXiv preprint arXiv:2504.16026 (2025). Liu, Aixin, et al. “Deepseek-v3 technical report.” arXiv preprint arXiv:2412.19437 (2024).","link":"/Blog/2025/09/24/State-of-AI-2025/"},{"title":"Overview of AWS: Machine Learning Services (2022 Edition)","text":"**Data Science and Machine Learning are surely some fast-moving industries and somewhat need you to study at all times to stay ahead and on top in the industry. But the first step of getting into this area seems dreadfully slow due to widely involved technologies and overwhelming terminologies that scare you out of shit. ** AWS lowers the barrier to entry for companies and organizations looking for solutions of leveraging ML capabilities by offerings more than 20 services including low-level service like SageMaker, which helps build and manage infrastructure for developing environments, as well as high-level systems like Rekognition that come with pre-built Machine Learning models for image recognition. This blog will go through nearly all the Machine Learning services offered by AWS. ToolsSageMakerSageMaker is the most important microservices set in AWS. It’s a set of tools for deploying machine learning applications. it streamlines all the Machine Learning tasks that come up from preparing data and building a model to training, and deploying it. Also, The benefits of SageMaker have to do with all the details of how to stage training tasks and deploy inference tasks across a variety of infrastructures. SageMaker is so powerful that can not be just compressed into one section to introduce. So, I’m decided to use another article to illustrate it in full detail. Please stay tuned. Development ToolsThe critical blockers for traditional developers to become Cloud practitioners are all kinds of new cloud development patterns, including new Cloud IDE, new backend design patterns, and new operation standards. From that end, AWS provides several development tools to ensure the working environment, and smooth the transition experience. Machine Learning plays a vital role in this task. Amazon CodeGuru is a developer tool that provides intelligent recommendations to improve code quality and identify an application’s most expensive lines of code. Integrate CodeGuru into your existing software development workflow to automate code reviews during application development and continuously monitor application’s performance in production and provide recommendations and visual clues on how to improve code quality, application performance, and reduce overall cost. CodeGuru Reviewer uses machine learning and automated reasoning to identify critical issues, security vulnerabilities, and hard-to-find bugs during application development and provides recommendations to improve code quality. CodeGuru Profiler helps developers find an application’s most expensive lines of code by helping them understand the runtime behavior of their applications, identify and remove code inefficiencies, improve performance, and significantly decrease compute costs. Amazon DevOps Guru is a service powered by machine learning (ML) that is designed to make it easy to improve an application’s operational performance and availability. DevOps Guru helps detect behaviors that deviate from normal operating patterns so you can identify operational issues long before they impact your customers. When DevOps Guru identifies a critical issue, it automatically sends an alert and provides a summary of related anomalies, the likely root cause, and context for when and where the issue occurred. Tools for Text MiningText is important for human society and it is also more difficult and more tricky to process than other standard machine learning tasks like numerical classification. So, AWS provides a wide range of tools for NLP (natural language processing), GLU (natural language understanding), as well as NLG (natural language generation). With all kinds of text mining appliances, you can do sentiment analysis, machine translation, also speech recording. They are also accommodating if you want to build conversational user interfaces or summarize texts. Amazon Comprehend for natural language processing and text analytics helps you understand text sentiment and relate texts to each other. Amazon Lex is a service for building conversational interfaces using voice and text. With Lex, you can use the same deep learning engine that powers Alexa in your own applications. Amazon Textract extracts text and data from scanned documents. It’s not just OCR but backed by Machine Learning models that have analyzed many types of documents, and can identify the contents of fields in forms and information stored in tables. Amazon Transcribe could be used to turn any speech recording into a text. If you need to go the other way around, Amazon Polly will synthesize lifelike speech from any text. Amazon Translate caters to your multilingual needs by translating every text into the language of your choice. Amazon Polly is a service that turns text into lifelike speech. Polly lets you create applications that talk, enabling you to build entirely new categories of speech-enabled products. Polly is an Amazon artificial intelligence (AI) service that uses advanced deep learning technologies to synthesize speech that sounds like a human voice. Tools for Image and VideoThe ability to verify, organize, analysis millions and tons of images will unlock a whole new set of possibilities. Amazon Rekognition offers pre-trained and customizable computer vision (CV) capabilities to extract information and insights from your images and videos. From face detection to text extraction. AWS Panorama is a machine learning (ML) appliance and software development kit (SDK) that brings CV to on-premises internet protocol (IP) cameras. Manufacturing CapabilitiesAmazon Lookout for Vision is a machine learning (ML) service that spots defects and anomalies in visual representations using computer vision (CV). With Amazon Lookout for Vision, manufacturing companies can increase quality and reduce operational costs by quickly identifying differences in images of objects at scale. Amazon Lookout for Equipment analyzes the data from the sensors on your equipment (e.g. pressure in a generator, flow rate of a compressor, revolutions per minute of fans), to automatically train a machine learning model based on just your data, for your equipment – with no ML expertise required. Amazon Lookout for Metrics uses machine learning (ML) to automatically detect and diagnose anomalies in business and operational data, such as a sudden dip in sales revenue or customer acquisition rates. Amazon Monitron is an end-to-end system that uses machine learning (ML) to detect abnormal behavior in industrial machinery, enabling you to implement predictive maintenance and reduce unplanned downtime. Tools for Low Level Scientific EnvironmentTensorFlow is one of many deep learning frameworks available to researchers and developers to enhance their applications with machine learning. AWS provides broad support for TensorFlow, enabling customers to develop and serve their own models across computer vision, natural language processing, speech translation, and more. Amazon Elastic Inference allows you to attach low-cost GPU-powered acceleration to Amazon EC2 and Amazon SageMaker instances to reduce the cost of running deep learning inference by up to 75%. Amazon Elastic Inference supports TensorFlow, Apache MXNet, PyTorch, and ONNX models. AWS Inferentia is a machine learning inference chip designed to deliver high performance at low cost. AWS Inferentia will support the TensorFlow, Apache MXNet, and PyTorch deep learning frameworks, as well as models that use the ONNX format. Other ToolsAmazon Augmented AI (Amazon A2I) is a machine learning service that makes it easy to build the workflows required for human review. Amazon Fraud Detector is a fully managed service that uses machine learning (ML) and more than 20 years of fraud detection expertise from Amazon, to identify potentially fraudulent activity so customers can catch more online fraud faster. Amazon HealthLake is a HIPAA-eligible service that healthcare providers, health insurance companies, and pharmaceutical companies can use to store, transform, query, and analyze large-scale health data. Amazon Kendra is an intelligent search service powered by machine learning. Kendra reimagines enterprise search for your websites and applications so your employees and customers can easily find the content they are looking for, even when it’s scattered across multiple locations and content repositories within your organization. Amazon Personalize is a machine learning service that makes it easy for developers to create individualized recommendations for customers using their applications.Amazon Personalize is like having your own Amazon.com machine learning personalization team at your disposal, 24 hours a day. AWS DeepRacer is a 1/18th scale race car which gives you an interesting and fun way to get started with reinforcement learning (RL). RL is an advanced machine learning (ML) technique which takes a very different approach to training models than other machine learning methods. References Summary AWS Machine Learning Tools (2022 edition) Overview of AWS : Machine learning Services| AWS White Paper Summary AWS re:invent 2021 AI &amp; Machine Learning Launches: 7 Things You Should Know Amazon SageMaker AWS Announces Six New Amazon SageMaker Capabilities Building a customized recommender system in Amazon SageMaker Train ALBERT for natural language processing with TensorFlow on Amazon SageMaker Serverless Machine Learning with AWS Lambda 5 Must Have AWS Serverless Tools for your Starter Kit Ultimate Guide to Monitoring Serverless Applications","link":"/Blog/2022/02/16/Overview-of-AWS-Machine-Learning-Services/"},{"title":"LAMBADA Method: How to use Data Augmentation in NLU?","text":"In this tutorial, I will walk you through the implementation to reproduce LAMBADA. From my previous article, which illustrate the basic idea of LAMBADA method that leverage Natural Language Generation(NLG) to boost training set for the Natural Language Understanding(NLU) task including text classification. Before you dive into the code fragment, you may have a look at my previous article about the basic idea of the LAMBADA, including the fundamental thinking, and the workflow. Step 1: Preparation We use distilBERT as a classification model and GPT-2 as text generation model. For both, we load pretrained weights and fine tune them. In case of GPT-2 we apply the Huggingface Transfomers library to bootstrap a pretrained model and subsequently to fine-tune it. To load and fine-tune DistilBERT we use Ktrain, a library that provides a high-level interface for language models, eliminating the need to worry about tokenization and other pre-processing tasks. 123!pip install ktrain!pip install transformers!pip install tensorflow Step 2: Load DataThen, we load the data from the csv file, which can be obtained from my repository. We split it into train set, valid set, and test set. 12345678910111213141516171819labels = data_train['Label'].unique()X_train, X_valid, X_test, y_train, y_valid, y_test = [], [], [], [], [], []for label in labels: intent_X_train, intent_X_valid, intent_y_train, intent_y_valid = train_test_split( data_train[data_train['Label'] == label]['Text'], data_train[data_train['Label'] == label]['Label'], train_size=0.8, random_state=43) intent_X_valid, intent_X_test, intent_y_valid, intent_y_test = train_test_split( intent_X_valid, intent_y_valid, train_size=0.5, random_state=43) X_train.extend(intent_X_train) X_valid.extend(intent_X_valid) X_test.extend(intent_X_test) y_train.extend(intent_y_train) y_valid.extend(intent_y_valid) y_test.extend(intent_y_test) You can see the labels are: array([‘__label__15’, ‘__label__7’, ‘__label__0’, ‘__label__13’, ‘__label__9’, ‘__label__8’, ‘__label__2’, ‘__label__4’, ‘__label__1’, ‘__label__10’, ‘__label__5’, ‘__label__3’, ‘__label__14’, ‘__label__11’, ‘__label__12’, ‘__label__6’], dtype=object) Step 3: Training the Initial Intent Classifier (BERT)Initialize model and learner12import ktrainfrom ktrain import text We download the pretrained DistilBERT model, transform the training and validation data from pure text into the valid format for our model and initialize a learner object, which is used in KTrain to train the model. 12345678910distil_bert = text.Transformer('distilbert-base-cased', maxlen=50, class_names=labels) processed_train = distil_bert.preprocess_train(X_train, y_train)processed_test = distil_bert.preprocess_test(X_valid, y_valid)model = distil_bert.get_classifier()learner = ktrain.get_learner( model, train_data=processed_train, val_data=processed_test, batch_size=10) Train classifier Train classifier for given learning rate and number of epochs. The number of epochs chosen depends on the size of your training data set. Make sure to monitor the accuracies and losses! Now it’s time to train the model. We feed the training data to the network multiple times, specified by the number of epochs. In the beginning both monitored metrics, namely the loss function (decrease) and the accuracy (increase), should indicate improvement of the model with each epoch passed. However, after training the model for a while the validation loss will increase and the validation accuracy drop. This is a result of overfitting the training data and it is time to stop feeding the same data to the network. The optimal number of epochs depends on your data set, model and training parameters. If you do not know the right number of epochs beforehand you can use a high number of epochs and activate checkpoints by setting the checkpoint_folder parameter to select the best performing model afterwards. 123N_TRAINING_EPOCHS = 1learner.fit_onecycle(5e-5, N_TRAINING_EPOCHS) Evaluate trained predictorTo check the performance of our trained classifier, we use our test data in the eval.csv file. 123456789predictor = ktrain.get_predictor(learner.model, preproc=distil_bert)predictions = predictor.predict(X_test)np_test_intents = np.array(y_test)np_predictions = np.array(predictions)result = (np_test_intents == np_predictions)print(&quot;Accuracy: {:.2f}%&quot;.format(result.sum()/len(result)*100)) Note that thanks to the KTrain interface we can simply feed the list of utterances to the predictor without the need to pre-process the raw strings beforehand. Prepare model for download1234567import datetimepredictor.save('models/initial/distilbert_{}epochs_{}'.format( N_TRAINING_EPOCHS, datetime.datetime.now().strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;)))!zip -r -X distilbert_initial.zip models/initial Step 4: Fine-tune GPT-2 to generate utterancesFine-tune GPT-2To fine-tune GPT-2, we use a Python script made available by Huggingface on their Github repository: https://github.com/huggingface/transformers Put transformed dataset in directory where this jupyter notebook located at in order to run python script smoothly. 1234567utterance_file = data_train[['Label', 'Text']]path = 'content'if not os.path.exists(path): print(f'Create directory: {path}') os.mkdir(path)save_to_csv(utterance_file, 'train.csv', path)print('Save training file successfully') Among others, we specify the following parameters: the pretrained model that we want to use (gpt2-medium). Larger models, typically generate better text outputs. Please note, these models require a large amount of memory during training, so make sure you pick a model that fits into your (GPU-)memory. the number of epochs. This parameter specifies how many times the training data is fed through the network. On the one hand, if the number of epochs is too small, the model will not learn to generate useful utterances. On the other hand, if the number is chosen too big, the model will likely overfit and the variability in the generated text data will be limited – the model will basically just remember the training data. the batch size. This determines how many utterances are used for training in parallel. The larger the batch size the faster the training, larger batch sizes require more memory, though. the block size. The block size defines an upper bound on the number of tokens considered from each training data instance that are used. Make sure that this number is sufficient so that utterances are not cropped. 123456789101112!python finetune_gpt.py \\ --output_dir=.//content//transformers//output \\ --model_type=gpt2-medium \\ --model_name_or_path=gpt2-medium \\ --num_train_epochs=3.0 \\ --do_train \\ --train_data_file=.//content//train.csv \\ --per_gpu_train_batch_size=4 \\ --block_size=50 \\ --gradient_accumulation_steps=1 \\ --line_by_line \\ --overwrite_output_dir Load and Manually Test ModelYou can play around with the model, generating utterances for different intents. See how the parameters top_k and top_p influence the result. 123456from transformers import GPT2Tokenizer, TFGPT2LMHeadModeltokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2-medium&quot;)model = TFGPT2LMHeadModel.from_pretrained( './/content//transformers//output//', pad_token_id=tokenizer.eos_token_id, from_pt=True) Top k sampling means sorting by probability and zero-ing out the probabilities for anything below the k’th token. It appears to improve quality by removing the tail and making it less likely to go off topic. But in some cases, there really are many words we could sample from reasonably (broad distribution below), and in some cases there aren’t (narrow distribution below). To address this problem, the authors propose top p sampling, aka nucleus sampling, in which we compute the cumulative distribution and cut off as soon as the CDF exceeds P. In the broad distribution example above, it may take the top 100 tokens to exceed top_p = .9. In the narrow distribution, we may already exceed top_p = .9 with just “hot” and “warm” in our sample distribution. In this way, we still avoid sampling egregiously wrong tokens, but preserve variety when the highest scoring tokens have low confidence. 12345678910111213input_ids = tokenizer.encode('i m trying to', return_tensors='tf')sample_outputs = model.generate( input_ids, do_sample=True, max_length=50, top_k=10, top_p=0.9, num_return_sequences=10)print(&quot;Output:\\n&quot; + 100 * '-')for i, sample_output in enumerate(sample_outputs): print(&quot;{}: {}&quot;.format(i, tokenizer.decode(sample_output, skip_special_tokens=True))) 0: i m trying to get a list of all the words in the wordlist and their synonyms. it s a wordlist of about 10k words. i m trying to do a word frequency plot. the word frequency plot shows that the frequency of1: i m trying to find out if there are any books on bitcoin cash which are free for anyone to download. the author of the book, jr. b. lang, is a bitcoin cash expert and has been quoted in many publications as saying that2: i m trying to find a way to get my iphone from to my apple apple tv via usb. my iphone is 3rd gen and apple tv 2nd gen.3: i m trying to determine if there are two sets of rules for a particular problem that can be applied to any other problem. one set of rules is for discrete cases and the second set is for continuous cases.\r4: i m trying to understand how a node can be a node in a dapp.\r\ni m trying to understand how a node can be a node in a dapp.\r5: i m trying to do a pulldown on a website with a template.i m using jquery.getElementsByTagName(‘meta’) and the following output6: i m trying to determine the best way to store my bitcoin. my bitcoin is stored on my master wallet. however, i want to add the bch address from my wallet to my bitcoin. my master wallet only has the private key of the wallet7: i m trying to find a way to show the number of lines in the code of a function. i m using the following code snippet from the os x man page: \r8: i m trying to find the code for my samsung galaxy s3. the s3 is a s1 with an ikon gsm camera. the camera is a 1.5m lens. it also has the moto g9: i m trying to figure out a way to find out the time and date of the most recent call. i m using the time-to-call function from the samsung s go-pro app, but the function does not work on my device Prepare model for download1!zip -r -X gpt-2_tuned.zip .//content//transformers//output adding: /content//transformers//output/ (stored 0%) adding: /content//transformers//output/tokenizer_config.json (deflated 37%) adding: /content//transformers//output/vocab.json (deflated 59%) adding: /content//transformers//output/config.json (deflated 51%) adding: /content//transformers//output/training_args.bin (deflated 44%) adding: /content//transformers//output/merges.txt (deflated 53%) adding: /content//transformers//output/special_tokens_map.json (deflated 52%) adding: /content//transformers//output/checkpoint-500/ (stored 0%) adding: /content//transformers//output/checkpoint-500/optimizer.pt (deflated 9%) adding: /content//transformers//output/checkpoint-500/tokenizer_config.json (deflated 37%) adding: /content//transformers//output/checkpoint-500/vocab.json (deflated 59%) adding: /content//transformers//output/checkpoint-500/config.json (deflated 51%) adding: /content//transformers//output/checkpoint-500/training_args.bin (deflated 44%) adding: /content//transformers//output/checkpoint-500/merges.txt (deflated 53%) adding: /content//transformers//output/checkpoint-500/special_tokens_map.json (deflated 52%) adding: /content//transformers//output/checkpoint-500/scheduler.pt (deflated 49%) adding: /content//transformers//output/checkpoint-500/pytorch_model.bin (deflated 9%) adding: /content//transformers//output/pytorch_model.bin (deflated 9%) Step 5: Generate and Filter New UtterancesWe now generate the new utterances for all intents. To have a sufficiently large sample that we can choose the best utterances from, we generate 200 per intent. 1234567891011121314151617NUMBER_OF_GENERATED_UTTERANCES_PER_INTENT = 200def generate_utterances_df(n_generated, tokenizer, model, intent): input_ids = tokenizer.encode(intent + ',', return_tensors='tf') sample_outputs = model.generate( input_ids, do_sample=True, max_length=50, top_k=n_generated, top_p=0.92, num_return_sequences=n_generated) list_of_intent_and_utterances = [( intent, tokenizer.decode(sample_output, skip_special_tokens=True)[len(intent)+1:]) for sample_output in sample_outputs] return pandas.DataFrame(list_of_intent_and_utterances, columns=['intent', 'utterance']) Generate the result by calling the function above: 123456789labels = data_train[&quot;Label&quot;].unique()generated_utterances_df = pandas.DataFrame(columns=['Outcome', 'Text'])for label in labels: print(&quot;Generating for intent &quot; + label) utterances_for_intent_df = generate_utterances_df( NUMBER_OF_GENERATED_UTTERANCES_PER_INTENT, tokenizer, model, label) generated_utterances_df = generated_utterances_df.append(utterances_for_intent_df) Save file: 1generated_utterances_df.to_csv(&quot;generated.csv&quot;, index=False) Train BERT classifier with augmented datasetAfter a while the data is generated, and we can have a closer look at it. First, we use our old distilBERT classifier to predict the intent for all generated utterances. We also keep track of the prediction probability indicating the level of confidence of each individual prediction made by our model. 12345678910111213141516171819generated_data = generated_utterances_df.assign( utterance=[utterance.replace('!', '') for utterance in generated_utterances_df['utterance']])predictions_for_generated = np.array(predictor.predict( generated_data['utterance'].tolist(), return_proba=False))proba_for_predictions_for_gen = predictor.predict( generated_data['utterance'].tolist(), return_proba=True)predicted_proba = np.array([ max(probas) for probas in proba_for_predictions_for_gen])generated_data_predicted = pandas.DataFrame({ &quot;intent&quot;: generated_data['intent'], &quot;utterance&quot;: generated_data['utterance'], &quot;predicted_intent&quot;: predictions_for_generated, &quot;prediction_proba&quot;: predicted_proba}) Let’s have a look at some of the utterances for which the intent used for generation does not match the predicted intent. 12generated_data_predicted[generated_data_predicted['intent'] != generated_data_predicted['predicted_intent']].head(20) intent utterance predicted_intent prediction_proba 0 __label__12 why is there a special category called coset… __label__14 0.602601 1 __label__12 how can i set up qgis for different operating … __label__13 0.918635 2 __label__12 is there a minimum required work weekly for eu… __label__6 0.835881 5 __label__12 who was ryan about the arcania __label__10 0.564748 7 __label__12 example of combining points data __label__13 0.927397 8 __label__12 how can i switch between my the clock/utc-rpi … __label__3 0.430493 9 __label__12 which mlb agent is responsible for taxonomy of… __label__11 0.572228 10 __label__12 how can i prove that two points are continuous… __label__14 0.950781 We can see that in some cases the prediction is clearly wrong. However, there are also cases where the prediction matches the utterance, but doesn’t match the intent used for generation. This indicates that our GPT-2 model is not perfect as it doesn’t generate matching utterances for an intent all the time. To stop from training our classifier with corrupt data, we drop all utterances for which the basic intent does not match the predicted intent. For those with matching instances, we only keep the ones with the highest prediction probability scores. Filter generated utterancesFilter utterances with old classifier when prediction matches: 123correctly_predicted_data = generated_data_predicted[ generated_data_predicted['intent'] == generated_data_predicted['predicted_intent']]correctly_predicted_data.groupby(&quot;intent&quot;).count() Check for the number of unique utterances per intent: 1234correctly_predicted_data.drop_duplicates( subset='utterance', keep='first').sort_values( by=['intent', 'prediction_proba'], ascending=[True, False]).drop_duplicates( keep='first').groupby('intent').count() Take TOP_N predictions per intent according to probability and drop duplicated We can see that for each intent, there are at least 35 mutually distinct utterances. To keep a balanced data set, we pick the top 30 utterances per intent according to the prediction probability. 12345TOP_N = 30top_predictions_per_intent = correctly_predicted_data.drop_duplicates( subset='utterance', keep='first').sort_values( by=['intent', 'prediction_proba'], ascending=[True, False]).drop_duplicates( keep='first').groupby('intent').head(TOP_N) 1top_predictions_per_intent intent utterance predicted_intent prediction_proba 103 __label__0 adding jquery files from within a wordpress theme __label__0 0.810196 66 __label__0 formatting wordpress content __label__0 0.808586 131 __label__0 add.php to front page of wordpress __label__0 0.807943 177 __label__0 adding wordpress in post_meta subcategory __label__0 0.804899 51 __label__0 is there a way to change how views are display… __label__0 0.801144 … … … … … 36 __label__9 new player __label__9 0.815141 147 __label__9 is there a way to get a different card with ea… __label__9 0.772208 117 __label__9 how do i recover my lost data __label__9 0.768557 49 __label__9 i need to save my first pakistani boy in dlc 2… __label__9 0.757511 15 __label__9 how can i bypass game engine antiophthalmic fa… __label__9 0.738432 Step 6: Train the Intent Classifier with Augmented DataCombine old and augmented dataWe now combine the generated data with the initial training data and split the enriched data set intotraining and validation data. 123456789101112131415161718192021222324252627data_train_aug = data_train.append(top_predictions_per_intent[['intent', 'utterance']].rename( columns={'intent':'Label', 'utterance':'Text'}), ignore_index=True)data_train_auglabels = data_train_aug['Label'].unique()X_train_aug, X_valid_aug, X_test_aug = [], [], []y_train_aug, y_valid_aug, y_test_aug = [], [], []for label in labels: intent_X_train, intent_X_valid, intent_y_train, intent_y_valid = train_test_split( data_train[data_train['Label'] == label]['Text'], data_train[data_train['Label'] == label]['Label'], train_size=0.8, random_state=43) intent_X_valid, intent_X_test, intent_y_valid, intent_y_test = train_test_split( intent_X_valid, intent_y_valid, train_size=0.5, random_state=43) X_train_aug.extend(intent_X_train) X_valid_aug.extend(intent_X_valid) X_test_aug.extend(intent_X_test) y_train_aug.extend(intent_y_train) y_valid_aug.extend(intent_y_valid) y_test_aug.extend(intent_y_test) Initialise augmented model and learnerNow it’s time to train our new intent classification model. The code is like the one above: 123distil_bert_augmented = text.Transformer('distilbert-base-cased', maxlen=50, classes=intents) 1234567891011processed_train_aug = distil_bert_augmented.preprocess_train( X_train_aug, y_train_aug)processed_test_aug = distil_bert_augmented.preprocess_test( X_valid_aug, y_valid_aug) model_aug = distil_bert_augmented.get_classifier()learner_aug = ktrain.get_learner( model_aug, train_data=processed_train_aug, val_data=processed_test_aug, batch_size=50) Train classifierTrain classifier for given learning rate and number of epochs. 123N_TRAINING_EPOCHS_AUGMENTED = 5learner_aug.fit_onecycle(5e-5, N_TRAINING_EPOCHS_AUGMENTED) Evaluate trained predictor123456789101112predictor_aug = ktrain.get_predictor( learner_aug.model, preproc=distil_bert_augmented)predictions_aug = predictor_aug.predict(X_test_aug)np_test_intents = np.array(y_test_aug)np_predictions_aug = np.array(predictions_aug)result_aug = (np_test_intents == np_predictions_aug)print(&quot;Accuracy: {:.2f}%&quot;.format(result_aug.sum()/len(result_aug)*100)) Accuracy: 87.85% Prepare model for download12345predictor.save('models/augmented/_distilbert_aug_{}epochs_{}'.format( N_TRAINING_EPOCHS_AUGMENTED, datetime.datetime.now().strftime(&quot;%Y-%m-%d-%H-%M-%S-%f&quot;)))!zip -r -X distilbert_augmented.zip models/augmented adding: models/augmented/ (stored 0%)adding: models/augmented/_distilbert_aug_5epochs_2021-04-01-15-13-40-763970/ (stored 0%)adding: models/augmented/_distilbert_aug_5epochs_2021-04-01-15-13-40-763970/config.json (deflated 61%)adding: models/augmented/_distilbert_aug_5epochs_2021-04-01-15-13-40-763970/tf_model.h5 (deflated 8%)adding: models/augmented/_distilbert_aug_5epochs_2021-04-01-15-13-40-763970/tf_model.preproc (deflated 55%) LAMBADA AI: SummaryWe employed the LAMBADA method to augment data used for Natural Language Understanding (NLU) tasks. We trained a GPT-2 model to generate new training utterances and utilized them as training data for our intent classification model (DistilBERT). The performance of the intent classification model improved by at least 4% in each of our tests. Additionally, we saw that high-level libraries such as KTrain and Huggingface Transformers help to reduce the complexity of applying state-of-the-art transformer models for Natural Language Generation (NLG) and other Natural Language Processing (NLP) tasks such as classification and make these approaches broadly applicable.","link":"/Blog/2021/10/30/LAMBADA-Method-How-to-use-Data-Augmentation-in-NLU/"},{"title":"Study Notes of LaTeX","text":"A study notes of LaTeX, including 特殊符号 常见用法 字体设置 空格设置 插图 表格 浮动体 数学公式 参考文献——BibTex 特殊符号 $：数学模式符号 %：注释符号 ^:上标符号 {}：分组符号 \\：宏命令 ~：带子（用于一些不可隔行的空格，比如名称与编号之间的空格：Question 2） #：用于宏定义 &amp;：用于表格对齐 _：数学模式下标 要在正文中使用这些需要在前面加\\，\\需要写成\\textbackslash，^需要写成\\^{}，~需要写成\\~{}，_需要写成\\_{} 常见用法 连字符：- 数字范围：-- 破折号（——）：--- ~：$\\sim$ …：\\ldots or \\dots 句中使用省略号：$\\ldots$（使用数学模式） 幻影：\\phantom{参数} 字体设置字体族设置（罗马，无衬线，打字机） \\textrm{} \\textsf{} \\texttt{} {\\rmfamily } {\\sffamily } {\\ttfamily } 字体系列设置（粗细，宽度） \\textmd{} \\textbf{} {\\mdseries } {\\bfseries } 字体形状设置（直立，斜体，伪斜体，小型大写） \\textup{} \\textit{} \\textsl{} \\textsc{} {\\upshape } {\\itshape } {\\slshape } {\\scshape } 中文字体设置（宋体，黑体，仿宋，楷体） {\\songti } {\\heiti } {\\fangsong } {\\kaiti } \\textbf{粗体 黑体表示} \\textit{斜体 楷体表示} 字体大小设置 导言区可以设置：\\documentclass[10pt]{article}(一般只有10-12pt) {\\tiny } {\\scripysize } {\\footnotesize } {\\small } {\\normalsize } {\\large } {\\Large } {\\LARGE } {\\huge } {\\Huge } 中文字号设置：\\zihao{5} 设置字号的时候在导言区设置（newcommand），在正文区使用 空格设置 两个quad空格（两个m的宽度）： a \\qquad b quad空格（一个m的宽度）： a \\quad b 1/6个m宽度：\\thinspace 1/6m宽度：\\enspace 大空格（1/3m宽度）： a\\ b 中等空格（2/7m宽度）： a\\;b 小空格（1/6m宽度）: a\\,b 紧贴（缩进1/6m宽度）： a\\!b 1pc = 12pt = 4.218mm 长度可以为负值：a\\kern -1em b or a\\kern 1pc b a\\hskip 1em b a\\hspace{35pt} b 占位宽度：a\\hphantom{xyz} b 弹性长度：a\\hfill b 插图 导言区：\\usepackage{graphicx} 语法：\\includegraphics[&lt; 选项 &gt;]{&lt; 文件名 &gt;} 格式：EPS，PDF，PNG，JPEG，BMP 图片在当前目录下的figures和PICS目录下： 1\\graphicspath{{figures/},{pics/}} 在正文中插入图片： 12345678910111213\\begin{document} \\LaTex{} 设置缩放比例 \\includegraphics[scale=0.3]{lion} 设置高度 \\includegraphics[height=0.3cm]{lion} 设置宽度 \\includegraphics[width=0.3cm]{lion} 设置版型高度 \\includegraphics[height=0.3cm]{lion} 设置版型宽度 \\includegraphics[width=0.3cm]{lion}\\end{document} 表格12345678910111213\\begin{document} 生成五列表格，分别是左对齐，居中，居中，右对齐和指定宽度(自动换行) \\begin{tabular}{l || c | c | r | p{1.5cm}} 姓名 &amp; 语文 &amp; 数学 &amp; 外语 &amp; 备注 \\\\ \\hilne \\hilne 两个命令可以产生双横线 姓名 &amp; 语文 &amp; 数学 &amp; 外语 &amp; 备注 \\\\ \\hilne 姓名 &amp; 语文 &amp; 数学 &amp; 外语 &amp; 备注 \\\\ \\hilne 姓名 &amp; 语文 &amp; 数学 &amp; 外语 &amp; 备注 \\\\ \\hilne \\end{tabular}\\end{document} 浮动体12345678910111213141516171819202122232425\\begin{document} 交叉引用 \\LaTeX{}中\\Tex系统吉祥物见图\\ref{fig-lion} \\begin{figure} 图片居中 \\centering \\includegraphics[scale=0.3]{lion} 设置图片标题并且设置标签 \\caption{\\Tex 吉祥物}\\label{fig-lion} \\end{figure} \\begin{table} 表格居中 \\centering 设置图片标题 \\caption{考试成绩单} \\begin{tabular}{l || c | c | r | p{1.5cm}} 姓名 &amp; 语文 &amp; 数学 &amp; 外语 &amp; 备注 \\\\ \\end{tabular} \\end{table} \\end{document} \\begin{figure}[&lt;允许位置&gt;] &lt;允许位置&gt;参数（默认tbp） h: 此处（here）-代码所在的上下文 t: 页顶（top）-代码所在页面或者之后页面的顶部 b: 页底（bottom）-代码所在页面或之后页面的底部 p: 独立一页（page）-浮动页面 标题控制：caption和bicaption等宏包 并排与子图表：subcaption，subfig和floatrow等宏包 绕排：picinpar和wrapfig等宏包 注意：应该合理使用交叉引用而不是硬编码。 数学公式行内公式$$a+b=b+a$$ $ a+b=b+a $ \\( a+b=b+a \\) \\begin{math} a+b=b+a \\end{math} 上下标上标 $3x^{20} - x + 2 = 0$ $$3x^{20} - x + 2 = 0$$ 上标 $a_0, a_1, A_{100x+2}$ $$a_0, a_1, A_{100x+2}$$ 希腊字母 $\\alpha$ : $\\alpha$ $\\beta$: $\\beta$ $\\gamma$: $\\gamma$ $\\epsilon$: $\\epsilon$ $\\pi$: $\\pi$ $\\Gamma$: $\\Gamma$ $\\Delta$: $\\Delta$ $\\Theta$: $\\Theta$ $\\Pi$: $\\Pi$ $\\Omega$: $\\Omega$ 数学函数 $\\log$: $\\log$ $\\sin$: $\\sin$ $\\cos$: $\\cos$ $\\arcsin$: $\\arcsin$ $\\arccos$: $\\arccos$ $\\sqrt{2}$ : $\\sqrt{2}$ 分式 $3/4$: $3/4$ $\\frac{3}{4}$: $\\frac{3}{4}$ 行间公式$$\\frac{3}{4}$$ 比例是 $$\\frac{3}{4}$$ 比例是 \\[frac{3}{4}\\] 比例是 \\begin{displaymath} frac{3}{4} \\end{displaymath} 对公式进行自动编号并且对公式进行交叉引用： 1234交换律见式\\ref{eq:commutative}\\begin{equation} a+b=b+a \\label{eq:commutative}\\end{equation} 对公式不进行自动编号并且对公式进行交叉引用（需要amsmath宏包）： 1234交换律见式\\ref{eq:commutative}\\begin{equation*} a+b=b+a \\label{eq:commutative}\\end{equation*} 矩阵（需要amsmath宏包）12345678\\begin{document} \\[ \\begin{matrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{matrix} \\]\\end{document} 加入小括号\\begin{pmatrix}\\end{pmatrix} 加入中括号\\begin{bmatrix}\\end{bmatrix} 加入大括号\\begin{Bmatrix}\\end{Bmatrix} 加入单竖线\\begin{vmatrix}\\end{vmatrix} 加入双竖线\\begin{Vmatrix}\\end{Vmatrix} 矩阵中的上下标：\\a_{11}^2 矩阵中的省略号：\\dots \\vdots \\ddots \\adots 矩阵中的跨列省略号：\\hdotsfor{&lt;列数&gt;} 矩阵整体下标：\\end{bmatrix}_{n \\times n} 矩阵行内小矩阵（smallmatrix） array环境排版更为复杂的环境 多行公式（需要amsmath和amssymb宏包）12345678910111213141516171819202122232425\\begin{document} % 用gather（带编号）和gather*（不带编号）环境（可以使用\\\\换行） \\begin{gather} a+b=b+a \\\\ ab ba \\end{gather} % 在\\\\之前使用\\notag阻止编号 \\begin{gather} a+b=b+a \\notag \\\\ ab ba \\end{gather} % 用align环境按照指定位置对齐（align*不对公式进行编号），对齐位置由&amp;决定 \\begin{align} a+b&amp;=b+a \\\\ ab&amp; ba \\end{align} % 用split环境将公式进行多行排版 \\begin{split} a+b&amp; \\\\ =b+a \\end{split}\\end{document} 参考文献——BibTex一次管理，一次使用1234567\\begin{thebibliography} \\bibitem[记号]{引用标志}文献条目1 %使用`\\emph`标重 \\bibitem{article1}陈立辉,苏伟.\\emph{基于LaTeX的Web数学公式提取方法研究}[J]. 计算机科学。 2014(06) %使用`\\texttt`引用链接 \\bibitem{latexGuide}Kopka,Patrick.\\emph{Guide to \\LaTeX}, $4^{th}$ Edition. Available at \\texttt{http://www.amazon.com}\\end{thebibliography} 使用\\cite{article1}引用文章 一次管理，多次使用123456789@BOOK{引用标志, title = {}, publisher = {}, year = {}, author = {}, series = {}, address = {}, adition = {}} 使用\\bibliographystyle{plain}指定样式 正文中使用： 123\\begin{document} \\bibliography{引用标志}\\end{document} 排版未引用的文献：\\nocite{*} 参考文献——BibLaTex12345678910111213\\usepackage[style=numeric,backend=biber]{biblatex}\\addbibresource{test.bib}%正文区\\begin{document} %一次管理，多次引用 无格式化引用\\cite{biblatex} 带方括号的引用\\parencite{a1-1} 上标引用\\supercite{6-1} %列出未引用文献 \\nocite{*} \\printbibliography[title = {参考文献}]\\end{document} 命令的定义与重定义 内容与格式分离的思想 12345%导言区\\documentclass{ctexart}（或者ctexbook, ctexrep）%\\newcommand-定义命令%命令只能由字母组成，不能以\\end开头%\\newcommand&lt;命令&gt;[&lt;参数个数&gt;][&lt;手参数默认值&gt;]{&lt;具体定义&gt;} 例如：使用\\PRC 相当于 People’s Pepublic of \\emph{China} 123456\\newcommand\\PRC{People's Republic of \\emph{China}}%正文区（文稿区）\\begin{docuemnt} \\PRC\\end{document} \\newcommand还可以添加参数，参数个数从1-9，使用时前添加# 123456\\newcommand\\loves[2]{#1 喜欢 #2}%正文区（文稿区）\\begin{docuemnt} \\loves{猫儿}{鱼儿}\\end{document} 为\\newcommand添加默认参数值 12345678910\\newcommand\\loves[3][喜欢]{#2#1#3}%正文区（文稿区）\\begin{docuemnt} %输出猫儿喜欢鱼儿 \\loves{猫儿}{鱼儿} %输出猫儿最爱鱼儿 \\loves[最爱]{猫儿}{鱼儿}\\end{document} 使用\\renewcommand\\abstractname重定义命令 使用\\newenvironment定义环境 123456789%\\newenvironment{&lt;环境名称&gt;}[&lt;参数个数&gt;][&lt;首参数默认值&gt;]% {&lt;环境前定义&gt;}% {&lt;环境后定义&gt;} \\newenvironment{myabstract}[1][摘要]{\\small \\begin{center}\\bfseries #1\\end{center} \\begin{quotation}} {\\end{quotation}} 使用\\renewenvironment重定义环境 \\newenvironment和\\newcommand嵌套使用 1234567891011\\newenvironment{Quotation}[1]{\\newcommand\\quotesource{#1} \\begin{quotation}} {\\par\\hfill--- 《\\textit{\\quotesource}》 \\end{quotation}} \\begin{document} \\begin{Quotation}{乾卦} 初九，潜龙勿用。 \\end{Quotation}\\end{document} 文章结构内容参考： https://www.bilibili.com/video/av16002978/","link":"/Blog/2019/05/14/Study-Notes-of-LaTeX/"},{"title":"Study Notes of Django","text":"The model layer The view layer The template layer [toc] Project Structure docs: documentation scripts manage.py: installed to PATH via setup.py [project_name] apps: project-specific applications settings: settings for different environments, see below urls.py wsgi.py static: site-specific static files templates: site-specific templates tests: site-specific tests (mostly in-browser ones) tmp: excluded from git setup.py The model layerA model is the single, definitive source of information about your data. It contains the essential fields and behaviors of the data you’re storing. Generally, each model maps to a single database table. Each model is a Python class that subclasses django.db.models.Model. Each attribute of the model represents a database field. Defined the model123456789from django.db import modelsclass Person(models.Model): &quot;&quot;&quot; first_name and last_name are fields of the model. Each field is specified as a class attribute, and each attribute maps to a database column. &quot;&quot;&quot; first_name = models.CharField(max_length=30) last_name = models.CharField(max_length=30) Using models Once you have defined your models, you need to tell Django you’re going to use those models. Do this by editing your settings file and changing the INSTALLED_APPS setting to add the name of the module that contains your models.py. 12345INSTALLED_APPS = [ #... 'myapp', #...] Model methods Define custom methods on a model to add custom “row-level” functionality to your objects. Whereas Manager methods are intended to do “table-wide” things, model methods should act on a particular model instance. 1234567891011121314151617181920212223242526from django.db import modelsclass Person(models.Model): first_name = models.CharField(max_length=50) last_name = models.CharField(max_length=50) birth_date = models.DateField() def baby_boomer_status(self): &quot;Returns the person's baby-boomer status.&quot; import datetime if self.birth_date &lt; datetime.date(1945, 8, 1): return &quot;Pre-boomer&quot; elif self.birth_date &lt; datetime.date(1965, 1, 1): return &quot;Baby boomer&quot; else: return &quot;Post-boomer&quot; @property def full_name(self): &quot;&quot;&quot; Also known as “managed attributes”, and a feature of Python since version 2.2. This is a neat way to implement attributes whose usage resembles attribute access, but whose implementation uses method calls. &quot;&quot;&quot; &quot;Returns the person's full name.&quot; return '%s %s' % (self.first_name, self.last_name) basic operations 12345678910111213141516171819# 增：增加一条数据，可以接受字典类型数据 **kwargsmodels.Tb1.objects.create(c1='xx', c2='oo')# 删：删除指定条件的数据models.Tb1.objects.filter(name='seven').delete()# 改：将指定条件的数据更新，均支持 **kwargsmodels.Tb1.objects.filter(name='seven').update(gender='0')obj = models.Tb1.objects.get(id=1)obj.c1 = '111'obj.save()# 查# 获取单条数据，不存在则报错（不建议）models.Tb1.objects.get(id=123)# 获取全部models.Tb1.objects.all()# 获取指定条件的数据models.Tb1.objects.filter(name='seven') Double Underscore 123456789101112131415161718192021222324252627282930313233343536373839# Get the numbermodels.Tb1.objects.filter(name='seven').count()# Greater than, Less thanmodels.Tb1.objects.filter(id__gt=1) # Get the value with id greater than 1models.Tb1.objects.filter(id__lt=10) # Get the value with id less than 1models.Tb1.objects.filter(id__lt=10, id__gt=1) # Get the value with id greater than 1 and less than 10# inmodels.Tb1.objects.filter(id__in=[11, 22, 33]) # Get data with id equal to 11, 22, 33models.Tb1.objects.exclude(id__in=[11, 22, 33]) # not in# containsmodels.Tb1.objects.filter(name__contains=&quot;ven&quot;)models.Tb1.objects.filter(name__icontains=&quot;ven&quot;) # icontains: Case-insensitivemodels.Tb1.objects.exclude(name__icontains=&quot;ven&quot;)# Not contain# rangemodels.Tb1.objects.filter(id__range=[1, 2]) # bettwen and# Other similar# startswith，istartswith, endswith, iendswith# order bymodels.Tb1.objects.filter(name='seven').order_by('id') # ascmodels.Tb1.objects.filter(name='seven').order_by('-id') # desc# limit / offsetmodels.Tb1.objects.all()[10:20]# group by# SELECT &quot;app01_tb1&quot;.&quot;id&quot;, COUNT(&quot;app01_tb1&quot;.&quot;num&quot;) AS &quot;c&quot; # FROM &quot;app01_tb1&quot; # WHERE &quot;app01_tb1&quot;.&quot;c1&quot; = 1 # GROUP BY &quot;app01_tb1&quot;.&quot;id&quot;from django.db.models import Count, Min, Max, Summodels.Tb1.objects.filter(c1=1).values('id').annotate(c=Count('num')) Making queries Retrieving specific objects with filters The QuerySet returned by all() describes all objects in the database table. Usually, though, you’ll need to select only a subset of the complete set of objects. To create such a subset, you refine the initial QuerySet, adding filter conditions. The two most common ways to refine a QuerySet are: *filter(*kwargs) returns a new QuerySet containing objects that match the given lookup parameters. *exclude(*kwargs) Returns a new QuerySet containing objects that do not match the given lookup parameters. Field lookups Basic lookups keyword arguments take the form field__lookuptype=value. (That’s a double-underscore). 1234# DjangoEntry.objects.filter(pub_date__lte='2006-01-01')# SQLSELECT * FROM blog_entry WHERE pub_date &lt;= '2006-01-01'; Filters can reference fields on the model 123456789# To find all the blog entries with more than twice as many comments as pingbacks, we modify the query:Entry.objects.filter(number_of_comments__gt=F('number_of_pingbacks') * 2)# To find all the entries where the rating of the entry is less than the sum of the pingback count and comment count:Entry.objects.filter(rating__lt=F('number_of_comments') + F('number_of_pingbacks'))# An F() object with a double underscore will introduce any joins needed to access the related object. # To retrieve all the entries where the author’s name is the same as the blog name:Entry.objects.filter(authors__name=F('blog__name')) The view layerWriting views A view function, or view for short, is a Python function that takes a Web request and returns a Web response. For the sake of putting the code somewhere, the convention is to put views in a file called views.py, placed in your project or application directory. Simple example The view returns an HttpResponse object that contains the generated response. Each view function is responsible for returning an HttpResponse object. To display this view at a particular URL, you’ll need to create a URLconf; see URL dispatcher for instructions. 12345678910111213# import the class HttpResponse from the django.http module, along with Python’s datetime library.from django.http import HttpResponseimport datetimedef current_datetime(request): &quot;&quot;&quot; This is the view function. We define a function called current_datetime. Each view function takes an HttpRequest object as its first parameter, which is typically named request. &quot;&quot;&quot; now = datetime.datetime.now() html = &quot;&lt;html&gt;&lt;body&gt;It is now %s.&lt;/body&gt;&lt;/html&gt;&quot; % now return HttpResponse(html) URL dispatcher To design URLs for an app, you create a Python module informally called a URLconf (URL configuration). This module is pure Python code and is a mapping between URL path expressions to Python functions (your views). How Django processes a request Django determines the root URLconf module to use. Ordinarily, this is the value of the ROOT_URLCONF setting, but if the incoming HttpRequest object has a urlconf attribute (set by middleware), its value will be used in place of the ROOT_URLCONF setting. Django loads that Python module and looks for the variable urlpatterns. This should be a sequence of django.urls.path() and/or django.urls.re_path() instances. re_path(): The route argument should be a string or gettext_lazy() (see Translating URL patterns) that contains a regular expression compatible with Python’s re module. Simple Example 12345678910111213141516171819202122from django.urls import pathfrom . import views# uising pathurlpatterns = [ path('articles/2003/', views.special_case_2003), # To capture a value from the URL, use angle brackets. path('articles/&lt;int:year&gt;/', views.year_archive), # Captured values can optionally include a converter type. path('articles/&lt;int:year&gt;/&lt;int:month&gt;/', views.month_archive), path('articles/&lt;int:year&gt;/&lt;int:month&gt;/&lt;slug:slug&gt;/', views.article_detail),]# /articles/2003/03/building-a-django-site/ would match the final pattern. # Django would call the function views.article_detail(request, year=2003, month=3, slug=&quot;building-a-django-site&quot;).# using re_pathurlpatterns = [ path('articles/2003/', views.special_case_2003), re_path(r'^articles/(?P&lt;year&gt;[0-9]{4})/$', views.year_archive), re_path(r'^articles/(?P&lt;year&gt;[0-9]{4})/(?P&lt;month&gt;[0-9]{2})/$', views.month_archive), re_path(r'^articles/(?P&lt;year&gt;[0-9]{4})/(?P&lt;month&gt;[0-9]{2})/(?P&lt;slug&gt;[\\w-]+)/$', views.article_detail),] Path converters str - Matches any non-empty string, excluding the path separator, '/'. This is the default if a converter isn’t included in the expression. int - Matches zero or any positive integer. Returns an int. slug - Matches any slug string consisting of ASCII letters or numbers, plus the hyphen and underscore characters. For example, building-your-1st-django-site. uuid - Matches a formatted UUID. To prevent multiple URLs from mapping to the same page, dashes must be included and letters must be lowercase. For example, 075194d3-6885-417e-a8a8-6c931e272f00. Returns a UUID instance. path - Matches any non-empty string, including the path separator, '/'. This allows you to match against a complete URL path rather than a segment of a URL path as with str. The template layerTemplates Variables A variable outputs a value from the context, which is a dict-like object mapping keys to values Dictionary lookup, attribute lookup and list-index lookups are implemented with a dot notation 12345My first name is {{ first_name }}. My last name is {{ last_name }}.{{ my_dict.key }}{{ my_object.attribute }}{{ my_list.0 }} Tags Tags provide arbitrary logic in the rendering process. This definition is deliberately vague. For example, a tag can output content, serve as a control structure e.g. an “if” statement or a “for” loop, grab content from a database, or even enable access to other template tags. 123{% csrf_token %}{% cycle 'odd' 'even' %}{% if user.is_authenticated %}Hello, {{ user.username }}.{% endif %} Filters Filters transform the values of variables and tag arguments. 12{{ django|title }}{{ my_date|date:&quot;Y-m-d&quot; }} Comments 1{# this won't be rendered #} Python programmers Overview You configure an Engine. You compile template code into a Template. You render the template with a Context. Loading a template The recommended way to create a Template is by calling the factory methods of the Engine: get_template(), select_template() and from_string(). In a Django project where the TEMPLATES setting defines a DjangoTemplates engine, it’s possible to instantiate a Template directly. If more than one DjangoTemplates engine is defined, the first one will be used. 123from django.template import Templatetemplate = Template(&quot;My name is {{ my_name }}.&quot;) Rendering a context 12345678910&gt;&gt;&gt; from django.template import Context, Template&gt;&gt;&gt; template = Template(&quot;My name is {{ my_name }}.&quot;)&gt;&gt;&gt; context = Context({&quot;my_name&quot;: &quot;Adrian&quot;})&gt;&gt;&gt; template.render(context)&quot;My name is Adrian.&quot;&gt;&gt;&gt; context = Context({&quot;my_name&quot;: &quot;Dolores&quot;})&gt;&gt;&gt; template.render(context)&quot;My name is Dolores.&quot; SOME TIPSVIEWS123# url: mypage.com/?page=2request.GET['page'] # That will force get page param, and you will if not foundrequest.GET.get('page', '1') # Tha will check if param exists, and return 1 if not found","link":"/Blog/2020/09/29/Study-Notes-of-Django/"},{"title":"Study Notes of MySQL 1 —— Query Function","text":"DQL (Data Query Language) [toc] MySQL 基础 MySQL 服务的登录和退出 123456# 登录方式一：MySQL 自带客户端# 登录方式二：通过 Windows 自带的客户端# mysql 【-h 主机名 -p 端口号】 -u用户名 -p密码# 退出方式：# exit or Ctrl + C MySQL常见命令 12345678910111213141516171819202122232425262728# 查看当前所有数据库# show databases;# 打开指定库# use 库名# 查看当前库的所有表# show tables;# 查看其他库的所有表# show tables from 库名;# 创建表# create table 库名(列名 列类型,列名 列类型);# 查看表结构# desc 表名;# 查看服务器版本# 登录到MySQL# select version()# 没有登录到MySQL# mysql --version# mysql -V MySQL 的语法规范 不区分大小写 建议关键字大写，表名列名小写 每条命令最好用分号结尾 每条命令根据需要进行缩进或者换行 注释的方式 单行注释：# 注释文字 单行注释：– 注释文字 多行注释：/* 注释文字 */ DQL (Data Query Language)123456789# 完整的查询语句select 查询列表from 表join 表2where 筛选条件group by 分组列表having 分组后的筛选order by 排序列表limit 偏移，条目数 基础查询1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 调用的库USE myemployees;# 查询列表可以是：表中的字段，常量，表达式，函数# 的结果是一个虚拟的表格SELECT 查询列表 FROM 表名;# 查询表中的单个字段SELECT last_name FROM employees;# 查询表中多个字段SELECT last_name, salary, email FROM employees;# 查询表中的所有字段SELECT * FROM employees;# 查询常量值SELECT 100;SELECT 'john';# 查询表达式SELECT 100%98;# 查询函数SELECT VERSION();# 起别名# 方法一SELECT 100%98 AS 结果;SELECT last_name AS 姓, first_name AS 名 FROM employees;# 方法二SELECT last_name 姓, first_name 名 FROM employee;# 别名和关键字相同SELECT last_name AS 'OUT PUT' FROM employee;# 去重SELECT DISTINCT department_id FROM employees;# '+' 的作用# 仅仅只有一个功能，就是运算符，不能用来操作字符串# 如果存在字符型，那么就会试图转换成数值型，转换成功，继续做加法# 如果转换失败，那么字符型就转换成0# 如果其中一方为 null, 结果返回 nullSELECT last_name + first_name AS 姓名 FROM employee;# 拼接字段SELECT CONCAT(last_name, first_name) AS 姓名 FROM employee; 条件查询123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293# 语法# 先执行 FROM 然后是 WHERE 最后是 SELECTSELECT 查询列表 FROM 表名 WHERE 筛选条件; # 分类# 按照条件表达式筛选，条件运算符： &gt; &lt; = != &lt;&gt; &gt;= &lt;=# 按照逻辑表达式筛选，逻辑运算符：&amp;&amp; || ! and or not# 模糊查询：like/between and/in/is null# 按照条件表达式筛选SELECT * FROM employees WHERE salary&gt;12000;SELECT last_name, department_idFROM employeesWHERE department_id != 90; # 按照逻辑表达式筛选SELECT last_name, salaryFROM employeesWHERE salary &gt;= 10000 AND salary &lt;= 20000; # 模糊查询# like 一般和通配符搭配使用# ％ 任意多个字符# - 单个字符#SELECT *FROM employeesWHERE last_name LIKE '%a%';# 转义方法一：\\ 进行转义SELECT last_nameFROM employeesWHERE last_name LIKE '_\\_%';# 转移方法二：ESCAPESELECT last_nameFROM employeesWHERE last_name LIKE '_$_%' ESCAPE '$'; # between and# 提高语句简洁度（包含连接值，等同于 &gt;= &lt;=）SELECT *FROM employeesWHERE employee_id BETWEEN 100 AND 120; # in# in 列表的值类型需要统一或者兼容（'123' 和 123）SELECT last_nameFROM employeesWHERE employee_id in ('IT','AD');# is null# = 和 &lt;&gt; 不能用来判断 NULLSELECT last_name, commission_pctFROM employeesWHERE commission IS NOT NULL; # 安全等于：&lt;=&gt;# 安全等于可以判断数值和 NULLSELECT last_name, commission_pctFROM employeesWHERE commission &lt;=&gt; NULL; 排序查询1234567891011121314151617181920212223242526272829303132333435363738/*语法: SELECT FROM ORDER BY 【asc(可省略) or desc(降序)】注意： order by 可以支持单个字段，多个字段，表达式，函数，别名 order by 一般放在查询语句的最后面，limit 子句除外*/# 案例SELECT * FROM employees ORDER BY salary DESC;SELECT * FROM employees ORDER BY salary ASC;SELECT *FROM employeesWHERE department_id &gt;= 90ORDER BY hiredate ASC;# 加入表达式SELECT *, salary*12*(1+IFNULL(commission_pct, 0)) 年薪FROM employeesORDER BY salary*12*(1+IFNULL(commission_pct, 0)) DESC;# 表达式加别名SELECT *, salary*12*(1+IFNULL(commission_pct, 0)) 年薪FROM employeesORDER BY 年薪 DESC;# 加入函数SELECT LENGTH(last_name) 字节长度FROM employeesORDER BY LENGTH(last_name) DESC;# 多个字段排序FROM *FROM employeesORDER BY salary ASC, employee_id DESC; 常见函数123# 功能：将一组逻辑语句封装在方法体中，对外暴露方法名# 调用：SELECT 函数名(实参列表) 【FROM 表】; 字符函数123456789101112131415161718192021222324252627# length 获取参数值的字节个数SELECT LENGTH('john');# concat 拼接字符串SELECT CONCAT(last_name,first_name) 姓名 FROM employees;# upper lowerSELECT CONCAT(upper(last_name), LOWER(first_name)) 姓名 FROM employees;# substr substring# 索引从1开始# 两个参数从指定索引数指定字符长度的字符SELECT SUBSTR('李莫愁爱上了陆展元',7) out_put;SELECT SUBSTR('李莫愁爱上了陆展元',1,3) out_put;# instr 返回子串出现的第一次索引，没有返回 0SELECT INSTR('杨不悔爱上了殷六侠','殷六侠') AS out_put;# trim 去掉前后字符SELECT TRIM(' 张翠山 ') AS out_put;SELECT TRIM('a' FROM ' 张翠山 ') AS out_put;# lpad 用指定的字符实现左填充指定长度SELECT LPAD('殷素素', 10, '*') AS out_put;# rpad 用指定的字符实现右填充指定长度SELECT RPAD('殷素素', 10, '*') AS out_put;# replace 替换SELECT REPLACE('张无忌爱上了周芷若周芷若','周芷若','赵敏') AS out-put; 数学函数123456789101112131415# round 四舍五入SELECT ROUND(1.55);# ceil 向上取整 返回大于等于该参数的最小整数SELECT CEIL(1.00)# floor 向下取整 返回小于等于该参数的最小整数SELECT FLOOR(-9.99)# truncate 截断 保留小数点后几位SELECT TRUNCATE(1.89, 1)# mod 取余 mod(a,b) = a - a/b*bSELECT MOD(10, 3);SELECT 10/3; 日期函数123456789101112131415161718192021222324# now 返回当前系统日期 + 时间SELECT NOW()# curdate 返回当前时间，不包含日期SELECT CURDATE()# curtime 返回当前时间，不包含日期SELECT CURTIME()# 可以获取指定的部分，年 月 日 小时 分钟 秒SELECT YEAR(NOW()) 年;SELECT YEAR('2018-1-1') 年;SELECT YEAR(hiredate) 年 FROM employees;SELECT MONTH(NOW()) 月;SELECT MONTHNAME(NOW()) 月;# str_to_date 将日期格式的字符转换为指定格式的日期SELECT STR_TO_DATE('9-13-1999','%m-%d-%Y');SELECT * FROM employees WHERE hiredate = STR_TO_DATE('4-3 1992', '%c-%d %Y');# date_format 将日期转化成字符SELECT DATE_FORMAT(NOW(), '%y年%m月%d日') AS out_put; 其他函数123SELECT VERSION();SELECT DATABASE();SELECT USER(); 流程控制函数1234567891011121314151617181920212223# if 函数：if else 效果SELECT IF(10&lt;5, '大', '小');SELECT last_name, commission_pct, IF(commission_pct IS NULL, '没奖金，呵呵','有将近，嘻嘻') 备注;# case 函数作用一：switch case 效果SELECT salary 原始工资, department_id,CASE department_id WHEN 30 THEN salary * 1.1WHEN 40 THEN salary * 1.2WHEN 50 THEN salary * 1.3ELSE salaryEND AS 新工资FROM employees;# case 函数作用二：多重 if 效果SELECT salary,CASEWHEN salary&gt;20000 THEN 'A'WHEN salary&gt;15000 THEN 'B'WHEN salary&gt;10000 THEN 'C'ELSE 'D'END AS 评级FROM employees; 分组函数12345678910111213141516171819202122232425# 用作统计使用，又称为聚合函数或者统计函数或者组函数# 简单的使用SELECT SUM(salary) FROM employees;SELECT AVG(salary) FROM employees;SELECT MIN(salary) FROM employees;SELECT MAX(salary) FROM employees;SELECT COUNT(salary) FROM employees;SELECT SUM(salary) 和, AVG(salary)平均FROM employees;# 参数支持哪些类型# SUM AVG 数值型# MIN MAX COUNT 所有类型# 是否忽略 null# SUM AVG MIN MAX COUNT 都忽略 null 值# 可以和 DISTINCT 搭配使用SELECT COUNT(DISTINCT salary) FROM employees;# count 函数的详细介绍# 统计行数SELECT COUNT(*) FROM employees;SELECT count(1) FROM employees; # 相当于增加了一列常量值，然后统计常量值个数，相当于统计行数 分组查询123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/*语法： SELECT 分组函数，列 FROM 表 WHERE 筛选条件 GROUP BY 分组的列表 ORDER BY 子句注意： 查询列表必须特殊，要求是分组函数和group by之后出现的字段 分组前筛选用 WHERE 数据源是原始表 放在分组前 分组后筛选用 HAVING 数据源是分组后的结果集 放在分组后 支持多个字段分组 可以添加排序，放在最后*/# 查询每个工种最高工资SELECT MAX(salary), job_idFROM employeesGROUP BY job_id;# 查询每个位置上的部门个数SELECT COUNT(*), location_idFROM departmentsGROUP BY location_id;# 查询每个部门，邮件中有a的员工，的平均工资SELECT AVG(salary), departemnt_idFROM employeesWHERE email LIKE '%a%'GROUP BY department_id;# 查询哪个部门的员工个数大于2# 添加分组后的筛选条件使用 HAVINGSELECT COUNT(*), department_idFROM employeesGROUP BY department_idHAVING COUNT(*) &gt; 2;# 按照表达式或函数分组# 按照员工姓名的长度分组，查询每一组员工的个数，并且筛选员工个数大于五SELECT COUNT(*) c, LENGTH(last_name) len_nameFROM employeesGROUP BY len_nameHAVING c &gt; 5;# 按照多个字段分组SELECT AVG(salary), department_id, job_idFROM employeesGROUP BY job_id. departemt_id;# 添加排序SELECT AVG(salary) a, department_id, job_idFROM employeesWHERE department_id IS NOT NULLGROUP BY job_id, department_idHAVING a &gt; 10000ORDER BY a DESC; 连接查询按照年份分类 sql92 标准：支持内连接 sql99 标准：支持内连接，外连接（左右连接）和交叉连接 按照功能分类 内连接（等值连接，非等值连接，自连接） 外连接（左外连接，右外连接，全外连接） 交叉连接 sql92 标准等值连接123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/*用法： SELECT name, boyName FROM boys, bueaty WHERE bueaty.boyfriend_id = boys.id;*/# 查询员工名，工种号，工种名SELECT last_name, employees.job_id, job_titleFROM employees, jobsWHERE employees.'job_id' = jobs.'job_id';# 为表起别名SELECT e.last_name, e.job_id, j.job_titleFROM employees e, jobs jWHERE e.'job_id' = j.'job_id';# 加筛选# 查询有将近的员工名和部门名SELECT last_name, department_name, commission_pctFROM employees e, departments dWHERE e.'department_id' = d.'department_id'AND e.'commission_pct' IS NOT NULL;# 加分组# 查询每个城市的部门数量SELECT COUNT(*) 个数, cityFROM departments d, locations lWHERE d.'location_id' = l.'location_id'GROUP BY city;# 查询有奖金的部门名和部门领导编号和最低工资SELECT department_name, manager_id, MIN(salary)FROM department d, employees eWHERE d.'department_id' = e.'department_id'AND commission_pct IS NOT NULLGROUP BY department_name, d.manager_id;# 加排序# 查询每个工种的工种名，员工个数，并且按照员工个数降序SELECT job_titile, COUNT(*)FROM employees e, jobs jWHERE e.'job_id' = j.'job_id'GROUP BY job_titleORDER BY COUNT(*) DESC;# 三表连接SELECT last_name, department_name,cityFROM emplyees e, departments d, locations lWHERE e.'departemnt_id' = d.'department_id'AND d.'location_id' = l.'location_id'; 非等值连接1234# 查询工资和工资级别SELECT salary, grade_levelFROM emplyees e, job_grades gWHERE salary BETWEEN g.'lowest_sal' AND g.'highest_sal'; 自连接1234# 查询员工名和上级名称SELECT e.employee_id, e.last_name, m.employee_id, m.last_nameFROM employees e, employees mWHERE e.'manager_id' = m.'employee_id'; sql99 标准12345678SELECT 查询列表FROM 表1 别名 【连接类型】JOIN 表2 别名 ON 连接条件WHERE 筛选条件GROUP BY 分组HAVING 分组后筛选ORDER BY 排序列表 内连接 – 等值连接1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/*语法: SELECT 查询列表 FROM 表1 别名 INNER JOIN 表2 别名 ON 连接条件*/# 查询员工名和部门名SELECT last_name. department_nameFROM employees, eINNER JOIN departments dON e.'department_id' = d.'department_id';# 加入筛选SELECT last_name, job_titleFROM omployees eINNER JOIN jobs jON e.'job_id' = j.'job_id'WHERE e.'last_name' LIKE '%e%';# 添加分组加筛选SELECT city, COUNT(*) 部门个数FROM departments dINNER JOIN locationsON d.'location_id' = l.'location_id'GROUP BY cityHAVING COUNT(*) &gt; 3;# 添加排序SELECT COUNT(*), department_nameFROM employees, eINNER JOIN departments dON e.'department_id' = d.'department_id'GROUP BY department_idHAVING COUNT(*) &gt; 3ORDER BY COUNT(*) DESC;# 三表连接SELECT last_name, department)name, job_titleFROM employees eINNER JOIN departments dON e.'department_id' = d.'department_id'INNER JOIN jobs jON e.'job_id' = j.'job_id'ORDER BY department_name DESC; 内连接 – 非等值连接12345# 查询员工工资级别SELECT salary, grade_levelFROM employees eJOIN job_grades gON e.'salary' BETWEEN g.'lowest_sal' AND g.'highest_sal'; 内连接 – 自连接12345# 查询员工名字和上级的名字SELECT e.last_nme, m.last_nameFROM employees eJOIN employees mON e.'manager_id' = m.'employee_id'; 外连接应用场景：用于查询一个表中有，另一个表中没有的情况 有主从表之分 外连接的查询结果为主表中的所有记录 左外连接，left join 左边是主表 右外连接，right join 右边是主表 123456789101112131415161718# 查询男朋友不在男神表的女神名（左外连接）SELECT b.name, bo.*FROM beauty bLEFT OUTER JOIN boys boON b.'boyfriend_id' = bo.'id';# 查询男朋友不在男神表的女神名（右外连接）SELECT b.name, bo.*FROM boys boRIGHT OUTER JOIN beauty b ON b.'boyfriend_id' = bo.'id';# 查询哪个部门没有员工SELECT d.*, e.employee_idFROM departments dLEFT OUTER JOIN employees eON d.'department_id' = e.'department_id'WHERE e.'employee_id' IS NULL; 交叉连接1234# 标准语法实现笛卡尔乘积SELECT b.*, bo.*FROM beauty bCROSS JOIN boys bo; 连接总结Here are the different types of the JOINs in SQL: (INNER) JOIN: Returns records that have matching values in both tables LEFT (OUTER) JOIN: Returns all records from the left table, and the matched records from the right table RIGHT (OUTER) JOIN: Returns all records from the right table, and the matched records from the left table FULL (OUTER) JOIN: Returns all records when there is a match in either left or right table 子查询 出现在其他语句中的 select 语句，称为子查询或者内查询 外部的查询语句，称为主查询或者外查询 分类：按照子查询出现的位置 select 后面（支持标量子查询） from 后面（支持表子查询） where 或者 having 后面（支持标量子查询，行子查询，列子查询） exists 后面（相关子查询，支持表子查询） 分类：按照结果集的行列数 标量子查询（结果集只有一行一列） 列子查询（结果集只有一列多行） 行子查询（结果集只有一行多列） 表子查询（结果集一般为多行多列） where 或者 having 后面12345678910111213141516171819202122232425262728293031323334353637383940414243444546/*用法： where 或者 having 后面 子查询放在小括号内，放在条件的右侧 子查询的执行是优先于主查询执行注意： 标量子查询搭配着单行的操作符：&gt; &lt; = &gt;= &lt;= &lt;&gt; 列查询一般搭配着多行操作符：in any some all*/# 标量子查询（where 后面）：谁的工资比 Abel 高SELECT *FROM employeesWHERE salary &gt; ( SELECT salary FROM employees WHERE last_name = 'Abel');# 标量子查询（having 后面）：查询最低工资大于 50 号部门，最低工资的部门id和其最低工资SELECT MIN(salary), department_idFROM employeesGROUP BY department_idHAVING MIN(salary) &gt; ( SELECT MIN(salary) FROM employees WHERE department_id = 50);# 列子查询（多行子查询）：返回 location_id 是 1400 或 1700 之间的部门的所有员工姓名SELECT last_nameFROM employeesdWHERE department_id IN( SELECT DISTINCT department_id FROM departments WHERE location_id IN (1400, 1700));# 行子查询（结果集一行多列或者多行多列）# 案例：查询员工编号最小并且工资最高的员工信息SELECT *FROM employeesWHERE (employee_id, salary) = ( SELECT MIN(emplyee_id), MAX(salary) FROM employees); select 后面1234567# 查询每个部门的员工个数SELECT d.*, ( SELECT COUNT(*) FROM employees WHERE e.'department_id' = d.'department_id')FROM departments d; from 后面12345678910111213/*用法：将子查询的结果充当一张表，要求必须起别名*/# 查询每个部门的平均工资的工资等级SELECT ag_dep.*, g.'level'FROM ( SELECT AVG(salary) ag, department_id FROM employees GROUP BY department_id) ag_depINNER JOIN job_grades gON ag_dep.ag BETWEEN lowest_sal AND highest_sal exists 后面（相关子查询）1234567891011121314151617181920212223242526272829303132333435363738/*语法： 查询结果有没有，返回布尔类型示例： SELECT EXISTS( SELECT employee_id FROM employees WHERE salary = 30000 );*/# 查询有员工的部门名SELECT department_nameFROM departments dWHERE EXISTS( SELECT * FROM employees e WHERE d.'department_id' = e.'department_id');# 查询没有女朋友的男神信息# 使用 IN 的方法SELECT bo.*FROM boys boWHERE bo.id NOT IN( SELECT boyfriend_id FROM bueaty)# 使用 EXISTS 的方法SELECT bo.*FROM boys boWHERE NOT EXISTS( SELECT boyfriend_id FROM beauty b WHERE bo.'id' = b.'boyfriend_id'); 分页查询1234567891011121314151617181920212223242526272829/*使用场景： 当显示的数据，一页显示不全，需要分页提交 sql 请求语法: SELECT 查询列表 FROM 表 LIMIT offset, size;offset: 要显示的条目的起始索引size：要显示的条目个数limit 放在查询语句的最后，执行顺序也是最后*/# 查询前五条员工信息SELECT *FROM employeesLIMIT 0,5;# 查询第十一条到第二十五条SELECT *FROM employeesLIMIT 10, 15;# 查询有奖金的员工中工资较高的前十名SELECT *FROM employeesWHERE commission_pct IS NOT NULLORDER BY salary DESCLIMIT 10; union 联合查询12345678910111213141516/*作用：将多条查询语句的结果合并成一个结果语法： 查询语句 1 UNION 查询语句 2 UNION ...*/# 查询部门编号大于 90 或者邮箱包含 a 的员工信息SELECT * FROM employees WHERE email LIKE '%a%' OR department_id &gt; 90;# 或者SELECT * FROM employees WHERE email LIKE '%a%'UNIONSELECT * FROM employees WHERE department_id &gt; 90;","link":"/Blog/2020/08/17/Study-Notes-of-MySQL-1--Query%20Function/"},{"title":"Study Notes of MySQL 3 —— Variables, Procedures and Functions","text":"Variables Stored Procedures and Functions Control Flow Functions [toc] Variables系统变量1234567891011121314151617181920/*分类： 全局变量：作用域是全局设置 会话变量：作用域仅仅是当前连接（会话）注意： 如果全局级别，则需要加 global，如果是会话级别，则需要加session，如果不写，则默认*/# 查看所有的系统变量show global variables;# 查看满足条件的部分系统变量show global variables likee '%char%';# 查看指定的某个系统变量的值select @@global.系统变量名;# 为某个系统变量赋值set global.系统变量名 = 值;set @@global.系统变量名 = 值; 自定义变量 说明：变量是用户自定义的，不是由系统提供的 分类 用户变量：作用域针对当前会话有效，等同于会话变量的作用域 局部变量：仅仅在 begin end 中有效 使用步骤 声明 赋值 使用（查看，比较，运算） 用户变量 1234567891011121314151617181920212223# 声明并初始化SET @用户变量名 = 值;SET @用户变量名 := 值;SELECT @用户变量名 := 值;# 赋值SET @用户变量名 = 值;SET @用户变量名 := 值;SELECT @用户变量名 := 值;SELECT 字段 INTO 变量名 FROM 表;# 使用：查看用户变量的值SELECT @用户变量名/*案例*/# 声明SET @count=1;# 赋值SELECT COUNT(*) INTO @countFROM employees;# 查看SELECT @count; 局部变量 应用在 begin end 中的第一句话 12345# 声明DECLARE 变量名 类型;DECLARE 变量名 类型 DEFAULT 值;# 赋值同上# 使用同上 Stored Procedures and Functions 存储过程和函数存储过程的使用 含义：一组预先编译好的 SQL 语句的集合，理解成批处理语句 减少了编译次数并且减少了和数据库服务器连接的次数，提高了效率 参数列表包含三部分：参数模式 参数名 参数类型 参数模式 IN：该参数可以作为输入，也就是该参数需要调用方传入值 OUT：该参数可以作为输出，也就是该参数可以作为返回值 INOUT：该参数既可以作为输入又可以作为输出，也就是该参数既需要传入值，又可以返回值 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 创建语法CREATE PROCEDURE 存储过程名（参数列表）BEGIN 存储过程体（一组合法的 SQL 语句）END# 参数列表IN stuname VARCHAR(20)# 调用语法CALL 存储过程名（实参列表）;# 判断用户是否登录成功（IN）CREATE PROCEDURE myp3(IN username VARCHAR(20), IN PASSWORD VARCHAR(20))BEGIN DECLARE result INT DEFAULT 0; # 声明并初始化 SELECT COUNT(*) INTO result # 赋值 FROM admin WHERE admin.username = username AND admin.PASSWORD = PASSWORD; SELECT IF(result&gt;0,'Success','Fail'); # 使用END $CALL myp3('Tim Duncan','000') $# 根据输入的女生名，返回对应男生的名字和魅力值（OUT）CREATE PROCEDURE myp2(IN beautyName VARCHAR(20), OUT boyName VARCHAR(20), OUT usercp INT)BEGIN SELECT boys.boyname, boys.usercp INTO boyname, usercp FROM boys RIGHT JOIN beauty b ON b.boyfriend_id = boys.id WHERE b.name beautyName;END $CALL myp('小昭', @name, @cp)$SELECT @name, @cp;# 创建带INOUT模式参数的存储过程# 传入 a b 两个值，最终 a b 都翻倍并返回CREATE PROCEDURE myp3(INOUT a INT, INOUT b INT)BEGIN SET a = a * 2 SET b = b * 2;END# 调用SET @m=10$SET @n=20$CALL myp3(@m,@n)$SELECT @m,@n# 存储过程的删除：drop procedure 存储过程名DROP PROCEDURE p1;# 存储过程的查看：show create procedure 存储过程名SHOW CREATE PROCEDURE myp2; 函数 含义：一组预先编译好的 SQL 语句的集合，理解成批处理语句 与存储过程的区别 存储过程：可以有 0 个返回，也可以有多个返回，合适做批处理插入，批处理更新 函数：有且仅有 1 个返回，合适做处理数据后返回一个结果 1234567891011121314# 语法CREATE FUNCTION 函数名(参数列表) RETURNS 返回类型BEGIN 函数体END# 调用SELECT 函数名(参数列表)# 查看函数SHOW CREATE FUNCTION myf;# 删除函数DROP FUNCTION myf; Control Flow Functions 流程控制结构分支结构IF1IF(表达式1, 表达式2, 表达式3) # 表达式1成立，则返回表达式2，否则返回表达式3 CASE 特点 可以作为表达式，嵌套在其他语句中使用，可以放在任何地方 可以作为独立的语句去使用，放在 BEGIN END 中 12345678910111213# 实现等值判断：类似于 JAVA 中的 SWITCHCASE 变量|表达式|字段WHEN 要判断的值 THEN 返回的值1 或者 语句1;WHEN 要判断的值 THEN 返回的值2 或者 语句2;ELSE 要返回的值n;END CASE;# 实现区间判断：类似于 JAVA 中的 多重 IFCASE WHEN 要判断的条件1 THEN 返回的值1 或者 语句1;WHEN 要判断的条件2 THEN 返回的值2 或者 语句2;ELSE 要判断的条件n;END CASE; IF 结构123IF 条件1 THEN 执行语句1;ELSEIF 条件2 THEN 执行语句2;ELSE 语句n; 循环结构循环主体 while：先判断后执行 repeat：先执行后判断 loop：没有条件就是死循环 123456789101112131415161718192021222324252627# while[标签:] WHILE 循环条件 DO 循环体;END WHILE [标签];# loop[标签:] LOOP 循环体;END LOOP [标签]# repeat[标签:] REPEAT 循环体;UNTIL 结束循环的条件END REPEAT [标签]# 批量插入：根据插入次数到 admin 表中多条记录CREATE PROCEDURE pro_while(IN insertCount INT)BEGIN DECLARE i INT DEFAULT 1; WHILE 1 &lt; insertCount DO INSERT INTO admin(username, 'password') VALUES (CONCAT('Rose', 1), '666'); SET i = i + 1; END WHILEEND $CALL pro_while(100)$ 循环控制 leave：类似于 break，用于跳出所在循环 iterate：类似于 continue，用于结束本次循环，继续下一次 12345678910111213141516171819202122232425# leave# 批量插入：根据插入次数到 admin 表中多条记录，如果次数大于 20 则停止CREATE PROCEDURE pro_while(IN insertCount INT)BEGIN DECLARE i INT DEFAULT 1; a: WHILE 1 &lt; insertCount DO INSERT INTO admin(username, 'password') VALUES (CONCAT('Rose', 1), '666'); IF i &gt;= 20 THEN LEAVE a; END IF; SET i = i + 1; END WHILE a;END $# iterate# 批量插入：根据插入次数到 admin 表中多条记录，只插入偶数次CREATE PROCEDURE pro_while(IN insertCount INT)BEGIN DECLARE i INT DEFAULT 1; a: WHILE 1 &lt; insertCount DO SET i = i + 1; IF MOD(i,2) != 0 THEN ITERATE a; END IF INSERT INTO admin(username, 'password') VALUES (CONCAT('Rose', 1), '666'); END WHILE a;END $","link":"/Blog/2020/08/31/Study-Notes-of-MySQL-3--Variables-Procedures-and-Functions/"},{"title":"Study Notes of MySQL 2 —— Manipulation, Definition and Transaction Control","text":"DML (Data Manipulation Language) DDL (Data Definition Language) TCL (Transaction Control Language) [toc] DML (Data Manipulation Language)Insert Classic Way 123456789101112131415161718/*语法： INSERT INTO 表名（列名，...） VALUES （值1，...）注意： 1. 插入的值的类型要与列的类型一致或者兼容 2. 不可以为NULL的列必须插入值 3. 可以为NULL的列插入值的方式： 列名写上，值填写 NULL 列名与值均直接省略 4. 列的顺序可以调换 5. 列的个数和值的个数必须匹配 6. 可以省略列名，默认所有列，顺序与表中顺序一致*/# e.g.INSERT INTO beautyVALUES(18, '张飞', '男', NULL, '119', NULL, NULL) Streamlined Way 123456789/*语法： INSERT INTO 列名 SET 列名 = 值, 列名 = 值*/# e.g.INSERT INTO bueatySET id = 19, NAME = '刘涛', phone = '999'; Classic Way VS Streamlined Way 12345678910# 方式一支持插入多行，方式二不支持INSERT INTO playersVALUES (33, 'Larry Bird'),(21, 'Tim Duncan');# 方式一支持子查询，方式二不支持INSERT INTO beauty(id, NAME, phone)SELECT id, boy_nameFROM boysWHERE id &lt; 3; Update1234567891011121314151617181920212223242526/*修改 *单表* 语法： UPDATE 表名 SET 列 = 值，列 = 值 WHERE 筛选条件； 修改 *多表* 92 语法： UPDATE 表名1, 表名2 SET 列 = 值, ... WHERE 连接条件 AND 筛选条件; 修改 *多表* 99 语法： UPDATE 表1 INNER/LEFT/RIGHT JOIN 表2 ON 连接条件 SET 列 = 值, ... WHERE 筛选条件;*/# 修改张无忌的女朋友的手机号为114UPDATE boys boINNER JOIN beauty bON bo.'id' = b.'boyfriend_id'SET b.'phone' = '114'WHERE bo.'boyName' = '张无忌'; Delete &amp; Truncate Delete 1234567891011121314151617/*DELETE 单表删除 语法： DELETE FROM 表名 WHERE 筛选条件; DELETE 多表删除 92 语法： DELETE 表1的别名，表2的别名（删除谁写谁的别名） FROM 表1 别名, 表2 别名 WHERE 连接条件 AND 筛选条件; DELETE 多表删除 99 语法： DELETE 表1的别名，表2的别名（删除谁写谁的别名） FROM 表1 INNER/LEFT/RIGHT JOIN 表2 WHERE 筛选条件;*/ Truncate 123456/*TRUNCATE 单表删除 语法： TRUNCATE TABLE 表名 注意： 一删全删，不能加 WHERE*/ Delete vs Truncate 假如删除的表中有自增长列 如果用 delete 删除之后，再插入数据，自增长列的值从断点开始 如果用 truncate 删除后，再插入数据，自增长列的值从 1 开始 返回值 delete 有返回值 truncate 无返回值 回滚 delete 删除不能回滚 truncate 删除可以回滚 DDL (Data Definition Language)库的管理 创建 1234567/*语法： CREATE DATABASE 库名;*/# e.g.CREATE DATABASE IF NOT EXISTS books; 修改 123456/*用途： 修改库的字符集语法： ALTER DATABASE books CHARCTER SET gbk;*/ 删除 1234567/*语法： DROP DATABASE 库名;*/# e.g.DROP DATABASE IF EXISTS books; 表的管理 创建 1234567891011121314151617/*语法： CREATE TABLE 表名( 列名 列类型(长度) 列的约束, 列名 列类型(长度) 列的约束, ... 列名 列类型(长度) 列的约束);*/# 创建表 bookCREATE TABLE book( id INT, # number BName, VARCHAR(20), # book name author VARCHAR(20), # author name authorId INT, # number of the author publishId DATETIME # date of publish); 修改 1234567891011121314151617181920212223242526272829303132333435363738394041/*修改 列名 语法： ALTER TABLE 表名 CHANGE COLUMNS 旧列名 新列名 新列类型;*/ALTER TABLE book CHANGE COLUMNS publishdate pubDate DATETIME;/*修改 列的类型或者约束 语法： ALTER TABLE 表名 MODIFY COLUMNS 列名 新列类型;*/ALTER TABLE bookMODIFY COLUMNS pubdate TIMESTAMP;/*添加 新列 语法： ALTER TABLE 表名 ADD COLUMNS 列名 列类型; [# FIRST/AFTER 字段名]*/ALTER TABLE author ADD COLUMNS annul DOUBLEAFTER t2; /*删除 列 语法： ALTER TABLE 表名 DROP COLUMNS 列名;*/ALTER TABLE authorDROP COLUMNS annual;/*修改 表名 语法： ALTER TABLE 表名 RENAME TO 新表名*/ALTER TABLE author RENAME TO book_author 删除 12345/*语法： DROP TABLE 表名;*/DROP TABLE IF EXISTS book_author; 复制 12345678910111213141516171819202122232425/*复制表的 结构 语法： CREATE TABLE 新表名 LIKE 旧表名;*/CREATE TABLE copy LIKE author;/*复制表的 结构 + 数据 语法： CREATE TABLE 新表名 SELECT * FROM 旧表名;*/CREATE TABLE copy2 SELECT * FROM author;# 复制部分数据CREATE TABLE copy2 SELECT id, author_nameFROM authorWHERE nation = 'China';# 复制部分结构（部分字段）CREATE TABLE copy SELECT id, author_nameFROM authorWHERE 1 = 2; # WHERE 0; 数据类型 数值型 - 整型 整数类型 字节数 范围 Tinyint 1 有符号：-128 -127 无符号：(0-255) Smallint 2 有符号：-32768 - 32767 无符号：(0 - 65535) Mediumint 3 很大 Int, integer 4 很大 Bigint 8 很大 123456789# 设置无符号（默认有符号）CREATE TABLE IF EXISTS tab_int( t1 INT; t2 INT UNSIGNED);# 如果插入数值超出范围：插入临界值# 如果不设置长度就是默认长度# 设置了长度不改变范围，只改变显示长度，真是的范围只由整数类型决定（搭配 ZEROFILL 使用可以用 0 在左侧填充） 小数 - 定点数 浮点数类型 默认 字节 范围 float(M,D) 根据插入的值来确定精度 4 很大 double(M,D) 根据插入的值来确定精度 8 很大 小数 - 浮点数 定点数类型 默认 字节 范围 DEC(M,D)DECIMAL(M,D) M=10, D=0 M+2 最大取值范围与double相同，给定decimal的有效取值范围由M和D决定 12345678# M：整数部位和小数部位总长度# D：小数点后几位# M 和 D 都可以省略CREATE TABLE tab_float( f1 FLOAT(5,2), f1 DOUBLE(5,2), f1 DECIMAL(5,2),); 字符型 - 较短的文本 字符串类型 差别 最多字符数 描述 char(M) 可变类型 M M 为 0-255 之间的整数 varchar(M) 不可变类型 M M 为 0-255 之间的整数 binary &amp; varbinary / / 保存较短的二进制 enum / / 用于保存枚举 set / / 用于保存集合 字符型 - 较长的文本 text blob（较长的二进制数据） 日期类型 日期和时间类型 字节 最小值 最大值 date 4 1000-01-01 9999-12-31 datetime 8 1000-01-01 00:00:00 9999-12-31 23:59:59 timestamp 4 1970010108001 2038年的某个时刻 time 3 -838:59:59 838:59:59 year 1 1901 2155 常见约束 含义：一种限制，用于限制表中的数据，为了保证表中的数据的准确和可靠性 语法 123CREATE TABLE 表名( 字段名 字段类型 约束 ); 分类：六大约束 NOT NULL：非空，用于保证该字段的值不为空（姓名，学号） DEFAULT：默认，用于保证该字段有默认值（性别） PRIMARY KEY：主键，用于保证该字段的值具有唯一性，并且非空（学号，编号） UNIQUE：唯一，用于保证搞字段的值具有唯一性，可以为空（座位号） CHECK：检查约束（MySQL中不支持） FORRIGN KEY：外键，用于限制两个表的关系，用于保证该字段的值必须来自于主键的关联列的值，在从表中添加外键约束，用于引用主表中的列值（学生表的专业编号，员工表的部门编号） 添加约束的时机 创建表时 修改表时 约束的添加分类 列级约束 六大约束都可以写（语法上都支持） 外键约束无效果 表级约束 除了非空/默认，其他都支持 创建表时添加 列级 约束 12345678910111213/*创建 *表* 时添加 *列级* 约束*/USE students;CREATE TABLE stuinfo( id INT PRIMARY KEY, # 主键 stuName VARCHAR(20) NOT NULL, # 非空 gender CHAR(1) CHECK(gender='Male' OR gender='Female'), # 检查 seat INT UNIQUE, # 唯一 age INT DEFAULT 18, # 默认 majorId INT FOREIGN KEY REFERENCES major(id) # 外键); 创建 表 时添加 表级 约束 12345678910111213141516171819202122232425262728293031/*创建 *表* 时添加 *表级* 约束语法：在各个字段的最下面 [constraint 约束名] 约束类型（字段名）*/DROP TABLE IF EXISTS stuinfo;CREATE TABLE stuinfo( id INT, stuName VARCHAR(20), gender CHAR(1), seat INT, age INT, majorId INT, CONSTRAINT pk PRIMARY KEY(id), # 主键 CONSTRAINT up UNIQUE(seat), # 唯一 CONSTRAINT ck CHECK(gender='Male' OR gender='Female'), # 检查 CONSTRAINT fk_stuinfo_major FOREIGN KEY(majorid) REFERENCES major(id) # 外键);# 通用写法CREATE TABLE stuinfo( id INT PRIMARY KEY, # 主键 stuName VARCHAR(20) NOT NULL, # 非空 gender CHAR(1), seat INT UNIQUE, # 唯一 age INT DEFAULT 18, # 默认 majorId INT, CONSTRAINT fk_stuinfo_major FOREIGN KEY(majorid) REFERENCES major(id) # 外键); 修改 表 时添加 列级 约束 123456789101112/*修改 *表* 时添加 *列级* 约束 alter table 表名 modify column 字段名 字段类型 新约束;*/# 非空约束ALTER TABLE stuinfo MODIFY COLUMN stuname VARCHAR(20) NOT NULL;# 默认约束ALTER TABLE stuinfo MODIFY COLUMN age INT DEFAULT 18;# 主键约束-列级ALTER TABLE stuinfo MODIFY COLUMN id INT PRIMARY KEY;# 唯一约束-列级ALTER TABLE stuinfo MODIFY COLUMN seat INT UNIQUE; 修改 表 时添加 表级 约束 12345678910/*修改 *表* 时添加 *表级* 约束 alter table 表名 add [constraint 约束名] 约束类型（字段名）;*/# 主键约束-表级ALTER TABLE stuinfo ADD PRIMARY KEY(id);# 唯一约束-表级ALTER TABLE stuinfo ADD UNIQUE(seat);# 外键约束ALTER TABLE stuinfo ADD [CONSTRAINT fk_stuinfo_major] FOREIGN KEY(majorid) REFERENCES major(id); 修改 表 时删除约束 12345678910111213/*修改 *表* 时删除约束*/# 删除非空约束ALTER TABLE stuinfo MODIFY COLUMN stuname VARCHAR(20) NULL;# 删除默认约束ALTER TABLE stuinfo MODIFY COLUMN age INT;# 删除主键ALTER TABLE stuinfo DROP PRIMARY KEY;# 删除唯一约束ALTER TABLE stuinfo DROP INDEX seat;# 删除外键约束ALTER TABLE stuinfo DROP FOREIGN KEY fk_stuinfo_major; 标识列 标识列：又称为自增长列 含义 可以不用手动的插入值，系统提供默认的序列值 特点 标识类不必须和主键搭配，但要求是一个 KEY 一个表中只能有一个标识列 标识列只能是数值型 可以设置步长（auto_increment_increment = 3） 可以通过手动插入值，来设置起始值 创建表时设置标识列 123456789# 创建表时设置标识列CREATE TABLE tab_identity( id INT PRIMARY KEY AUTO_INCREMENT, NAME VARCHAR(20));# 可以通过手动插入值，来设置起始值INSERT INTO tab_identity(id, NAME) VALUES(10, 'john');INSERT INTO tab_identity(id, NAME) VALUES(NULL, 'john'); 修改表时设置标识列 1ALTER TABLE tab_identity MODIFY COLUMN id INT PRIMARY KEY AUTO_INCREMENT; 修改表时删除标识列 1ALTER TABLE tab_identity MODIFY COLUMN id INT; TCL (Transaction Control Language)Basic Terminology **事务：一个或者一组 sql 语句组成的一个执行单元。**这个执行单元要么全部执行，要么全部不执行。如果单元中的某条 SQL 语句执行失败，那么整个单元将会回滚。 存储引擎 概念：在 MySQL 中的数据用各种不同的技术存储在文件或者内存中。 通过 Show Engines 来查看 MySQL 支持的存储引擎 在 MySQL 中用的最多的存储引擎有：innodb, myisam, memory 等。其中 innodb 支持事务。 事务的 ACID 属性 Atomicity 原子性：事务是一个不可分割的工作单位 Consistency 一致性：事务必须使数据库从一个一致性状态变换到另一个一致性状态。 Isolation 隔离性：事务的执行不能被其他的事务干扰，即一个事务内部的操作以及使用的数据对并发的其他事物是隔离的，并发执行的各个事物之间是不能互相干扰的。 Durability 持久性：事务一旦提交，对数据库的改变就是永久性的。 事务的创建 隐式事务：事务没有明显的开启和结束的标记 比如：insert / update / delete 显式事务：事务具有明显的开启和结束的标记 123456789101112131415161718192021# 前提：必须先设置自动提交功能为禁用set autocommit = 0;# 可选START TRANSACTION;# 结束事务COMMIT; # 提交事务ROLLBACK; # 回滚事务# e.g.SET autocommit = 0;START TRANSACTION;UPDATE account SET balance = 500 WHERE username = 'Tim Duncan';UPDATE account SET balance = 1500 WHERE username = 'Larry Bird';COMMIT;# e.g.SET autocommit = 0;START TRANSACTION;UPDATE account SET balance = 1000 WHERE username = 'Tim Duncan';UPDATE account SET balance = 1000 WHERE username = 'Larry Bird';ROLLBACK; 数据库的隔离级别 隔离级别 描述 READ UNCOMMITTED 读未提交数据 允许事务读取未被其他事物提交的变更。脏读，幻读，不可重复读的问题都会出现。 READ COMMITTED 读已提交数据 只允许事务读取已经被其他事务提交的变更。可以避免脏读，但不可重复读和幻读仍然可能出现。 REPEATABLE READ 可重复读 确保事务可以多次从一个字段中读取相同的值，在这个事务持续期间，禁止其他事务对这个字段进行更新。可以避免脏读和不可重复读，但幻读仍然存在。 SERIALIZABLE 串行化 确保事务可以从一个表中读取相同的行，在这个事务持续期间，禁止其他事务对该表执行插入，更新和删除。所有并发问题可以避免。 如果没有设置隔离机制 脏读：T1 读取了已经被 T2 更新但是还没有被提交的字段。 不可重复读：T1读取了一个字段，然后 T2 更新了该字段之后，T1 再次读取了同一个字段，值就不同了。 幻读：T1从一个表中读取了某一个字段，T2 在该表中插入了一些新的行，如果 T1 再次读取同一个表，就会多出几行。 MySQL的默认事务隔离级别：REPEATABLE READ 设置当前 MySQL 连接的隔离级别 set transaction isolation level read committed 设置数据库系统的全局的隔离级别 set global transaction isolation level read committed 回滚点123456SET autocommit = 0;START TRANSACTION;DELETE FROM account WHERE id = 25;SAVEPOINT a; # 设置保存点DELETE FROM account WHERE id = 29;ROLLBACK TO a; # 回滚到保存点 视图 含义：虚拟表，和普通表一样使用（MySQL 15.1 版本出现的新特性，是通过表动态生成的数据）。只保存 SQL 逻辑，不保存查询结果。 应用场景 多个地方用到同样的查询结果 该查询结果使用的 SQL 语句比较复杂 特性 复用 SQL 语句 简化复杂的 SQL 操作，不必知道查询细节 保护数据，提高安全性 视图的创建和使用 1234567891011121314151617181920/*创建 视图 语法： CREATE VIEW 视图名 AS 查询语句;使用 视图 语法： SELECT * FROM 视图名;*/# 查询张姓同学的姓名和专业SELECT stuname. majornameFROM stuinfo sINNER JOIN major m ON s.'majorid' = m.'id';SELECT * FROM v1 WHERE stuname LIKE '张%'; 视图的修改 123456789101112131415161718192021/*方式一： CREATE OR REPLACE VIEW 视图名 AS 查询语句;*/CREATE OR REPLACE VIEW myv3ASSELECT * FROM employees;/*方式二： ALTER VIEW 视图名 AS 查询语句;*/ALTER VIEW myv3ASSELECT * FROM employees; 视图的删除 123456/*语法： DROP VIEW 视图名, 视图名;*/DROP VIEW myv1, myv2; 视图的查看 12DESC myv3;SHOW CREATE VIEW myv3; 视图的更新 123# 插入 INSERT INTO myv1 VALUES('张飞','1234@sina.com') 修改 UPDATE myn1 SET last_name = ‘Tim Duncan’ WHERE last_name = ‘Larry Bird’; 删除 DELETE FROM myv1 WHERE last_name = ‘Tim Duncan’; 123456789101112131415 - **具备以下关键词的视图不允许更新** - 包含以下关键字的 SQL 语句：```分组函数```，```distinct```，```group by```，```having```，```union``` 或者 ```union all``` - 常量视图：```SELECT 'John' NAME;``` - ```select``` 中包含子查询 - ```join``` - ```from ``` 一个不能更新的视图 - ```where``` 子句的子查询引用了 ```from``` 子句中的表- **视图与表的对比** - 视图不占用实际的物理空间，表占用 - 视图保存语句逻辑，表保存了实际结果数据 - 视图一般不进行增删改","link":"/Blog/2020/08/29/Study-Notes-of-MySQL-2--Manipulation-Definition-and-Transaction-Control/"},{"title":"Study Notes of TOEFL Speaking Part -- Task 2-4","text":"TPO TASK 2 - 4TASK 2【TPO 1】A student thinks that the university shouldn’t require this sculpture While, a student thinks that the university shouldn’t require this sculpture, umm, this is because, you know, the university is in no financial position of doing so, and also, the sculpture is actually quite large, so it will take up all the green space. However, the women didn’t like this idea, this is because, umm, actually, the university didn’t pay for the sculpture , there is a donor donating this to the university. also she know why this students didn’t like this sculpture**, this is because, you know,** he can’t play soccer in that green area anymore, and for him, he doesn’t want to move. So, these are the reason why the women does not like the idea. 【TPO 2】The university plans to eliminate the bus service While, the university plans to eliminate the bus service. uum, this is because only few students ride it and also it is kind of expensive to operate it. uum, on top of that, the money saved by eliminate the bus could be used to expand the parking lots. However, the man did not like this idea. He thinks that the route of the bus is out of date, and this is why students didn’t take the bus. you know, the bus only go through the expensive neighborhood. and the students also didn’t like the idea of expanding the parking spaces, because this will encourage more students to drive, which will add more noise and you know students will end up needing more parking spaces. Therefore, he doesn’t like the idea. 【TPO 3】The university want to eliminate the hot breakfast While, the university want to eliminate the hot breakfast. This is because they think the cold breakfast is healthier. and also this can help the university to same money, so they can keep the meal plan affordable. However, the women does not like this idea. For example, in the cold morning, students will love to have some hot cereal rather than cold yogurt. And also, the women doesn’t think this can help them to save money, because if school doesn’t offer hot breakfast, students have to go to off-campus places to get them. So, this is why the student doesn’t like this idea. 【TPO 54】Community Service Requirement While, the students want university to require every students to do the service in the town. This is because this action can strengthen the relationship between school and town. and also, it can help students to discover the love of doing the service that can inspire them. However, the men doesn’t like this idea. He think it will hurt the relation between the school and the town. Because students will see the volunteer as the chore and do it without enthusiastic and also do not put their heart into it. So, this can not make the things better. Besides, he said that students will have negative feelings because they already have enough time. Therefore, this is why the student doesn’t like this idea. 【TPO 53】University announces this new energy saving plan While, the university announces this new energy saving plan First of all, the new lighting with less power will be installed in the library, and also, the air conditioning will be reduced on hot days. However, the women doesn’t like this idea. First of all, she think the new lights are not bright enough, so the students will turn on the reading lamps, in this way, any saving will go away. And besides, the woman said that the library will become less comfortable on hot days if the air conditioning is reduced, so students will go back to dorm, and dorm’s study environment is not as good as the library because of the noisy. Therefore, the woman doesn’t like this idea. 【TPO 49】University Should Make the Textbook List Available Earlier. While, a student has suggest that the university should make the textbook list available earlier. Because it can give students more time to shop around to find cheaper books. And also, students can work on reading for the new courses in the next semester earlier. And, in the conversation, the man really like the idea. First of all, he thinks that it is kind of expensive for students to buy new books. On campus, there are a few used ones. By having this list earlier, students can buy online to save money. And also, the man says that, things get pretty busy when semester starts, so it is really hard to keep up the reading. By having the list available earlier, it is a great opportunity to do the preparation. 【TPO 48】University should build an art museum While, the students think that university should build an art museum. because, it can give students an opportunity to appreciate fine arts. Also, he said that the university can write letters to the alumni and ask them to donate the money for building the art museum. However, in the conversation, the man doesn’t like this idea. First of all, he think that there is an art museum in the down and only a half hour bus ride. And only cost two dollars, so students can see some great work there. And besides, the university has already ask the alumni to donate for building the student center and library, so students can not expect them to donate again any time soon. 【TPO 47】School should publish students’ evaluation about professor While, the student suggests that school should publish students’ evaluation about professor. Because, professor will be motivated to improve their teaching. And also, this evaluation can be helpful for students to make informed decisions about which course to take. However, the women doesn’t like the idea. First of all, professor would not be happy to be criticized by students. So, professor wouldn’t take it seriously. And besides, students are often in a hurry making the evaluation, so they aren’t say anything specific about the course, so it can not help students to better make their choice. Therefore, the woman doesn’t like the idea. 【TPO 46】No more posters outside the student center While, the university has decided that there will be no more posters outside the student center. Because, it can improve the campus appearance. And also, students can use the bulletin in the dining hall for posters. However, the women doesn’t like the idea. First of all, she said that, without these posters, the student center just a boring build. You know, nothing about it is interesting. But a lot of posters are artistic and they give building some personalities and some characters. And besides, the women doesn’t like the idea that using the bulletin in dining hall, because not everyone eat in the dining hall. So, some students will not able to see these announcements. Therefore, the woman doesn’t like this idea. 【TPO 45】The university should close the coffeehouse While, a student thinks that the university should close the coffeehouse. because, coffeehouse is often empty, so it not a good place to meet people. And also, the light is poor in this coffeehouse, so it is not a good place to study either. However, the women doesn’t like this idea. First of all, she said that, actually, lots of students go to the coffeehouse during the evening. the place is empty during the day because students are often busy. Besides, the women also said that, the coffeehouse has done some renovation, you know, now, the lighting is as good as the library. Therefore, the women doesn’t like this idea. 【TPO 44】university should create a student committee While, a student thinks that the university should create a student committee to decide funding for students organization. First of all, he thinks that only students knows which organization is more important. And also, by serving this committee, students can gain valuable leadership experience. However, in the conversation, the man doesn’t like this idea. First of all, he thinks that students are too close to the situation, they may end up giving their money to friends. So, he thinks that the administrators are more fair. Also, the man says that the committee can not find people to serve, because students are often too busy, and the job is just too big of time commitment. 【TPO 43】 The university has decided to change the orientation program. While, the university has decided to change the orientation program. It used to be a two days hiking trip. But now students could choice which activity they want to participate in. And university thinks these changes will encourage more students to take advantage of the opportunity and to get to know each other better. And also, the orientation program will only last for 1 day, not 2 days. You know, the man really likes the idea. First of all, he says that, not everyone like the same thing, like he didn’t like sleeping in a tent, so that is why he didn’t go to his orientation program. Also, with this new program, students just don’t have to give up the whole weekend. You know, there are often busy during the weekend like buying books like so on. And the big time commitment used to get in the way. TASK 3【TPO 1】Group Think While the reading passage talks about the group think, you know, it means that the individual members try to conform their opinion to what they believe to be group consensus, even though the result could be negative. For example, the professor suggest that a design makeover for the company’s product, at first, more than half of people support his idea, but one senior manager did not like his idea because he said the focus should be on the technology not on the design, suddenly, people start to change their attitude, because they don’t want to disagree with the manager. Eventually, most people decided to stay with the current design. Unfortunately, the competitor at that year, came out a new design that attracted some of their customers. Therefore, this is a good example for group think. 【TPO 2】Audience effect While, the reading passage is talking about audience effect. It means that individual at work is effected by the knowledge that they are being visible to others. uum, for example, there are two groups of students who are putting on their shoes, the first group are told that they were observed, and the second group, they didn’t know they are observed. And for the results, the group one tight their shoes faster. While another example is about how to type, if we know there are some people are observing us, we tend to type faster, at the same time, interestingly, we may also to make more mistakes. Therefore, these are two examples to explain the idea of audience effect. 【TPO 3】Cognitive Dissonant While, the reading passage is talking about cognitive dissonant. you know, it means that discomfort by conflict between the one’s believes and one’s action. for example, the professor was addicted to the video game in high school, so he doesn’t perform well in the chemistry. Then he just had this conflict, because he thought he has to do well in all subjects if he want to succeed in futures. In order to solve this conflict, the professor changed his perspective, since he didn’t want to be a chemist, he realize that chemistry didn’t matter, he just have to do well in sociology because he want to be a sociologist. So, he reinterpret the situation to solve the conflict. Therefore, this is the good example to explain the cognitive dissonant. 【TPO 54】System Thinking While, the reading passage is talking about system thinking. you know, it means that company will solve the problem in a long-term, considering the interaction of different parts contributing to the problem. For example, the professor used to work in a company where workers were absent a lot. So, the company hire a consultant to solve the problem. So, they did a lot of research, such as interview a lot of employees, as well as research on the eating facilities and health service. Then they found that, worker missed their work because of the health problem, you know, they are lack of exercises, so the consultant suggested that company could build a gym and offer more nutritious menu at cafeteria. And about a year late, attendance were no longer a problem. Therefore, this is a good example to explain the system thinking. 【TPO 53】Chaining Behavior While, the reading passage talks about chaining behavior You know, it is a technique used by parents to teach kinds how to do a complex behavior. For example, the professor taught her daughter how to wash hand. First, he divided this behavior into five small steps, turning on the water, wetting hands, putting on soap, washing hands and turning off water, so first, he taught her daughter step one, and he taught her daughter step two when she can do step one herself and ask her to do both step one and step two. And so on, he taught her daughter step three, four and five, and ask her daughter to practicing all the steps a few days. Eventually, her daughter can wash her hands all by her self. 【TPO 49】Procedural Memories While, the professor talks about procedural memories. Which is means the memories of the process of performing a task that become automatic with practice. For example, the professor took a guitar lesson when he was young. During the class, his teacher show him how to hold the guitar, where to play his finger on the string. When he back home, he would play for hours everyday. So, the professor could pick up the guitar and play a son without thinking. But then, he stop playing for years. But on day, when he found his old guitar, he realized that he still knew how to play every song. Therefore, it is a good example to explain the procedural memories. 【TPO 48】Optimal Foraging While, the reading passage talks about optimal foraging. You know, it means that at most try to minimize the energy they expend in foraging process and maximizing the nutrition benefits. For example, There is kind of birds that eats shell fish. So it dive into the water to catch the shell fish and carry it into the air so as to drop it to let it open. And this birds select the biggest birds available because this action can make sure they get the biggest possible meal. And besides, the birds also carry the fish to a certain height and then drop it, because if it goes any lower, it may have to drop it again and any higher altitude seem unnecessary to them and just the waste of energy. Therefore, the bird is a good example to explain the optimal foraging. 【TPO 47】Reactance While, the reading passage is talking about reactance. You know, it means that individual desire to reestablish the freedom and to control the situation. For example, like a kids, he likes to play at a playground, but his parent won’t want him to play in there anymore. But still, the kids will snick to play despite the rules. And a second example is about banning a certain kind of soap. People are really upset about this decision because they though they should be able to buy whatever they want. So, before the actual ban, a lot of people went to store and bought a lot of soap. So, these are two examples to explain the reactance. 【TPO 46】Warning Correlation While, the reading passage is talking about warning correlation. You know, it means that animals, they have distinct coloring that signal predators of the defense mechanism they have. **For example, the skunk. **They have a black body, they also have a white stripe running from their head to the tail. And skunk, they can produce horrible smell liquid to protect themselves. So for example, when a wolf approaches to a skunk, it will just spread the liquid to the wolf. And the wolf would just back off because of that terrible smelling liquid. So, in the future, if the wolf see a white stripe again, it will force the wolf to recall the terrible smell and it would stay far away. Therefore, this is a good example to illustrate warning correlation. 【TPO 45】Method of loci While, the reading passage talks about method of loci. You know, it means a technique to remember several pieces of information in a particular order. For example, someone has to remember the sequence of planet in the order of their distance from the sun. So this person can imagine the walk from dormitory to the students center. So, the first land mark is the door and the second land mark is the beautiful tree, and a statue. So to remember the name of planets, this person can assign one planet to each land mark. Like mercury with the door and Venus with the tree and so on. So, in the test, when he needs to recall this information, he can just imagine the walk from the dorm to the student center. 【TPO 44】Scope Creep While, the reading passage talks about scope creep. You know, it means that, as the project progresses, clients may ask for more than the business originally expected to provide. For example, the professor has a construction company. The company was hired to build a wooden fence for a lady. They made a verbal agreement and they didn’t put anything in writing. Later, when the job was almost done, the lady said she wanted the fence to be painted white. But the professor’s friends thought he was hired to build the fence, not to paint the fence in addition. But eventually, he just agreed to paint but he was not happy about it. Therefore, this is a good example to illustrate scope creep. 【TPO 43】Population Changes. While, there are two types of factors that can cause population changes. The first one is biotic factors, which are living factors that can influence the size of population. Like mice and awl, since awls eat mice. So, the number of mice depend on the number of awl. And one year, awls are more than usual, so the number of mice drops. And the other fact is called abiotic factor, which are the non-living things in the environment. Like rabbits. They often start to have their young in the end of the winter. But one year, the winter season are really short, so the rabbits can start to reproduce much earlier. As a result, the number of rabbits increase. Therefore, these are the two factors that can cause population changes. TASK 4【TPO 1】Baby’s arithmetic ability While, in this lecture, the professor talks about that the babies are actually have the ability of mathematics. To be more specific, he believe that the baby can add. For example the professor introduce a experiment. A baby was shown a doll, and then the researchers are lower the screen to hide the doll. Next the researcher took another doll and act obviously behind the screen to make sure the baby saw that, but after this the researchers secretly took away one doll and raise up the screen. And baby was surprised by just seen one doll behind the screen which should be two. This is why professor believe that the baby’s can add. 【TPO 2】Two definitions of money While, the professor talks about two definitions of money. The first one is a narrow one to definite money. Money could be considered as coins or bills. For example, the person 5 dollars to taxis driver in order to purchase a ride, and the the taxis driver may give the farmer 5 dollars to purchase the vegetables. So, this is the narrow definition of money. Besides, there is a broad definition of money, which is that the good or services are use to exchange for other goods or services. For example, the taxis driver may use a drive to exchange for the vegetables from a farmer. Therefore, there are two definitions of money. 【TPO 3】Two strategy of advertising While, the professor talks about two strategies of advertising. The first one is repetition. For example, in a car commercial prefer to the roomy cars, this car just keep picking up people, and every time when someone gets on, a person said plenty room for friends, or plenty room for family. So, by being repeating that, the people will be convinced that the car is spacious. Besides, another strategy is to use celebrities. Like, a celebrities in a car said that I like my car being fast. Then, in this case, the people will believe that the car is impressive for its speed. Therefore, these are strategies for advertising. 【TPO 54】Two factors of Biological cycle While, the professor talks about two factors about animal’s biological cycle. The first one is the internal clock. For example, the flying squeals, they flying during the night and sleep during the night. But if they are not exposed to the sunlight, they can still maintain the activity pattern. This is because their internal clock. Besides, another factors is external cue. Like, after this constant darkness, the flying squeals are exposed to daylight. At first, their schedule can not match up, they will wake up at the mid night. But after a while, the external cue and help them to adjust the internal clock until they go back to normal activity cycle. 【TPO 53】Food Spoilage While, the professor talks about two ways to prevent food spoilage and why they are effective. The first way is to control the temperature Like the fresh fish, if it is left out under the sunlight, it would spoiled in hours, but if we freeze it, it could be kept for month. It is because the low temperature can slow down the bacteria growth. Besides, another way is to control the moisture. Like the regular milk, it spoils really quickly, but milk in powder form can last for years. This is because, without moisture and water condition, the bacteria can not grow. So, these are two ways to prevent food from spoilage and why they are effective. 【TPO 49】Living in Groups While, the professor talks about two disadvantages of living in groups. The first one is that, they are more visible to predators. Like sardines, they often swim in groups, so, the whales, well their predators, probably wouldn’t notice one sardine. But, whale could easily a group of sardines and eat them. Besides, another disadvantage has to do with caring for the young. For example, millions of bats lives in a cave, so a mother bat returns from finding food, she might feed the baby of another mother bat. In this case, her own baby couldn’t get fed. Therefore, these are two disadvantages of living in group for animals. 【TPO 48】Advertising While, the professor talks about two ways that advertising may negatively affect the environments. While, the first case is that, it could be wasting of natural resources. Like once, the advertising kitchen renovation service, it is all waste of papers and trees, because the whole book is irrelevant to the professor. Besides, you know, sometimes the advertising will negatively affect the natural beauty. For example, you can see some big ads on the side of the road when passing through a mountain. In this case, people can not just appreciate the beauty of the environment because the big ads get in the way. So, these are two ways that advertising can negatively affect the environment. 【TPO 47】Domestication While, the professor talks about two benefits of animal domestication for early humans. The first one is providing human with consistent meat. Like goats were domesticated, and they are able to move from place to place. So, when early human need meat. They can provide consistent and reliable meats. The another one is that they can supply a variety of foods other than meat. Like domesticated goats could produce milk, so early human can collect it and drink it. They can also process the milk into cheese or yogurt, which could be stored. So, these are two benefits of animal domestication for early human. 【TPO 46】Easy to remember While, the professor talks about two explanation why we may remember some things better than others. The first case is we remember better if we have previous knowledge. Let’s say someone goes to a classic concert, you know, if the person knows the classic music a lot, it is much easier for one to recall the details of the concert. Besides, another thing is that we remember better if there is something special. For example, in a class with more than one hundred students. People can remember someone who is tall or someone who is intelligent. Because this features make them easy to be remembered. Therefore, these are two explanations for why we remember something better. 【TPO 45】Producing electricity for fishes While, the professor talks about how producing electricity benefit the fish. The first benefit is that this can help the fish to capture the prey. For example, the electric eel, it can produce a strong electric current. So, when it finds the food, it would just shock the fish and make sure it can get the food. Besides, another benefit is that producing electricity can help the fish to navigate their environment. Like, the knife fish, it creates electrical field around it. So, when it swim around a rock, it would sense the interference, so it can swim around the rock and avoid crashing into it. So, these are the two benefits of producing electricity for fishes. 【TPO 44】Benefits of Forest Fire While, the professor talks about two benefits of forest fires for animals. The first one is that forest fires could make it easier for predator to find food. Like wild turkeys, they usually just wait at the edge of forest fire. They just wait for insects escaping from it. So, in this case, they can easily gather a lot of foods. Besides, another benefits is that forest fires can make the forest a good place for animal developments. For example, like some trees, their are poisonous for some beetles. But after the fire, the trees are died, so the beetles can easily and safely lay eggs, and young beetles can use the tree as nourishment. So, these are the two benefits of forest fires for animals. 【TPO 43】Animals provide themselves with food. While, the professor mainly talks about two different ways in which animals provide themselves with food. The first one is that animals may cultivate plants. Like danzo fish, they likes to eat a kind of sea weed. So, basically, they will provide protection to sea weeds. If they see some plants block the sunlight from the sea weeds, they would just bite them off. In this case, the sea weeds could grow and danzo fish can have a lot to eat. Besides, another one is that animals may take care of other animals. Like black ants, they take care of aphids, because aphids can provide sweet liquids that they eat. So, ants will guard them, and even raise their young. So, in this case, ant could make aphids as a reliable resources of foods.","link":"/Blog/2019/08/31/Study-Notes-of-TOEFL-Speaking-Part-Task-2-4/"},{"title":"Study Notes of TOEFL Speaking Part -- Task 1","text":"Some people believe that television has had a positive influence on society. Others believe it has had a negative influence on society. Some college students choose to take courses in a variety of subject areas in order to get a broad education. Others choose to focus on a single subject area in order to have a deeper understanding of that area. etc [toc] TASK 1【TPO 1】Some people believe that television has had a positive influence on society. Others believe it has had a negative influence on society.Which do you agree with and why? Use details and examples to explain your opinion. Honestly, I think television has a positive influence on society. First of all, it can help people to relax. I mean people can watch some comedies or other programs that can make them laugh and forget about the troubles and stress. And also TV is important because it can help people to learn new skills. You know nowadays many people learn how to cook on TV, they learn how to speak English on TV. So, I think television has a positive influence on society. 【TPO 2】Some college students choose to take courses in a variety of subject areas in order to get a broad education. Others choose to focus on a single subject area in order to have a deeper understanding of that area.Which approach to course selection do you think is better for students and why? Well, I would like to take courses that are from a variety of subjects areas, because this can help me to make more friends. You know, I can meet people from different fields and get to know them, so this can help me to build a large network. And besides, I like to take courses from different subject areas because many subjects fascinate me a lot. I mean sometimes I am just fascinated by math courses, and sometimes I get to learn about the history, and then I get to learn about something about arts. So this also can help me to switch my mind and to take a break. 【TPO 3】Some students prefer to work on class assignments by themselves. Others believe it is better to work in a group.Which do you prefer? Explain why. Honestly, I would like to work on class assignments in the group. First of all, I think this can take less time, because in a group project we can always share that tasks, and everyone just needs to complete a part of it. In this case. we can finish the whole thing much faster. Besides, I would like to do group activity,I mean group projects, because it’s a great chance for me to learn from I others. For example, I can learn how others solve a problem, and in this case I can improve myself. Therefore I would like to do assignments in the group. 【TPO 4】Many universities now offer academic courses over the Internet. However, some people still prefer learning in traditional classrooms.Which do you think is better? Explain why. Well, I think studying on-line is actually better. because, personally, I can study better on the Internet. This is because on-line lessons could be replaced for as many times as I want to. So, I can watch it over and over again until I understand all the key points. Besides, I would like to study on-line, because it really saves time. I mean with on-line courses, I can just stay at home. So I don’t need to spend any time traveling to the school. And in this way, I can just spend that time to do something that I like. Therefore, I would like to study on-line. 【TPO 5】It is more important to study math or science than it is to study art or literature?Do you agree or disagree with the following statement, why or why not? Use details and examples to explain your answer. While, I personally think that learning art and literature is, actually more important. because, when learning arts we get to study music, and it can help us to relax. I mean, whenever we feel tired, we can just play a beautiful piece of melody, which can help us to empty our mind. Beside, the reason why I think study literature is important is that through study these subject, we can actually learn some foreign culture. You know, we can understand some foreign customs, foreign festivals, while, it also can help us to make friends with foreigners. 【TPO 6】Some people have one career throughout their lives. Other people do different kinds of work at different points in their lives.Which do you think is better? Explain why. Well, I think it is better to do different kinds of works at different points of life. This is because, it allows me to learn a lot of skills. For example, if I do a customer service job, I can learn how to communicate with people and how to solve problems efficiently. And if I do, lets say, accountant job, I can just learn how to deal with numbers. So this can help me a lot. And also, doing different jobs helps me to make friends. Because I can meet different people at different work, and we can become close with each other. Therefore, I would like to do different jobs in life. 【TPO 7】 Learning through online courses is more effective than learning in the traditional classroom setting.State whether you agree or disagree with the following statement. Then explain your reasons, using specific details in your explanation. While, I believe learning through online courses is more effective. Because, when taking those online courses I could study whenever I feel best. You know, I am a morning person, so, I can just take the online courses in the early morning, like 6 o’clock or 7 o’clock. And by doing so, I will really sharp and can concentrate better. So this can help me to understand all the key points much better. And, online courses could be repeated, so I can watch it again and again until I remember all the detail and key points. So, I think taking online courses in more effective. 【TPO 8】Some people enjoy taking risks and trying new things，others are not adventurous: they are cautious and prefer to avoid danger. Which behavior do you think is better? While, I would like to take risks all the time. Because, sometimes taking risks can help me to gain more. For example, some people would just like to invest in those risky stock, and if they succeed, they could earn millions of dollars. And beside, I would like to take risks because it can help me to improve my skills in risk assessment. I can help me learn how to manager it, and how to take advantages of that. So, that why I want to be a adventurer and to take risks. 【TPO 9】Some people think that family members are the most important influence on your adults. Others believe that friends are the most important influence. Which do you agree with? While, I believe that friends are the most important influence. Because, we spent more time with the friend than adults like parents. You know, we go to the same school, which means we spend at least 8 hours a day together, and at least 5 days even 6 days in a week. So, friend have a lot of chances to influence us. you know, they can change our thoughts or even behaviors. And beside, we do not have generation gaps with each other. And we tend to believe that they can fully understand us and know our circumstances. That’s why we are more likely to accept their idea and suggestions. As a result, we receive more influence from friends. 【TPO 10】All children should be required to learn a second language in school.Do you agree or disagree with the following statement? Use details and examples to explain your answer. While, I believe all children should be required to learn a second language in school. This is because, being able to speak a second language is such a important skill. Because, nowadays, it is so common to make business with foreigners. So by being able to speak a second language, you know, those students are able to find better jobs in the futures, and they are able to work in the foreign company and make more money. And also, learning a second language could be really fun. Because one can learn about the foreign culture, such as foreign food and foreign festivals such as Halloween and so one. So, students should learn a second language at school. 【TPO 11】Some people think that children should be allowed to watch whatever television programs they choose to. Others think that parents should exercise control over the television programs their children watch.Which do you agree with? Explain why. While, I believe the parents should exercise control over the television programs their children watch. This is because, sometimes the TV could be addictive. You know, some kids just can not stop watching TV, so, by limiting the programs children can watch, the parents can make sure their kids have enough time to study, or enough time to exercise, or sleep. Besides, the parents should control the programs that their kids watch because, there are some programs that are not appropriate for the teenagers to watch, such as their a lot of screens that contain blood or violence which could really bring bad influences. 【TPO 12】Some people believe it’s essential for a person’s education to learn to play a musical instrument. Others don’t believe music education is important.Which view do you agree with? Explain why. While, I believe it is essential for a person to learn how to play a musical instrument, because, after learning this skill, people can just better relax themselves. I mean, when they feel tired or stressed, the can play the musical instruments; They can play a beautiful melody which take their mind away from their troubles. Besides, I think this is an important skill because it can easily impress others. On a party or a social gethering, a person who can play the piano can always win people’s respect. So, it’s important for people to learn how to play the musical instruments. 【TPO 13】When looking for information for a research project, some students prefer to get their information mainly from the Internet. Others prefer to mainly use printed materials such as books and academic journals.Which do you prefer, and why? While, I would like to look for information on the Internet. Because in this case, I can just complete my research paper much better. I mean, on the Internet, I can access information from everywhere around the world. You know, I can read the latest research paper from america; or even read about the latest, the newest experiments in Europe. So this can help me to get enough information, and make my paper perfect. And besides, it is also convinient to look for information online. I just need to type the keywords, then the results will show up. 【TPO 14】 One of the best ways to learn is by making mistakes. Use specific examples and details to support your opinion.Do you agree or disagree with the following statement? While, I think the best way to learn is just by making mistakes. This is because the mistakes can leaves us a deep impression, which can help us to remember the lesson learnt from it for quite a long time. You know, when I was a young kid, I used to be very selfish, and I as really rude to my friend. Then I realized that I disappointed them a lot and finally lose a lot of friends. So even nowadays, I can still remember the lesson learnt from it. I always remind myself to be considerate, to be nice to others. Therefore, I believe the best way to learn is by making mistakes. 【TPO 15】It is important to remember and learn from the past. Use details and examples to explain your opinion.Do you agree or disagree with the following statement? While, I think it is important to learn from the past. Because, it can lead to future success. You know, when thinking about the experience that I did well in a test, you know, I just realise that taking good notes is the key. So by repeating that it can help me to do well in the future tests. Also, learning from the past can help me to avoid my mistakes. You know, I used to be rude to my friends, which disappointed them a lot. So by learning from that experience, now I am super nice to them. Therefore, it is important to learn from the past. TIPS: Everything from the past is useful whether from the pragmatic view or the idealisticview. Becuase, if it was good, it is a memory. If it was bad, it’s an experience. 【TPO 16】Some people who unexpectedly receive a large amount of money spend it on practical things, while others spend it for pleasure only.Which do you think is better and why? While, I think people should spend money for pleasure only. Because, it can bring us a lot of fun. For example, they can use the money to pay for a trip. They can travel to different countries, and they can see a lot of beautiful views, such as picturesque shores beside the river, the spectacle of the aurora in Iceland. Besides, they can learn some local culture and customs, such as different festivals. In this cases, they will wnjoy their lifes, so they should spend their money for pleasure only. 【TPO 17】 Students should not be allowed to bring cell phones (mobile phones) into the classroom.State whether you agree or disagree with the following statement. Then explain your reasons, using specific details in your explanation. While, I think students should bring cell phones into the class. I mean, cell phones can help them to learn better. First of all, they can use cell phones to do research. For example, if I meet a term that I don’t understand or do not clearly know the definition. I can use the cell phones to look them up online, so I this can help me to better understand the professor’s lecture. Besides, students can use their cell phones to record the lecture. I mean, they can record the whole thing, so they can listen to it again and again after class to make sure they understand everthing. Therefore, students should bring cell phones into the class. 【TPO 18】 It is important to learn about other cultures. Use details and examples to explain your opinion.Do you agree or disagree with the following statement? While, it is quite important to learn about other cultures. Becuase, it can really help peole to communicate with others from different countries. Because, you know, nowadays, it is quite common to do business with other foreigners, so by learning their cultures, it could help us to better respect them, respect their customs and traditions. As a result, you know, we can better communicate with them. And also, learn other cultures could give us a chance to better travel the worlds. We can travel to different countries, and they can see a lot of beautiful views, such as picturesque shores beside the river, the spectacle of the aurora in Iceland. So, that could be fasinating too. 【TPO 19】More and more people are buying items on the Internet and from magazines or catalogs. Other people prefer shopping in a store.Which do you prefer and why? While, I prefer to shopping online. Because, it help me to save money. You know, the stuff selled online are much cheaper than those in the stores. This is because, I guess, they don’t have to pay for rent. So, that why they can offer lower prices. And also, I like to shop online because it’s quite convenient. You know, I don’t need to leave my house, I don’t need to go to the stores which may take hours. So, I can easily just shop in my bedroom and living room which, you know, makes my life so much easier. Therefore, I would like to shop online. 【TPO 20】Some people prefer to learn about current events from watching television news programs. Others prefer to read about current events in newspapers or on the Internet. Which do you think is better: watching the news or reading the news?Explain why. While, I would like to read online or on the newspaper. Because, it really saves time. You know, when I read, I can just read the story that interest me most and just skip the boring parts or the stories that I am not interest in. But if I choose to watch TV, I am forced to learn every single one of them because I can not skip and I can not to pay more attention on one of them. This is why I choose to read those events online or in newspaper, because I can skip and I can decide what to read first. 【TPO 21】 Your friends are the most important influence in your life. Use details and examples to explain your opinion.Do you agree or disagree with the following statement While, I totally agree that our friends are the most important influence in our lifes. This is because we spend a lot of time together, you know, we study with each other in school, sometimes, we hang out with each other during the evening, even the weekend. I mean, by spending so much time with each other, they can easily influence me during the process. And beside, my friends influence me the most. Because, they really understand me. You know, they can give me a lot of good advises, and I will most likely to accept them, since we don’t have the generation gap. Therefore, friends influence me most. 【TPO 22】Children should be required to learn practical skills in school, such as cooking or personal finance, in addition to academic subjects.Do you agree or disagree with the following statement, Use details and examples to explain your opinion. While, I believe that children should be required to learn practical skills in school, such as cooking or driving. This is because, these classes are so fun. Like, in the cooking class, students can learn to use tea to complements a wider variety of foods such as snacks or sweets. And in driving lessons, you know, driving itself is kind of enjoyment for some people. So, these experiment can make their school life more enjoyable. Beside, learning these skills can also be very useful, for example, if you know how to cook, you can better take care of yourself. Therefore, children should be required to learning this skills. 【TPO 23】Some people enjoy spending their free time alone in activities such as reading, thinking, or writing. Others enjoy spending their free time in shared activities with other people.Which do you prefer and why? While, I would like to spend my free time with others. Because, this can help me to make more friend, for example, when participating in a group activity, like basketball, you know, I can always meet some new teammates. And by playing with each other, you know, I can easily get to know them. Besides, participating in the group activity, I think it’s just more fun. Again, let’s take basketball as example, you know, the sense of competition just makes the whole game more exciting. Everyone is competing to win. Therefore, I would like to participate in activities. 【TPO 24】Students benefit more from classes with a large number of students than they do from smaller classes.Do you agree or disagree with the following statement? Use specific examples and details to support your opinion. While, I believe the larger classes are more beneficial. This is because, students can make more friends in a larger class. Because they can meet a large number of classmates, and they can get to know each other during the lessons, or even after the class. So, quickly, they can become friends together. And besides, when studying in a large class, students can always get help easily. If they have a question or a problem, they can easily ask the students who sit around them. So, they have a larger chance to get the right answer. Therefore, I think students benefit more from larger classes. 【TPO 25】Some people do not enjoy shopping and shop only when they have a specific purchase to make. Others like to go shopping for pleasure whether or not they have something to buy.Which do you prefer and why? While, I prefer to go shopping only when I have something to bug. Because, I think this can help me to save time. You know, I live in the suburb, so it would take me at least two or three hours on the road. By going to shop when I have to, I can avoid some meaningless shopping trips. So, I can save the time and do something I like. Such as reading books or watching movies, to name a few. Therefore, I prefer to go shopping when I really have something to buy. 【TPO 26】Some people like to have their cell or mobile phone with them at all times. Other people prefer not to bring their cell or mobile phone with them everywhere they go, or they choose not to own one at all.Which do you prefer? Explain why. Well, I would like to have my cell phone all the time. Because I can use the cell phone to keep in touch with all my friends. You know, when I encounter some emergencies, I can easily tell them to help me. Moreover, I can touch them whenever I miss them, and chat with them and catch up. But I have only one occasion that I like to turn the cell phone off is Watching Movies. When I go to the cinema, I usually turn the cell phone off, because I really don’t want to be interrupt during my movie time. 【TPO 27】Parents should be in the process of helping their children to choose a university.Do you agree or disagree the following statement? Use specific details and reasons in your response. While, I don’t think parents should help their kids to choose the university. Because, parents usually don’t know what are their children really want. You know, they and their kids always are aiming to different things. Maybe their kids focus on the major that they like, but the parents will think about something different, such as their career, their jobs or their opportunities, even including their salaries, staff like that. That why, if their parents help them to choose the university, I think they may end up arguing. They may argue about what, you know, what to study, or where to go. So, because of this, it is better that parents just stay out. 【TPO 28】To protect the health of young children, advertisements for candy and junk food should not be shown on television.Do you agree or disagree with the following statement? Use specific reasons and details in your response. While, I believe that the advertisements for candy or junk food should not be shown on the television. This is because, young people, their are quite impulsive and sometimes can not recognize what is good or what is bad for them. So, if there are a lot of advertisements for French fries, or hamburgers, they are more likely to be influenced, you know, their might eat this kind of food everyday complement with coca cola. That is really bad for their health. Not to mention that, junk foods are full of fat which may lead to weight problems or even heart problems. So, that’s why they should not be shown. 【TPO 29】Some people like to study in public places where there are other people around. Others prefer to study in places where there are few or no people around. Which kind of place do you prefer? Explain why. While, I would like to study in a quite place, you know, just by myself. This is because it is actually really quite and I can better focus myself. You know, I don’t have to worry about like others distracting me at all. So, in this case, I can just cope with my work efficiently and finish my work very quickly. And besides, when I study in somewhere just by myself such as home, I can be very comfortable. I can dress comfortably and do whatever I want. I can even eat when I am studying. So, in this case, I just prefer to be alone to study. 【TPO 30】 Parents should be involved in the process of helping their children to choose a career.Do you agree or disagree with the following statement? Use specific examples and details to support your opinion. While, I don‘t think parents should be involves in helping their kids to choose a career. First of all, I don’t think parents really understand their kids. They don’t know what their kids really like and what their kids want. So, if they really get involved, they wouldn’t do any good, instead they maybe end up arguing with their kids. You know, they have different ideas, different opinions and different focus. And such arguing will take up long time. So, I believe, parents, if they get involved, that will be just the waste of time. 【TPO 31】Parents should be involved in the process of helping their children to choose a career.Do you agree or disagree with the following statement? Use specific examples and details to support your opinion. While, I don’t think parents should be involved in the process of helping their children to choose a career. First of all, I don’t think parents really understand their kids, their don’t know what their kids really like, or what their kids what. So if their really get involved, they wouldn’t do any good. Instead, they may end up arguing with their kids, you know, since they have different ideas, or different opinions. And this kind of arguing my take up long time. So, I believe, parents, if they get involved, it will be just the waste of time. 【TPO 32】Some university students choose to take difficult classes even if they know they might not get a good grade in the class. Other students prefer to take easier classes in which they know they will get a good grade.Which do you prefer? Explain why. Honestly, I rather take the easier classes. Because, when taking this kind of easier classes, I just don’t need to spend all day long studying. You know, I can only sepend a few hours everyday and I can just finish all my assignments. In this cases, I will have the time to do the things that I like. Such as hanging out with parents or friends, or just do a part time job to make some money. And besides, I like this easier classes because I want to get a better grade. You know, that will make me feel good and feel more confident if I do have a better grade. And not to mention, It will help me to find better job in the future. Therefore, I would like to take easier classes. 【TPO 33】Do you think that eating healthy food is easier or more difficult today than it was 40 or 50 years ago?Use examples and details to support your answer. While, I believe that it was much easier to eat healthy food in the past. Because, the environment is better, you know, there was no pollution, so the water was clean, the air was clear. So the food we ate was generally much healthier. And besides, in the past, people have more time to cook at home. So, the meal they had was healthy because they could make the dinner complement with vegetables. But nowadays, since people are busy, they just rely heavily on fast food which is not healthy at all. So, that why I believe it’s much easier to eat healthy food in the past. 【TPO 34】Private car should not be allowed in the city centers of large cities.Do you agree or disagree with the following statement? Use details and examples to explain your opinion. While, I believe that the private car should not be allowed in the city centers of larges cities. Because, nowadays, there are too many cars in the downtown area. You know, the traffic condition is really bad in the rush hours and no one can move in the road. You know it took people hours to get to the destination. But, if the private car are not allowed, and people work in the city center could use the public transit, so, without a lot of private cars, the bus can move freely and easily, so it’s a win-win strategy because everyone can arrive their destination much faster. 【TPO 35】A university’s academic reputation is the most important thing a student should consider in deciding which university to attend.Do you agree or disagree with the following statement. Use examples and details to support your opinion. While, I believe that the academic reputation is the most important factor for students to choose a university. Because, if one can choose a university with good academic reputation, that means, he can learn quite a lot in there, this is because, this university often have a lot of famous professors, and also have really practical curriculum. So by going to or choosing such a university, this can just make sure students can learn everything that they need to know. And guarantee them a bright future. So, that’s why I believe that academic reputation is actually the most important factor to consider. 【TPO 36】Some people buy food that is already prepared. Other people buy fresh food and prepare meals themselves.Which do you prefer? Explain why, using details and examples in your answer. While, I would like to buy fresh food and prepare my own meals. Because, it’s just much cheaper, you know, the vegetables and raw meat cost only half as much as the already prepared meals. So by doing this, I can save a fortune every month. And besides, I like to do so, because, I just enjoy cooking, you know, I would like to experiment different recipes, complement various foods different vegetables. So, in this case, by purchasing the fresh food and preparing my own meals at home, it just gives me the joy that I am looking for. 【TPO 37】Students will learn more if the teacher is kind and friendly.Do you agree or disagree with the following statement? Use specific examples and details to support opinion. While, I believe that the students will learn more if the teachers is kind and friendly. Because, in this case, students will feel relax, you know they can feel they are ease, so during the class, they are more likely to focus on the content in the lesson, and I am sure, they can learn better. And besides, when teacher is kind and friendly, students will become friends with them, we always listen to our friend right. So, in this cases, their students are more likely to accept the teachers ideas, opinions, even the criticisms. Therefore, I believe that the students will learn more if the teacher is kind and friendly. 【TPO 38】It is more enjoyable for students to read works of fiction (such as novels and stories) than nonfiction.Do you agree or disagree with the following statement? Use specific examples and details to support your opinion. While, I believe it is more enjoyable to read fiction such as novels and stories. Because, the fictions will often have really interesting characters. For example, in some fictions, the antagonist maybe a monster and the protagonist maybe a hero. This kind of fictions or stories fascinate me a lot. So, you know it just so much more exciting than reading some nonfictions. And besides, reading some fictions is more enjoyable, because, fictions could take place in the magic world where everyone has magic power, so, it’s just more fun than reading a dull nonfiction such as documentary or biography. 【TPO 39】If you were given the choice of a school or work assignment, would you prefer to write a long report or give a speech in front of a large group of people?Use details and examples to explain your choice. While, I would like to give a speech before the people. Simply because that I am good at it. You know, I would like to give presentations. By giving a speech, I am sure that I can done this work perfectly. And I am sure that I can get a better grade. And besides, as for me, a speech just take less time to prepare, you know, if I need to write a long report, I have to do research for weeks, but if I need to give a speech, you know, I only need to practice or rehearse a couple of times. So, in this case, it just take less time for me to prepare. Therefore, I prefer to give a speech. 【TPO 40】Some people think that materials printed on paper, such as books and newspapers, will one day be replaced by electronic versions of those materials. Others believe that printed materials will always be popular.Which point of view do you agree with? Explain why. I believe that one day the paper material will be replaced by the electronic versions. This is because, e-books are convenient. You know, nowadays, many people enjoy reading on their cell phones, and that is much easier to carry their cell phone around than carry heavy paper books. And also, e-books are cheaper, because they usually cost only half as much as the paper materials. So, reading electronic versions, people can save a lot of money. Therefore, I believe the electronic books will be more popular in the future. 【TPO 41】Some people believe it is important for university to provide funding for student entertainment, such as movies or concerts on campus. Others believe that university money should only be used for academic purposes.Which view do you agree with? Explain why. While, I believe that university should spend money on students’ entertainment. Because, this can help students to relax themselves, you know, they can have l lot of fun in concerts, and, they can totally enjoy the stories while watching movies. So, they can easily clear the negative emotions ( So, the negative emotions will quickly be dissolved). And also, by spending money on student entertainment, it can help students to make friends with others, you know, they can meet new people during the concert, and get to know each other afterwards. So, in this case, you can have a lot of fun. Therefore, I think it’s better for university to spend money on students’ entertainment. 【TPO 42】Some people prefer living in a big city. Other people prefer living in the countryside, away from urban areas.Which do you think is better? Explain why, using specific details in your explanation. While, I believe that living in a big city is better. Because, it’s so much more fun. You know, in big cities, there are a lot of entertainment places to go to, such as the movie theaters, and various parks. So, in this case, I can hang out with my friends. And besides, in big cities, I can get better jobs. Because, there are a lot of big companies here, I can work for them. So, in this case, I can get a better salary and enjoy a comfortable life. Therefore, I would like to live in a big city. 【TPO 43】Some students attend college full-time, while others attend college part-time.Which do you think is better? Explain why. While, I think it would be better to attend college full-time. Because, students can learn more in this case. They can spend all their time taking different courses, or learning different subject. So, I’m sure they will become knowledgeable, and they can find job easily in the future. And also, attending college full-time can help them to make more friends. You know, they can spend a lot of time together with their classmates. So, this can help them to get to know each other. So, I’m sure this can have a lot of fun. Therefore, it’s better to attend college full-time. 【TPO 44】Some people believe that primary schools should no longer teach children how to write by hand, and instead should spend time teaching them how to type on a computer. Other people believe that it is still important for schools to teach children to have good handwriting.Which point of view do you agree with? Explain why. While, I think primary schools should teach children how to type. Because, typing is much faster than writing by hands. I think people can do that at least two times faster. So in this case, you know, students can be more efficient when they type. And also, I think typing is more convenient. Because, nowadays, when people type a paper on the computer, they can use the spell check to correct their mistakes. So, this can make sure they can write a good article. Therefore, school should only teach the students how to type. 【TPO 45】Artists and musicians are important to a society.Use details and examples to explain your answer. While, I do believe that artists and musicians are important to a society. Because, they can help people to relax, I mean, if they feel frustrated, they can just listen to music, and the negative emotions will be quickly dissolved, so they can forget about their troubles. And also, artists pay lots of tax every year. You know, they need to pay hundreds of thousands of dollars, so the government can use the money to build the infrastructures such as schools and hospitals. Therefore, I think artists and musicians are important to the society. 【TPO 46】People today have healthier lifestyles than people did 100 years ago.Do you agree or disagree with the following statement? Use specific examples and details to support your opinion. While, I do believe that people in the past had healthier life. Because, they had healthier food. You know, everything they ate was organic. So, it was really nutritious, and much healthier than foods today. They don’t have any junk food like hamburgers or French fries. And also, people did much more exercise in the past. You know, they really didn’t have cars and buses, so they had a lot of chance of walk, and walk everyday can hugely reduce the risk of suffering cardiac disease in the future. Therefore, people in the past had more healthier lifestyles. 【TPO 47】In the future, people will read fewer books than they do today.Do you agree or disagree with the following statement? Use specific examples and details to support your opinion. While, I believe that people will still read a lot of books in the future. Because, reading become more convenient than before. I mean, people nowadays can read on their cell phones, or lap tops. So, I’m sure that reading will become a very popular activity because of that. And also, I think reading itself is so fun. I mean, when people read, they can be in a imaginary world, like the future utopia society, or past of Stone Age, even the Harry Potter world. So, in this case, they can forget about their troubles in the real world. So, people will still read because this experience can not be replaced by any other materials whenever in the future or at present. 【TPO 48】Some people like to shop in large grocery stores and department stores. Other people prefer to shop in small specialty stores.Which do you prefer? Explain why. While, I would like to shop in large places. Because, I can save money in this way. You know, this large grocery store often have better deals and sometimes decide to sell of part of their goods. So, they can offer better price for me. So, I can save the money and use the money to do something I like such as watching movies or buying books. And besides, I would like to shop in large places, because it’s convenient. You know, it has almost everything I need. So, I can get all the stuff in my shopping list in one place. So, it can really save a lot of time. Therefore, I like to shop in large places. 【TPO 49】Some teachers think that it is important for students to sit in assigned seats, that is, to sit in the same place every day in class. Other teachers think that students should be allowed to choose where they will sit, and they allow them to sit in different seats on different days.Which do you think is better? Explain why. While, I do believe that students should be allowed to choose their own seat. I mean, they can sit in different seats on different days. Because, this can help them to make more friends. Because, by sitting in different seats in classroom, he can just get to know different people, and in this case, he can be friends with them and they can have a lot of fun together. And beside, by sitting in different places, he may have to work with different person on class team projects. In this way, he can improve his communication skills and cultivate the sense of confidence. Therefore, student should be able to choose the seat. 【TPO 50】Some people like having a wide variety of friends and acquaintances they can spend time with. Others like to spend most of their free time with the same small group of close friends.Which do you prefer? Explain why. While, I prefer to have a wide variety of friends. Because, This is really exciting. I mean, when I hang out with different group of people, I get to do different things. Like, if I hang out with friends who like movies, we can talk about movies all day. And when I meeting the friends who are interested in books, I will eager to share my point of view such as the characteristic of the protagonist in The Great Gatsby. So, by having a great variety of friends can really bring joy to my life. And also, I get to learn something new from different people all the time too. Therefore, these are the reason why I would like to have a wide variety of friends. 【TPO 51】Eighteen-year-olds are not mature enough to vote.Do you agree or disagree with the following statement? Use specific examples and details to support your opinion. While, I don’t think eighteen-year-olds are mature enough to vote. Because, I think they are easily influenced by their parents. I mean, most of them still live at home, and they may listen to what their parents says. As the result, their vote might present their parents’ preferences, not their own. And also, eighteen-year-olds are high school students in my country, facing the graduation examination, they are very busy. They probably have to study over 8 to 10 hours a day, so, I don’t think they have enough time to be familiar with the politics. So, because of these, they really couldn’t cast a vote. Therefore, because of there two reasons, I think they are not mature enough to vote. 【TPO 52】People are more likely to enjoy themselves at concerts or films if they go with a group of friends.State whether you agree or disagree with the following statement. Then explain your reasons, using specific details in your explanation. While I do believe that people will enjoy themselves better at concerts or films if they have their friends with them. This is because they can totally spread the joy. Because one of the main purpose for me to go to concerts is to strengthen my contact with my friends, you know, when we all listen to a concert, we are all immersed in a harmonious atmosphere which is better for us to spread the joy. And besides, if I have friends around me after the movie, I will eager to share my point of view of the movie. So, that’s why I can better enjoy myself at concerts or films if I go with a bunch of my friends. 【TPO 53】When some people have a little extra money, they like to spend it right away on something they enjoy. Others prefer to save the extra money.Which do you like to do? Explain why. While, I am the kind of person who will spend the money right away. Because, this can make me feel happy. For example, if I buy a very nice outfit, I can receive a lot of compliments of my friends. You know, that will make me feel more proud and make me more confident. And besides, I would like to spend my money right away, because this kind of actions can really motivate me to earn more money. So, I will be motivated by my self to do more hours in my part-time job or just to take on some extra projects to earn the money. So, in this case, it motivate me to work hard. So, because of these two reasons, I would like to spend the money right away. 【TPO 54】Some people say that childhood is the best time in a person’s life. Other people disagree.What is your opinion? Explain why. While, actually, I do agree that childhood is the best time in one’s life. This is because, children have a lot of vacations, in my country, students have summer and winter vacations which last one and half long. So, usually, students can use the vacation to go on a trip, or they can do the things that they like. And besides, I think childhood is the best time, because children don’t have much heavy responsibility. I mean, they don’t have to look after their parents and certainly they don’t have to take care of their children. So, in this case, they have less stresses. Therefore, I think childhood is actually the best of in one’s life.","link":"/Blog/2019/08/20/Study-Notes-of-TOEFL-Speaking-Part-Task-1/"},{"title":"Study Notes of TOEFL Speaking Part -- Task 1 Outline of Real Test Questions","text":"Should people be allowed to take photos when visiting a museum? Do you agree or disagree with the following statement? Being polite is more important than being truthful. etc [toc] 真题你认为极限运动比如攀岩是勇敢的行为还是愚蠢的行为？What your opinion? and Why? and Why? and So what? and So what? 愚蠢 对我危险 因为我不配 做了可能受伤 受伤可能失去工作 勇敢 对专业的人ok 他们把自己逼到了极限 将不可能变为可能 While, honesty, if someone tries rock climbing, it could be a sign of bravery or it could be a sign of stupidity. While, for example, if I try rock climbing, it will be very stupid, because I am not professional athlete, so I’m not fit enough, so doing this rock climbing, I may fall and get hurt. While I just don’t think it is worth trying. And besides, if some professional athlete tries rock climbing, it could be a sign of bravery. While this is because they are pushing their bodies to the limit, and they are trying to make impossible possible. So, I think it’s quite inspiring. So, because of these reasons, rock climbing could be both sign of bravery and a sign of stupidity. 是否同意读消极的新闻比积极的新闻好？ While, I think it is better to receive positive news. This is because, my life is already depressing enough, you know, I work a lot, and I am under a lot of tress. So, receiving such positive news can easily cheer me up. It can make me believe that life is actually wonderful. While however, I think sometimes, we do need to hear about the negative news. Because, when hearing the negative news, it can stir up some dissatisfactions about the reality, which can push me to do some changes about my life. Base on this satisfaction, I can make some improvements. 你喜欢 experienced teachers 还是 new teachers？ While, I prefer to be taught by an experienced teacher. This is because, experienced teacher can teach better. Since they have been teacher so many years. So, he must know what the key points are. During the classes, he can better focus on the key points and provide a lot of example. So, students can better understand it. And besides, experienced teachers are better because they can better communicate with students, they can even become friends with students. So, I am sure students will love their classes and enjoy the school life. Therefore, experienced teacher are better. 在工作中，Luck 和 Hard working 是否同等重要？ While, honestly, I don’t think luck is as important as hard work for a person’s success. This is because, hard work is actually more important. You know, in one’s career, if he or she is hard working, this person can totally improve himself or herself. For example, in the business industry, if he works really hard, he can improve his problem solving skill. He can also learn how to communicate with clients effectively. And this can help a person to be more successful. Besides, if one is hard working, he or she can leave a great impression to the boss. So, in this case, he or she would likely to be promoted to a higher position. While, however, it might be true that luck can dramatically changes one’s life, but it is not wise to bet on luck. So, because of these reasons, I think hard work is more important. TASK 1 - 12.01【01】Should people be allowed to take photos when visiting a museum? While, to be honest, I think people should be allowed to take photos when visiting a museum. First of all, photos we took at the museum can help us to recall the vivid memory and momentous time. Like me, I love Vincent van Gogh so much that I took a photo with the painting Starry Night over the Rhone in the Orsay museum and many years later, I can still remember the time I spent in that museum, standing in front of this painting, marveling at his painting skills and being immersed in his fantasy world he drew on the canvas. Besides, if we do not use flash light when shooting the photos, we would not harm the exhibits at all and also we would not disturb other visitors at all. So, because of these reasons, I think we, visitors, should be allowed to take photos when visiting a museum. 【02】Do you agree or disagree with the following statement? Being polite is more important than being truthful. While, to be honest, I don’t agree that being polite is more important than being truthful. This is because, being polite is a communication skill, but being truthful is an essential characteristic or a merit everyone should possesses. And to some extent, being truthful is a crucial part of an high-quality communication or conversation. This is because, being truthful can help both sides of a conversation to create a congenial climate that is conducive to reach a consensus. Besides, being truthful can help one to earn (assist in earning) others’ respect. When you treat others in good faith, they will also be truthful to you and also respect you which have a beneficial effect on your social activities. So, because of these reasons, I turn to disagree that being polite should be give priority. 【03】Someone choose to work in a small company or organization with a few workers. Others prefer to work in a large company or organization with thousands of employees. Which do you think is better? While, I prefer to work in a small company or organization with a few workers. This is because, for an employee, working in a small company will have more chances to be promoted. It’s not hard to understand, in a small company, an ordinary employee is easier to reach a high level or a higher position in the company, compared with working in a large company. The reason is obvious, the fewer rivals or opponents you have, the higher likelihood you will have to be promoted. Besides, Working in a small company is beneficial for one to better make contribution to the company. The reason is apparent too, in a small company, one is easier to communicate to the executive level and express one’s own idea without any obstacle. On the contrary, it is extremely difficult to meet them by chances, not to mention making your point in front of them. Therefore, I prefer to work in a small company, rather than large company with a lot of employees. 【04】Which of following aspects do you think contributes most to country’s success: many business opportunities or a developed educational system? While, honestly, I think a developed educational system contributes most to country’s success. This is because, a developed educational system ensures talents and gifted person can be well trained before they step into the society. It is those person that well educated by a developed educational system contribute most to country’s success. Besides, compared with many business opportunities, a developed educational system can better promote the development of a country in a long term. Because a developed educational system can bring innovation and encourage entrepreneurship unceasingly and persistently. And innovation and entrepreneurship matter to country’s success. Therefore, I think a developed educational system contributes most to country’s success. 【05】Many people prefer to read books in electronic format on a computer screen or other devices. Some other people prefer to read books &gt;Which way do you prefer? Why? While, to be honest, I definitely will choose to read books rather than read in electronic format. This is because, reading in paper material is conducive for me to better dive in to the book, in other words, to be immersed in the contents. It is not hard to understand, if someone read on the cell phone, he or she would stand a fair chance to be interrupted because others will call in, or a massage is received. So, there are many other thing that will prevent us to enjoy the books. Besides, anther reason that let me choose the paper books is that electronic devices is detrimental to our physically health. Many youngsters suffer from myopic just because they spend a lot of time in their electronic devices, such as their mobile phones or computers. Therefore, this is why I love to read books in material, because I don’t need to worry about being disrupted by others as long as I mute my cell phone and put it far away from me. 【06】You live in a crowded city with only one green space-the city park. The government recently proposes to build a housing complex&gt;Do you think this is a good idea? While, I do not think it is a great idea to use this green space to build a housing complex. This is because, the city park is the only green space in the city, we should keep it green, rather to transform it into a housing complex. Since that is the case, many dwellers rely on it to relax and hang out with friends. As for me, I once lived in a crowd community, and the center garden is the only green place that I used to hang out with others, and a great amount of my precious memories happened in there. If the only green space was devastated by government, I would have no idea to place my childhood. Therefore, that’s why I don’t think it is a great idea to replace the green space with housing complex. 【07】Do you agree or disagree that to succeed we need to make enemies? While, I can not agree that to succeed we need to make enemies. Admittedly, it is true that we will encounter tons of enemies in order to succeed, but it does not mean that we need to make enemies, especially we are already have a lot of enemies. And besides, the best strategy to succeed is to make friends of an enemy. In other words, instead of making more enemies, the best way for us to be successful is to make friends with person that used to be our enemies. If that is the case, we will have more helping hands and fewer obstacles in our way to success. Which means if you can convert an powerful enemy into friends, that will make you ever stronger and even more closer to the success. Let’s look at the Los Angeles Lakers in NBA. Kobe Bryant &amp; Shaquille O’Neal , they are two best players in the whole association. And there is no need for them to be enemy if they want the championship. And when O’Neal left the Lakers, It’s much harder for Kobe to win the championship. Therefore, I do not agree that we need to make enemies. 【08】If there’s something you want to buy, do you prefer saving money yourself to buy it or ask someone to borrow some money to buy it? While, I would like to save money myself to buy something that I awfully want. This is because, the process of saving money for a long time will make one thing even more precious. Saving money is an arduous process and this exhausting experience will help you to remember how precious it was and I will make this thing worth it at all costs. By the way, borrowing money to buy something doesn’t means that we don’t need to pay for it. On the contrary, if we borrow money from the bank, we even need to pay more to buy it eventually, because of the interest and risk of deflation. And we still need to save money to pay my due in the end. Therefore, I would prefer saving money to buy one thing that I desire. 【09】Do you agree or disagree that government should ban violence and dirty words in TV programs? While, I totally agree that government should ban violence and dirty words in TV programs. This is because, children and youngsters are ingenuous and vulnerable, and those violence and dirty words will immensely have a bad influence on their minds. And also, youngsters are not mature enough to distinguish between good and bad. So, they would stand a fair chance to imitate and learn those violence and dirty words in TV program. And that is also a detrimental effect to their bright future. Therefore, I think violence and dirty words should be banned by the government. 【10】Some people prefer sending messages while others prefer making phone calls directly. Which one do you prefer?While, I prefer to make a phone call directly. This is because, phone calls is more directly and efficient than sending messages. If you want to communicate with or get informations from someone, you could just call them and you will get the answer and whatever you what as soon as possible. And besides, making phone call can reduce the loss of information and misunderstanding. Because, other can quickly find your misunderstanding and misstatement and correct you during the dialogue in a phone call. Therefore, I prefer to choose the phone call. 【11】Some people prefer to buy movies or books that they like; others prefer to rent. Which would you choose? Explain why. While, to be honest, I prefer to buy movies or books instead of renting them. This is because, if I buy a book, in other words, I own a book, I can start reading this book as any time I want. This thing matters to me because I like reading very much, so I sometimes will buy a lot of books. This practice and habits will lead to a fact that there are books that I started reading after I bought them for many years. On the contrary, if I rent a book, I have to start reading it immediately because of limited time span of borrowing. Besides, some books and movies are collectible. This is another reason that I want to buy books and movies. Therefore, I think it is better to buy books and movies rather than rent them. 【12】Some people prefer a job which deals with the same tasks every day. Others prefer a job which deals with many different tasks. Which do you prefer and why? While, to be honest, I prefer a job which deals with many different tasks. This is because, by dealing with different tasks, one can hone and perfect one’s skills persistently and unceasingly. And this matters to one’s own improvement. On the contrary, if one only asked to do one thing every day, there is no possibility for him or her to improve himself or herself in a well-rounded way. Besides, If one has opportunities to deal with different tasks, he or she stands a fair chance to meet a large amount of people. They can make the most of this advantages to enlarge his or her social network and establish abundant firm relationship that will definitely useful for anyone. Therefore, I think dealing with many different tasks make a job wonderful. 【13】Some people prefer to be a leader in a group project; while others prefer to be a supporting member in a group project. Which one do you prefer and why? While, to be honest, I prefer to be a leader in a group project. This is because, I have the capability of being a leader, because I used to be leaders in many group projects. So, I have a lot of experience to mobilize every members in the team to pitch in, making time schedule and coordinate works of each members. So, under my lead, the team stands a fair chance to succeed. And besides, by being a leader, I can make more contributions to the project, than just being a member. This is because, a leader has more chances to express and convey his opinions and aspirations precisely and clearly to the whole team. If that is the case, my opinion are more likely to be the fundamental scheme of the whole team. Therefore, this is why I prefer to be a leader. 【14】Your university is planning to allow people in the community to take courses with students. This course will be free for them and they will not receive feedback or grades about their papers. Do you think this is a good program and why? I think this is really a excellent program. You know, in my country, there are numerous people who are not allow to attend university to study even if they still want to study without getting a master degree, this project give them a chance to study in university, and they will be thankful. And besides there is no need to give them feedback or grades, so teachers in university will not be required to add more work, in other words, there will be no more burden on them. They just need to go to the classroom with more people. 【15】Do you agree or disagree with the following statement? Employees shouldn’t send personal texts or emails during work hours. While, I definitely agree that employees shouldn’t send personal texts or emails during work hours. This is because, the work hour means the time that the company pays you to work for them. So, the time in the work hours should be spent on things that related to the work. Apparently, sending personal texts or emails do not belong to the things that related to the work. And besides, sending personal texts or emails would definitely interrupt one’s work. If that is the case, the efficiency will decrease, the high-quality of the work can not be guaranteed. Therefore, I totally agree that employees shouldn’t send any personal texts or emails during work hours. 【16】Some students enjoy decorating their surroundings; other choose to keep their surroundings simple and free of any decorations. Which do you prefer and why? While, I would like to decorate surroundings rather than just keep it simple and free. This is because, by decorating my surroundings, I would feel more sense of belonging. For example, I used to live in a dormitory in high school and I decorated my dormitory at the first day I had the key of the dorm. And every time I step into my dorm, it feels like going back home and I can better relax myself. And besides, keeping things I like around me can mobilize myself and energize myself all the time. Every time I feel frustrated and discouraged, I would like to spend a lot of time staring at a painting which is a duplicate of Starry night of the Rhone, an extraordinary works by Vincent van Gogh. This painting will empower me to face the predicaments in life and pull me back on the rails. Therefore, that’s why I think it is a great idea to decorate surroundings. 【17】Your school plans to set up a study hall for students to take a break and do projects. Do you think this is a good idea or not? While, I think it is an excellent idea to set up a study hall for students to take a break and do projects. This is because, I am a college students who is busy as a beaver. When I have a break time between classes, a study hall would really helpful for me to take a break. So, I don’t have to rush back to dormitory to have a rest. And also, there wasn’t even a convenient place for students to assemble between classes if there is no study hall in the university. When we have to accomplish a team work, the study hall is the only place that suitable for us to talk and communicate with each other. You know, library is not a good place to communicate and arguing about the idea, because these conduct may disrupt others who are concentrate on their own works. Therefore, setting up a study hall is appropriate and even necessary for my school. 【18】What kind of job will you choose? To find a job through which you can get a lot of money or to find a job through which you can get great personal satisfaction? Use specific reasons and examples to support your answer. While, I would like to choose a job which I can get great personal satisfaction. This is because, by doing things that are satisfied with, I can work with full energy for a whole day, and that is the guarantee of the high productivity. For example, I like painting every much, especially the masterpieces painted by Vincent van Gogh, so I find a job in a museum which has a few painting of Vincent van Gogh. By being a stand-attendant of the exhibition, I would not feel any exhausted and discouraged. On the contrary, if I find a job with high payment but boring tasks. I can not guarantee that I can bear this job for a long time, not to mention with high productivity. Therefore, that’s why I would definitely find a job with great personal satisfaction. 【19】Do you think advertisements have a great impact on what we buy?While, to be honest, I do think that advertisements have a great impact on what we buy. This is because, advertisements will demonstrate the merits and advantages of a product precisely and clearly in a way which is easy to remember. That will help us to save a large amount of time searching the website for information. For example, my last laptop was broken in an accent, so I urge to find a new and suitable laptop. When I saw the advertisement of surface which highlights the convenience and portability of the computer. I immediately realize that this kind of laptop that I want. I bought it without hesitation. 【20】Which do you think is more important for a person to be successful: taking risks or making safe decisions? While, to be honest, I think making safe decisions is way more important for a person to be successful. This is because, by making safe decisions with meticulous attention, one will always spot the potential and latent risks of huge loss and cut off the loss at the very beginning. By courtesy of these merit, one can always have positive profits in a long run. And beside, it is entirely possible that one will loss all his money, if one always taking risks. This is because choices with huge risks are emotional and impulsive. The rational decision making should not take risks and make safe decision and prepare well for the plan B in case accidents happen. Therefore, it is wise to make safe decisions so as to be successful. TASK 1 - 10.10【1】When students have questions about an assignment for class, some prefer to ask the professor for help. Others prefer to ask other students in the class for help understanding the assignment.Which do you prefer? Explain why? While, I prefer to go ask the professor directly. This is because, assignments are designed and issued by the professor. So his opinion and interpretation is the most precise one and without any misinterpretation. So, it is a efficient approach to get to better understand the assignment. And besides, students’ understanding also could have some misunderstanding even if they say that they are totally understand what the professor want to say. So, if you just reach an agreement with your classmates, doesn’t mean that you have the right understanding of the assignment. Therefore, I prefer to ask professor straightforward. 【2】Should students be required to evaluate their professors at the end of the semester? While, I prefer that students should not be required to evaluate their professors at the and of the semester. This is because, at the end to the semester, every students will get a score from their professor according to their performance. But students sometimes will evaluate their professor just according to their own score but not according to their professor’s own performance. That is not fair to the professor. And besides, students have the right not to evaluate any professor. So, the university should give students the chance to evaluate the professor and at the same time not to ask everyone to evaluate for obligation. Therefore, I prefer no to required all students to evaluate the professors. 【3】Do you think it’s necessary for children’s growth for them to live far away from home and stay with relatives or friends during school breaks? While, I would like to say it is not necessary for children to live far away from home during the school breaks. This is because, their classmates are their friends and they already spend a lot of time together, you know, they spend 8 hour a day and 5 days a week. And even hang out with other friends at night. And besides, students are really exhausted by the campus life, because of tons of works and projects and reports. So the school break is the best time for them to relax if they can stay at home all day long. And another thing that matters is that, for some students, the school break is the only time that they can spend with their parents. Therefore, I think it’s not necessary for children to live far away from home during school breaks. 【4】Which would you prefer: start a project as early as possible or wait until the deadline? While, I would prefer to start a project as early as possible. This is because, if we start a project as early as possible, and you will stand a fair chance to end this project as early as possible. Because the total time you need to complete the project is constant and not related to the time you start. And you could start the next project immediately, so, you will go to the track of positive circulation, promote the over all efficiency of working. And besides, if you start a project early, then you have the chance to polish it to the perfect and increase the odds of the real life success. Therefore, I stand for the law that always start a project as early as possible. 【5】Some students prefer to study for exam in the night while other students prefer to study in the day.Which do you prefer, explain why. While, I prefer to study for exam in the day. This is because, You know, I am a morning guy, so, when I get up early, like 6 o’clock or 7 o’clock, I can just get to work immediately. And in the early morning, my mind is really sharp and I can better concentrate on my tasks. And productivity and inventiveness will spring up. And beside, when I study in the night, I sometimes will pull an all-nighter, and it is definitely detrimental to our body and physical health. Therefore, I prefer to study in the day rather than in the night. 【6】Some students prefer to do things which they are good at. Some students force themselves to do what they are not good at?Which do you prefer? Why? While, I would like to force myself to do what I am not good at. This is because, doing something that I am not good at can help me to overcome the fear of unknown and get the gut to step out of the comfort zone. That is definitely crucial for one to abroad the horizon and expand the scope of knowledge. And besides, doing something that you don’t good at can force you to ask others for help. And during this learning process, you can make a great number of friends which are proficient at diverse fields. And such a large interpersonal network is a valuable asset in life (will be a great treasure for the rest of your life). 【7】Do you agree or disagree that to succeed we need to make enemies? While, I can not agree that to succeed we need to make enemies. Admittedly, it is true that we will encounter tons of enemies in order to succeed, but it does not mean that we need to make enemies, especially we are already have a lot of enemies. And besides, the best strategy to succeed is to make friends of an enemy. Which means if you can convert an powerful enemy into friends, that will make you ever stronger and even more closer to the success. Let’s look at the Los Angeles Lakers in NBA. Kobe Bryant &amp; Shaquille O’Neal , they are two best players in the whole association. And there is no need for them to be enemy if they want the championship. And when O’Neal left the Lakers, It’s much harder for Kobe to win the championship. Therefore, I do not agree that we need to make enemies. 【8】Good teachers admit they make mistakes or don’t know something.Do you agree or disagree with the following statement? While I definitely agree that good teachers should admit they mistakes or don’t know something. This is because, every human being will make mistakes, and teacher is no exception. But it is OK to make mistakes, as long as we apologize with sincere attitude, and promise we will never make that mistake again. And besides, Admitting there is something that we don’t know is a excellent chance for teacher to tell students that knowledge is boundless and infinite and no body is too old to study. 【9】Do you agree or disagree with the statement that people should be fined for checking or looking at their cellphones when walking in streets and crossing roads? While, I totally not agree that it’s fine for checking cellphone when crossing roads. This is because, it is definitely a dangerous conduct, because once we dive into our cellphones, there is no chance to obverse the road and spot the dangerous situation and keep away from it. So, the right thing we should do when crossing roads is to slow our speed and check around meticulously. And besides, if we do so, pedestrians who walking aside will stand a fair chance to emulate our wrong behavior, and it is also perilous for them not to looking at the road, and probably will cause traffic accident. 【10】Some people think that with the development of technology and Internet, libraries will disappear; while others think libraries are always necessary.Which one do you agree? Please give specific details to support your opinion. While, I believe libraries will not disappear, even if more and more people reading in cell phones. This is because, there are still many people are willing to read in paper materials, because paper material has an advantage that could not be replaced by electronic format which is that reading in paper material are more likely to be in progress of deep reading which is immersed in the book. And besides, libraries also provide all the citizens a quite place to read, if I want to concentrate on my work better at weekend, I could choose to go to city library. Therefore, I think the libraries will no disappear in the future. 【11】Your school is planning to ban library computers from accessing social media websites.Do you agree or disagree with such a plan? While, I totally agree that library computer should be banned from accessing social media websites. This is because, the main goal of library computer is to access some specific database that only library have the authority to access. For the sake of safety, the computer shouldn’t be able to connect both specific database and social media in the same time. And besides, get away from social media can help students to concentrate on their tasks so that his efficiency will be improved dramatically. 【12】Your university is planning to allow people in the community to take courses with students. This course will be free for them and they will not receive feedback or grades about their papers. Do you think this is a good program and why? I think this is really a excellent program. You know, in my country, there are numerous people are not allow to attend university to study even if they will want to study instead of getting a master degree, this project give them a chance to study in university, and they will be thankful. And because there is no need to give them feedback or grades, so teachers in university will not be required to add more work. They just need to go to the classroom with more people. 【13】Students should take some additional courses so that they can get their credits more quickly.Do you agree or disagree with the following statement? While, I am not agree that students should take additional courses to get credits more quickly. This is because, students are required to attend three or four courses every semester, and assignment of them are already choking high, so there is no time for them to take the additional courses. And besides, provided that a student take additional courses so as to get more credits. He stands a fair chance never learn any subject well, because they have to squeeze their time from full schedule, and hardly be full of vigor. 【14】Some people prefer to give their opinions immediately. Others prefer to wait and listen to others’ opinions before giving their own.Which one do you think is better? While, I definitely will wait and listen to others’ opinions before giving my own. This is because listening is a excellent chance for everyone to learn from others. If we just talk and talk all the time, there is no chance for us to learn something new. But if we always listen to others’ opinions, we will end up learning numerous ideas that differs from ours’. And besides, wait and listen before talk is a good way to show respect. And it is a decent conduct to communicate with others. And if you do so, you will stand a fair chance to build up a better relationship. 【15】Do you agree or disagree: children should learn to draw or paint? While, I definitely agree that children should learn to draw or paint. This is because, painting and drawing is an excellent way for kids to express themselves. You know, children sometimes can not present their feeling well using language due to the lack to vocabulary. So painting give them another way to better express and communicate. And besides, learning to painting can foster them ability to concentrate on one thing in a relatively long time. And this ability will help them occupy a vantage point in fierce competition in the future. 【16】A company plans to interview you. You can go to their company for the interview, but the company is far from where you live. Or you can have a telephone interview.Which do you prefer? Why? While, I will definitely choose to have a telephone interview. This is because, time is precious and should not be wasted. If I have to go to the company to have a face to face interview, I have to squander a large amount of time on the commute. That is not cost-effective. And besides, I can still distinctly and clearly express my opinion in a telephone interview and try hard not to cause misinterpretations. So, nothing is lost and detrimental for my to take a telephone interview instead of a face to face interview. 【17】Business conferences should meet in person instead of using video calls.Do you agree or disagree with the following statement? While, I am not agree that business conferences should meet in person instead of using video calls. This is because, using video calls is an excellent way for everyone to improve their efficiency. You know, we don’t have to squander our precious time in the travel to the company. And besides, high technology furnish us with the ability to conduct the high quality video calls. So there is no chance for me not to use it. So I by no means will not to use it. So on no account will I not to use it. 【18】 Do you agree or disagree with this statement that we should help our friends only when they ask for help? While, I really do not agree that we only help our friend when they ask for help. This is because, as a truly friends, we should help our friends when they need help, not when they only ask for help. Because, sometimes, they do need help and are willing to be help, but they are just shy or ashamed of being helped. And besides, even if they don’t need out help, they can still better and quicker tackle the problem under our help. Therefore, I don’t agree that we only should offer a helping hand when they ask for help. 【19】Do you agree or disagree that children who do sports at a young age will be more aggressive in the future? While, I really to not agree that children will become more aggressive in the future just because they do sports at a young age. This is because, doing sports is an excellent conduct for everyone to release their aggressive feeling or thinking in a proper way, as well as a legitimate way. It is not the way to spark up one’s aggressive feeling. And besides, it is just the other way around, if one can unleash their dissatisfaction and some aggressive thoughts, he will stand a fair chance to tackle the quandaries in a more amiable and genial way. 【20】When your friend is about to take a visit to your house, do you prefer them to inform you before their coming, or do you prefer a surprise visit? While, I do prefer they inform me before their coming. This is because, as for me, I will stand a fair chance to better prepare myself, you know, I can just get dressed, get ready for snacks, and then clean the house if I have time, just before their coming. And besides, as for them, informing me before visit can help them to check if I am home. Because sometimes I will not stay at home all day, so informing before coming can know whether I am home, and if I am not home, their can take a rain check and not need to squander the time in commute. Therefore, I prefer to be informed before anyone comes my home. TASK 1 - 9.07【1】Some universities accept the students to choose a major field of study when they enter the school; while other universities wait until the second or the third year before students deciding to choose a major field of study.Which do you prefer? Why or why not? While, I think universities should wait until the second or the third year before students deciding to choose a major field. This is because, the university students should be cultivated by a all-rounded way, if a student choose a major field as soon as he enter the university, he would probably directly dive into one direction and just ignore the others, it is not a very wise choice. And beside, delay the time to decide the major allow students to find their own interest, like, they can sign up a variety field of class and cautiously choose the major afterwards. Therefore, I would like to say universities should wait until the second or later. 【2】When your friend is about to take a visit to your house, do you prefer them to inform you before their coming, or do you prefer a surprise visit? While, I do prefer they inform me before their coming. This is because, as for me, I can better prepare myself, you know, I can just get dressed, get ready for snacks, and then clean the house if I have time, just before their coming. And besides, as for them, informing me before visit can help them to check if I am home. Because sometimes I will not stay at home all day, so informing before coming can know whether I am home, and if I am not home, their can take a rain check. Therefore, I prefer to be informed before anyone comes my home. 【3】If you are to choose between 2 apartments to live in next semester, one apartment is near the campus but slightly expensive; the other is a little far from the campus but cheaper.Which do you prefer? Explain why. While, I would like to choose the apartment which is near the campus. This is because, it can help me to save my time. You know, if I live far way from school, I will spend lot of time on the way to school. And, I if live near the school, I will have a lot of time to do whatever I want. And besides, the time saved can be used to sleep well, which is conducive to better concentrate during the daytime, you know my brain is sharper and can understand what professor says quickly. Therefore, I prefer to choose the apartment near to school. 【4】Children born with talent should be treated in a different way or they should be treated in the same way as average children.Which one do you agree? Personally, I agree that children born with talent should be treated in a different way. Because they are the potential experts in various fields in the future, so they should be put together in a specific institute for special training, which will inspire their potential to the best, thus people can make fully use of it in the hope of making breakthroughs in some fields. I remember that my younger cousin showed a talent in drawing when he was little, so his father, sent him to a school that focus on enhancing the ability of painting. Now he become a famous designer and has created some brilliant projects. 【5】Some people prefer to make friends with people who are of the same age. Other people prefer to make friends with people who are of different ages.Which one do you prefer? Why? While, I prefer to make friends with people who are of the same age. This is because, we possibly have the same schedule and we will spend a lot of time together, you know, we are going to the same school, taking the almost the same curriculum, so we almost spend 8 hours a day. That will give us more time to communicate with each other, and get to better know each other. And besides, we don’t have the generation gap, and we are more easier to understand each other and provide better and pragmatic advises. Therefore, I prefer to make friends with people who are of the same age. 【6】We should help our friends only when they ask for help?Do you agree or disagree with this statement c 【7】With more and more people reading in electronic format, libraries will disappear.Do you agree or disagree with the following statement? While, I believe libraries will not disappear, even if more and more people reading in cell phones. This is because, there are still many people are willing to read in paper materials, because paper material has an advantage that could not be replaced by electronic format which is that reading in paper material are more likely to be in progress of deep reading which is immersed in the book. And besides, libraries also provide all the citizens a quite place to read, if I want to concentrate on my work better at weekend, I could choose to go to city library. Therefore, I think the libraries will no disappear in the future. 【8】Your professor suddenly cancels the class for today. Would you prefer to go shopping with your friends or prepare for the exam? While, I prefer to prepare for the exam. Because, the daytime is the prime time in one day which my mind is sharp and can better concentrate on difficult tasks, and if my professor cancels the class, them I will have an entire morning or entire afternoon to study and prepare the exam because I really want to have the better grade. And besides, as for shopping, it will be better to shop at night when I am tired and exhausted by the study, I can shift my mind to do something relaxed such as shopping. Therefore, I prefer to prepare for the exam if professor cancels the class. 【9】It’s getting harder and harder to save money than before.Do you agree or disagree with the following statement? While, I do believe that it is harder than before to save money. This is because, nowadays, we are on a stage that there are so many place for us to spend many than before, you know, we have more electronic device to choose, and a lot of attraction to go, and none of them for free. And sometimes we spend money in a impulsive way. And besides, because of the currency inflation, the price of the daily merchandise are going up. So, it is getting harder to save money. 【10】Your community has received a large amount of donation, should it be used to construct a playground for children or build a garden for the community?While, I prefer to build a garden for the community. This is because, a garden can serve more citizens in the community. You know, the elders can use this place to relax and teenagers can hang out with each other in this garden. And besides, the children can also use this garden to play as a playground. In the community I once lived, many parents will bring their children to the community garden to play. So, it is also a nice choice for children. Therefore, I think building a garden for the community is better. 【11】Participating in team sports is a good way for young person to learn team cooperation.Do you agree or disagree with the following statement? While, I do believe that participating in team sports is a good way, and maybe the best way to learn team cooperation, especially for the young people. This is because, teamwork in sports is so important that no one can just win without an excellent cooperation with teammates. For example, members of a football team need to cooperate to successfully perform a play, whether it be a running play or a passing play. Without all involved in the play working together to make the play happen, the other team could wind up in an ultimate victory. 【12】Do you prefer to work from home or in the office? While, I do prefer to work at home. This is because, I can work in anytime I want. You know, I am a morning guy, so, when I get up early, like 6 o’clock or 7 o’clock, I can just get to work immediately. And in the early morning, my mind is really sharp and I can better concentrate on my tasks. And besides, working at home really make me feel comfortable and relaxed. You know, I can dress whatever I want, and I don’t need to worry about being interrupt by others. Therefore, I would like to work from home. 【13】It is a good idea to get a job under the influence of other people.Do you agree or disagree with the following statement? While, I do believe that it is a good idea to get a job under the influence of others. This is because, if we take the parents idea into consideration, they will give us some precious opinion, because they are more likely to have a deep view of the strengths and weaknesses pf a job. And besides, the influence from friends also important because we don’t have generation gap, so their suggestions is pragmatic and suitable for me. Therefore, I would like to say that getting a job under others’ influence is a good idea. 【14】Some kids like to play games outdoors, and some kids like to play inside their house.Which did you prefer when you were a kid? Why? While, I would like to play inside our house. This is because, playing inside the house is more safe for everyone, we don’t need to worry about being stolen or robbed, and if somebody left something, it is more likely to get back. And there is little air pollution inside the house. And besides, I like to play inside because it is more comfortable. You know, there is no need to be well-dressed and have to wear the jacket and tie. We could dressed informal and suit ourselves. Therefore, I prefer to play inside. 【15】Some people believe old people should not take risks and participate in adventurous events as the young people.Do you agree? Why? While, I do believe that elders should not take risks and participate in adventurous. This is because, they have to take the safely into consideration, maybe safety is the prime consider for them. Because, compare with the young man, doing the same adventure is more risky for the elders because of their physical status are more fragile and more likely to cause physical deterioration. For example, my grandpa have a slight cardiac disease, although it is not a big deal, doctor suggest him not to do the strenuous exercises because life always comes first. Therefore, I agree that elders should not take risk to be involved in adventures. 【16】Some people prefer sending messages; while others prefer making phone calls directly.Which one do you prefer? While, I prefer to make a phone call directly. This is because, phone calls is more directly and efficient than sending messages. If you want to communicate with or get informations from someone, you could just call them and you will get the answer and whatever you what as soon as possible. And besides, making phone call can reduce the loss of information and misunderstanding. Because, other can quickly find your misunderstanding and misstatement and correct you during the dialogue in a phone call. Therefore, I prefer to choose the phone call. 【17】When you disagree with your friends or family on certain things, would you like to convince them or let them keep their own opinion? While, when I disagree with my friends or family, I tend to let them keep their own opinion. This is because, we are different person with different background, we could not totally agree with each other on everything. So, if we try hard to convince others, we may often slide into drastic argument or even fierce fight, and sometimes end up with nothing. And as for me, I think distinctly express my opinion is important, but getting involved in an argument which meant to convince others are meaningless and just the waste of time. Therefore, I would like to let them keep their own opinion. 【18】To teach old people to use the computer in the community, which do you think is better? To find a professional to teach them outside, or to find a student to teach them at home?While, I think finding a students to teach elders at home is better. This is because, it is more convenient for elders to be taught. You know, some of the elders are not capable of leaving home because of some specific health conditions. So, it is more possible for them to be taught at home. And besides, it can save a lot of time for the old people, because they don’t have to spend a lot of time on the way to the class outside. Therefore, I think finding a student to teach them at home is the best choice. 【19】Some people believe that we should not discuss about the private activities of the popular people, like movie stars and singers.Do you agree? Why? While, I do believe that we should not discuss about the celebrities’ private activities. This is because, their private activities are none of my business, and their private activities are undoubtedly their privacy, and paying too much attention to their privacy will possibly cross the line and break the law. And besides, staying out of their personal life can help me better focus on their works. You know, when I watch a movie I really want to do deep in to the character they play not be entangled by their anecdotes. Therefore, I agree that we shouldn’t pay any attention to their private activities. 【20】Many people think that students study course materials more effectively by taking exams; while others think that students learn more effectively through doing other activities such as writing paper or completing projects.Which do you think is more effective for students to learn? While, I do believe that students learn more effectively through doing other activities such as subject report. This is because, these kind of report or papers will give us a chance to deeply understanding the field or subject in a thorough and well-rounded way. But taking exams will limit us to study some key point which gonna be tested and ignore the others. And besides, completing a project will allow us to study under less pressure than the exams. And a relaxation and happiness will help us to enjoy the research which lead to high efficiency. Therefore, I prefer to do other activities than exams.","link":"/Blog/2019/08/21/Study-Notes-of-TOEFL-Speaking-Part-Task-1-Outline-of-Real-Test-Questions/"},{"title":"Study Notes of TOEFL Writing Part —— Outline of Real Test Questions","text":"Some people prefer to buy technological devices as soon as they are available to the public, while other people prefer to wait. Schools always collect information about teachers’ teaching performance and give rewards to those teachers who perform well. etc [toc] 机经提纲 - 12.1【01】Some people prefer to buy technological devices as soon as they are available to the public, while other people prefer to wait.Which do you prefer? 开头 物联网，人们离不开 technological devices 了 中间段1 过一段时间等功能完善了再买会比较明智 Firstly, it is wise to buy the device later when its deficiencies get fixed. 新出来的产品，往往会不完善。用户多了，提问题给他们，问题解决了，手机就完美了。 It is because when a new technological device is available to the public for the first time, there may well be several inconspicuous imperfections existing in it. With the growing number of users reporting the problems to the manufacturers, the following version is bound to be better than its predecessor. IPhone 4，信号不好。 Taking iPhone4 for example, this product suffered sharp criticism over signal-receiving problems caused by its poorly-design antenna. 等一段时间，稳定了，名扬天下了。 After rounds of adjustments, the later versions of iPhone4 gradually became stable in function and earned its worldwide reputation. 中间段2 新出来的产品价格普遍比较高，等一段时间后价格会回归理性。 Secondly,the price of a newly launched technological device is relatively higher, which means the quality-price ratio is not satisfactory when people buy it at the very beginning. 经济学讲，上来会短缺，价格会很高。 According to laws of economics, when popular device is firstly brought to the market, the supply falls short of demand, which results in a higher price than its real value, but after a period of time, the price will return to a reasonable level. Surface，上来价格很高，大家买不起。 For example, the initial price for Surface, a product by Microsoft, was set at around $600 that is unaffordable to most people. 三个月后，价格下降，更理性了。 3 month later, the price was cut down to ​300 that is much more reasonable. 结尾 To put it in a nutshell, I prefer to buy a technological device after many people begin to use them. As an old saying goes, good things are worth waiting. Facing a new fascinating technological device, we are advised to wait until the price comes down and the quality becomes better. http://www.igo99.cn/toefl/xiezuo/102474.shtml 【02】Schools always collect information about teachers’ teaching performance and give rewards to those teachers who perform well.Which way do you think is more useful? 1）to evaluate teachers’ performance by students2）to evaluate teachers’ performance by teachersUse specific reasons and examples to support your answer. 三个理由 学生接触老师比老师接触老师时间长，所以更能知道老师水平 老师应该回应学生，而不是回应同行 能帮老师省时间 开头 A teacher is generally regarded by many an authoritative role in teaching activities and being a teacher requires solid academic ground in the subject which the teacher specializes in. Therefore, teachers’ teaching performance should be assessed by teachers, who have been trained in teaching methodology and thus are more professional in evaluating process. However, as far as I’m concerned, it is more useful for schools to collect information about teachers’ performance and allocate rewards to those who perform well based on students’ evaluations. 中间段1 To start off, it is more reasonable and effective if students evaluate teachers’ teaching performance. Because students are the direct receivers in everyday teaching activities. They are better acquainted with their teachers’ teaching performance either in teaching expertise or teaching styles. They can judge whether their teachers’ methods are effective for them or not in a long run. Whereas, other teachers are not able to know well the teacher’s teaching performance by attending only one demo class of the teacher being evaluated. Since a teacher’s overall performance shouldn’t and can’t be assessed merely through several class demonstrations, evaluation for teachers’ performance from their colleague teachers shouldn’t be seen as reasonable and valid enough. 中间段2 In addition, teachers are supposed to respond to students, not to their colleagues nor supervisors. An experienced teacher should prepare teaching materials and give assignments based on students’ learning abilities. Teaching adjustment is always necessary in making lesson plan. This process, however, may conflict with the standard curriculum plan that schools assign. In this case, the teacher will probably choose to conform to the standard lesson plan to appeal to his or her peers and the school’s authority if his or her teaching performance is evaluated by them. Therefore, it is essential to design a valid evaluation system to incentivize teachers to respond to students rather than others. 中间段3 Secondly, evaluation through students can save a lot of time and effort for other teachers. For example, In Chinese schools, each teacher typically has to deal with over fifty students each class. Therefore, it will be a nuisance to pay detailed attention toward all other teachers on how they teach students, and this is under the assumption that they want to evaluate their peers in a fair and just manner. Since students have already spent so much time with their educators, they can easily come to conclusions on how good or bad their teachers are. In this sense, students can be more inclined to make fair evaluations compared to other teachers. https://toefl.koolearn.com/20190115/830946.html http://www.360doc.com/content/18/0604/22/46601607_759707749.shtml 【03】Some people prefer to play sports within a group, while others prefer to do exercise alone.Which one do you prefer and why. 扩大自己的社交网络 更安全 【04】In order to be successful, people have two choices1）To take risks 2）To keep cautious and careful Which one do you think is the better way to succeed? Give reason to explain your choice. From ancient time to the present, 小心和谨慎是两个必不可少的因素 小心可以帮助我们很好的发现机会 “Buy low, sell high” is the mantra of the stock market. Perhaps the most extreme example of this is arbitrage, the act of buying and selling goods simultaneously in different markets to gain an immediate profit. Impressive, but tricky. 谨慎可以及时止损和发现错误 If you cut your losses, you stop doing what you were doing in order to prevent the bad situation that you are in becoming worse. 冒险可能可以发一笔横财，终归无法持续，终会失败 【05】In order to get a higher promotion and salary, many people chose to improve their job performance in two ways.Which one do you prefer? Why? – To do additional work and assignments – To actively participate in the group work 能够有更多的机会与领导合作，留下深刻的印象。 能够建立自己的交集网，得到同事的支持，在晋升的时候更有把握 【06】Most adults believe that modern children (5-10 years old) behave worse than those in the past. What action parents should take do you think will have the most positive effect on children to help them be have better like respecting and treating others kindly?1）limit the types of the TV programs and movies they watch 2）spend more time talking with children 3）supervise and monitor children while they are playing with their friends Use specific reasons and examples to support your answer. In my opinion, for parents, sacrificing their spare time to actively and friendly communicate with their children is a more effective way. By spending more time deeply communicating with them, parents can better understand their children and exert positive guidance. Correspondingly, their children also tend to accept their suggest, or even criticism rather than roughly ignore it. By comparison with the second one, the other two ways in the three options are somewhat unfriendly and compulsory, which would backfire. Generally speaking, children may develop a rebellious mentality if their parents are too strict with them. To sum up, faced with children’s undesirable behavior habits, compared with mandatory measures, such as limiting the types of TV programs or movies, and monitoring their interactions with their friends, directly speaking to or chatting with them in person is a tender and amiable way and as well as more effective and less side-effect way. https://www.jianshu.com/p/f4b7fa82ad0c 【07】As a student of university that has a long break between university semesters, the university requires all students to do one of the following for one month during the break:1）students must take a course on the subject that has no direct connection to their majors of study. 2）students must volunteer to work in the city where the university is located or their hometowns to improve some aspects of life of the city or their own town. which one do you think is more beneficial for students in their university. Why? Schools should require students to volunteer on the projects in the university’s city or hometown. 参加志愿服务活动可以培养学生的社会责任感（cultivate the sense of responsibility）。因为在做志愿服务的过程中，会帮助那些需要帮助的人，或者是帮助一些机构解决问题，比如美化环境，清扫街道等，这会让学生感觉自己是社会的一份子，这可以让他们实现个人的社会价值。 参加志愿服务活动还可以促进学生的社交能力 (facilitate their social interaction)。因为在活动的过程中他们需要学会如何与周围的人相处得融洽（get along well with other people），如何表达得加得体（how to express properly），如何解决棘手的问题（how to deal with thorny problems）。 参加志愿服务活动还可以适当得减少学生的课程压力（alleviate their pressure on courses）从而帮助他们获得好的学习成绩（which assists with their test performance）。因为在做志愿活动时，学生可以得到适当的休息，同时，社会活动的经历也让他们明白，只有通过努力，才能让自己立足在这个充满竞争的社会中（only by making painstaking efforts on study can they keep a favorable position in the society full of fierce competition），因此学生就会在学习上取得好的成绩。 【08】Rather than help their children do schoolwork, parents should encourage their children do their homework independently. 培养他们解决问题的能力 建立自信和成就感，不怕困难 (Self-confidence and fulfillment) 家长参与也有好处，帮助他们集中精力，但是不应帮助做作业，不然他们成长的空间就被剥夺了 be deprived of To start with, encouraging children to do their own their independently is conducive to cultivating self-confidence. Besides, the best way to teach children responsibility is to encourage them to finish tasks on their own. Admittedly, in some cases, parents should not hesitate to help their children due to the consideration of safety. 【09】At some universities, students take part in making decisions about the issues that affect daily life of everyone on campus, such as how many hours that the libraries should be open each day or what kinds of food should be served in the cafeteria. But at some universities, experts are hired to make these decisions, students almost never involved. Which approach do you prefer and why. To start with, when students consider about issues related to school life, such as library operating hours or dining options, they might only think about their own needs and preference while at the same time, some more relevant factors will be overlooked. 学生一般来讲比较考虑好处，很少考虑安全问题 学生和老师都很重要，学生考虑会忽略老师的意见 Furthermore, experts are more professional. 学生的定见能够被部分考虑，由于是直接关系学生日子的决议，但不应该要学生彻底做决议。 学生提出意见，专家进行决策 【10】One can learn a lot about a person from the type of friends this person has.我们可以从一个人的朋友身上了解到这个人的价值取向 你有一个朋友他一直非常努力，坚信着努力才能创造奇迹的信条，无论是学习还是工作他都尽可能做到最好，他身边的朋友也是如此。如果能了解他们中的一个人的话就会知道我的朋友也有相同的价值取向。 You have a friend who has been working very hard and firmly believes that hard work can create miracles. He does his best to study and work, as well as the friends around him.If I could understand one of them, I would know that my friends have the same value orientation. 写我们可以了解到这个人的兴趣爱好 My hobbies are painting. My friends also like to paint. We often go to the park together when we have time sketching in the suburbs.Therefore, if I know my friends, I will know that my hobby is also painting 【11】Do you agree that it is better to work for business owned by someone else than to work for the business of one’s own family.Obviously, the benefits of working for a business owned by someone you do not know outweigh its disadvantages. 自由 社交网络 To start with, to work in an irrelevant company gives you a lot of freedom of choice since personal emotions and family relationship will not be taken into consideration when you make decisions. And besides, working for a business owned by strangers provides you a bunch of chances to expanding your social network instead of working with a lot of relatives. Admittedly, working in a business of your own family do facilitate your adjustment to the company because your relatives will help you know about how the whole system work and maintain quickly. However, if viewed from another angle, this point of view seems not plausible enough for one to choose family business, because if you have favorable communication ability, one could become acquaintance with colleagues and they are willing to help too. To sum up, facing the choice between working in family business and competing independently in job market, people should opt for the latter one. 【12】Students aged 13-18 are taught different subjects by different teachers while younger students are taught by only one teacher all day long. Some people suggest it would benefit young students to be taught by different teachers.Do you agree with this view? Why or why not? 老师精力有限 学生注意力难集中 I agree with this statement that younger students will benefit from being taught by several different teacher every day. Teachers have limited time and energy. If they are to teach students all day long, they will be too tired to guarantee the teaching quality. 老师的时间和精力有限。如果一整天都不休息，一直给学生上课，这样老师会很累，教学效果也不好。 It is much harder for younger students to concentrate themselves. If they are taught by only one teacher all the day, the newness (the feeling of freshness and attraction) will soon wears off and they will feel bored, shifting their attention to something more interesting. 年纪更小的学生更难集中注意力。如果一整天都是一个老师在上课，学生很快就没有新鲜感，觉得无聊，注意力很快就会转移到其它更有趣的地方。 有的人说一个老师更容易产生依赖。但这是好事吗？ All in all, I do believe that elementary students should also be taught different subjects by different teachers. 总之，我认为，位于小学教育阶段的学生也应该由不同老师来教授不同科目。 http://www.sohu.com/a/159375812_292611 http://www.kekenet.com/toefl/201710/523980_2.shtml 【13】A lot of high school students now cheat in homework assignments, by asking other students for answers.Which of the following do you think is the most efficient way to stop? – asking parents to help stop the students from cheating – penalty or punishment to the students – asking teacher to create homework assignment that cannot be easily cheated” Now, the education institutions are in a stage that cheating as a serious problem become a press matter of the moment. So, this phenomenon triggers a grave discussion on how to oppose cheating. In my opinion, The second one, which is stepping up efforts to penalty to the cheating students. To start with, if schools increase the punishment, students would not dare to cheat once they recognize that they have to pay a high price to do so, and school will receive quick returns with this method. Furthermore, this kind of methods can help cultivate students the sense of integrity and foster their conception of abidance of the rules whether in a school or in a society. Last but not least, it is difficult either for parents to effectively supervise children’s behavior or for teachers to design homework that is impossible or difficult to cheat. From what we discussed above, we can readily reach the conclusion that to cope with there increasing number of students who cheat in homework, school’s best choice is to impose the severe penalty on those who dare to cross the line. This is the most effective method to force them to willingly regulate behaviors by themselves. https://www.sohu.com/a/193435143_292611 http://www.xuexila.com/yc/3881162.html 【14】Do you agree or disagree with the following statement? In the past people ate food that was better for their health than they do today.Use specific reasons and examples to support your answer. 过去的粮食和蔬菜比较少大规模使用杀虫剂和化肥，食物本身的有害残留物更少。 In the past, pesticides and fertilizers were rarely used in food and vegetables There were fewer toxic residues left on the food. Fast food fails to take balanced nutrition into consideration. 现代食物，尤其是快餐，很大程度上忽视了营养搭配，将高油高糖高卡路里的食物推荐给消费者。 Modern foods, especially fast foods, largely ignore the combination of nutrition and recommend high-oil, high-sugar, high-calorie foods to consumers. 现代食物往往是精加工食物，在追求口感的同时损失了营养。 Processed food which is popular in this day and age may sacrifice its nutrition to the taste. 选择更多，不代表更健康，相反我们会选择更想吃的，而不是更健康的 【15】Which area the government should fund to improve children’s education?1）hiring more teachers to teach in a small class 2）preschool education before kindergarten 3）providing some training courses so that teachers can be more professional 学前教育可以帮助发现兴趣 学前教育可以帮助孩子提高注意力 学习基础知识 Preschool education can help to mold children’s character Preschool education can be conducive to the cultivation of some basic but critical abilities for children, which are the prerequisite capabilities to the future success. Certainly, hiring more teachers can be beneficial to children’s better development and providing training classes can enhance teacher;s professional skills. But both of them can not be seen as the fundamental approach to improve the children’s education. 然而，教育的主体从来都是学生，而学前教育恰恰是一个人一生中最重要的性格、能力塑造和培养阶段，所以普及学前教育才能从根本上让学生甚至整个社会受益。 【16】Physical exercise is important to older people than to younger people.Nowadays, an increasing number of people, work in the office and seat whole day, are beginning to realize〝doing exercise is closely related to health.〞That is why I see lots of people, old and young, exercise in the playground. 因为老人更需要锻炼延长寿命 For the elderly, using exercise for physical fitness is the most important 老人没有工作压力，体育锻炼可以做为他们的休闲，可以结交朋友，利于 mental health Elderly people do not have work pressure, physical exercise can be used as their leisure, they can make friends, which is good for mental health 但是不能等到年纪大再做锻炼，或是锻炼要注意保护 【17】People in daily lives would frequently do the jobs that need creativity, such as the job you have never done before. Under this circumstance, do you prefer to work alone or work with others? 一起做有利于头脑风暴，找到最佳的解决方案 Brainstorming is a group creativity technique by which efforts are made to find a conclusion for a specific problem by gathering a list of ideas spontaneously contributed by its members. 一起做有利于找到漏洞 Doing it together is good for finding loopholes attend to each and every aspect of a matter; be well considered in every aspect; try to cover every aspect We need teammates to correct our mistakes, or it is necessary to check the places we have not considered 【18】A government spends money on all adults after 25-year-old on a training course for the most up-to-date skills at workplace. Do you think it is effective？Why or why not？ 25岁太晚 所有人太多 所有人太多 A society needs a competent workforce equipped with up-to-date skills. Otherwise, it will be unproductive and fall behind the time. However, when the administration spends money on a nationwide work skills development training program where eligible trainees are only adults at the age of 25 or older, it should not expect the desired return. First, the age of 25 may be a late point of start. Instead, an earlier age would be far desirable. Furthermore, the nationwide scale is too wide to be pragmatic. From what we have discussed above, we can readily draw a conclusion that I hardly can be optimistic about the effectiveness of the program. I think it is more likely to be a failure or a borderline pass than to be a success program. 【19】Your teacher assigns a project to you, and you can select the members to work with.– choose the members who think and work in similar ways – choose the members who have totally different ideas Which would you think is more effective to work with?” 相同的人可以减少交流成本 相同的人可以一起面对困难 It is tempting to think that finishing a project with members who have different ideas could rise the diversity of the team, leading to a better result of the team assignment. Nevertheless, from my perspective, working with people who share similar opinions is a more effective way to achieve success. On the one hand, teaming up with classmates of a similar kind will reduce the cost of communication, which results from disagreements among team members. People who share the same character with each other are more likely to reach into an agreement quicker and easier since they share similar logical patterns. On the contrary, it is difficult for the whole group of students to agree with each other, which not only could cause conflicts among members but also waste a significant amount of time for each student during the process of negotiation. My roommate, for instance, once teamed up with her classmate to deliver a presentation in Marketing class. Unfortunately, she and her team member shared an entirely different value and interest. My roommate wanted to force on the finance industry and to make her PowerPoint look more professional. However, her classmate’s only attention was video games, and he insisted on making their presentation more entertain and enjoyable. Eventually, their cooperation went to a dad end. They had to do their whole work again separately a few days before the deadline. On the other hand, it is almost impossible to find a perfect time of meeting for every team member if they have different thoughts. The same kind of students is likely to have an identical lifestyle as well. It is not surprising, for example, that hard-working students usually get up early and typically do not stay up late. Nonetheless, people who enjoy night clubs and bars could not regulate themselves as strictly as other students. Taken my own experience as an example, I initially work with six people in my Business Communication class to finish homework. Some people were eager to finish it sooner while others did not want to start it until the deadline. Hard as we tried, we could only not find a proper time for everyone, and our homework was finished in a hurry. Admittedly, having a bunch of people who have different ideas could bring the team with fresh thoughts and unique insights of solving problems. Nevertheless, relatively simple schoolwork assigned by teachers, such as a presentation for English class or a Math question, is unlikely to be so complicated that so many different views and creativity are needed. In summary, no doubt working with people of similar mind is a more efficient method to finish a task because same people usually are more comfortable to communicate and that it will be less exhausting to arrange a suitable time for meeting for each team member. https://www.testbig.com/independent-toefl-writing-essays/your-teacher-assigns-project-you-and-you-can-select-members-work-0 First of all, working with similar people can avoid unnecessary arguments, thus promoting efficiency. Admittedly, appropriate arguments are rewarding, since they might trigger new thoughts and creative ideas. Despite these potential benefits, working with people of different ideas inevitably makes the whole team take the risks of discussing too much time to finish the assignment in time. For example, once I had a homework to conduct a survey about the purchasing power of young people. To my disappointment, my team spent almost half of the given time struggling to select a location to do the survey, which, in my opinion, was merely trifle details worth little attention and could be skipped if working with people of similar thoughts. Finally, mistakes are common in teamwork, even in the team consisting of people with similar thoughts. However, mistakes may be more destructive in the team composed of people with different thoughts. Imagine a mission not going smoothly as planned, those whose ideas were ignored at the very beginning would make a lot of complaints. Chances are that they might claim that it would be better if their original suggestions were considered. These sorts of complaints are extremely detrimental, since they would spark a situation where members in the team don’t trust one another. On the contrary, if the team is made up of people of similar thoughts, they would possibly stand together firmly and strive to address the problem. https://www.testbig.com/independent-toefl-writing-essays/your-teacher-assigns-project-you-and-you-can-select-members-work-4 【20】The government is not doing enough work to educate people the importance of nutrition and healthy eating.政府就没有怎么作指导 政府做的一些指导也没有什么用 Currently, living in a fast-paced society, there is an increasing number of city dwellers suffering from the pressure generated from the excessive workload, consequently, majorities of city dwellers, especially those white-collar, are enslaved by diversified diseases, like cervical diseases, cardiovascular diseases and so on. To begin with, governments have done little in arousing public’s awareness to develop a healthy diet. That’s why bad eating habits are ubiquitous around us. For instance, fast food restaurants like McDonald’s are always crowded where junk food is served, ranging from burgers, French Fries to cola which are high in fat and sugar. People can easily gain weight if eating too much of that. Besides, every time I visit my grandmother’s home, she treats me a full table with delicious dishes, like braised pork, steamed beef and roast mutton but no vegetables. Although meat in Chinese tradition shows hospitality, to some degree, it indicates my grandmother’s lack of awareness about what a balanced diet is. Based on such prevalent phenomena, some practical approaches should be implemented by governments to increase people’s motivation to eat healthily, such as printing some booklets with tips of how to eat healthily and foods containing good nutrition. The government’s guidelines are vague, though nutritional advice generally isn’t. Thomas Sherman, a medical school professor and biochemist at Georgetown University, points to the guidelines’ heavy influence on what foods and serving sizes are included in the national school lunch program, as well as what products can be purchased using benefits from the Women, Infants and Children and Supplemental Nutrition Assistance Programs. Still, he said, the government has struggled over the years to present the guidelines in a way that they are easily understandable and usable, creating a gray area filled in by an “industry of experts” who “interpret, critique, rephrase or reimagine the guidelines,” making them particularly difficult to follow in any meaningful way. And this is despite the fact that the guidelines have not changed much since the government first began offering nutrition advice many decades ago. The heart of the message has mostly remained that we should eat fruits and vegetables, whole grains, nuts, and legumes, and limit our consumption of meat, sugar and salt intake – though the wording has varied. “I think the average person would have a very difficult time just making sense of it,” Sherman told The Huffington Post. “It’s difficult to go shopping with the guidelines in your hand and do meal planning.” https://www.testbig.com/independent-toefl-writing-essays/do-you-agree-or-disagree-following-statementthe-government-not https://www.testbig.com/independent-toefl-writing-essays/government-not-doing-enough-work-educate-people-importance https://essayforum.com/writing/governments-done-enough-educate-66488/ https://www.huffpost.com/entry/dietary-guidelines-eating-habits-impact_n_56a7f862e4b04936c0e8a61b 机经提纲 - 10.13- As a student of university that has a long break between university semesters, the university requires all students to do one of the following for one month during the break:(1) students must take a course on the subject that has no direct connection to their majors of study. (2) students must volunteer to work in the city where the university is located or their hometowns to improve some aspects of life of the city or their own town. which one do you think is more beneficial for students in their university. Why? Schools should require students to volunteer on the projects in the university’s city or hometown. 参加志愿服务活动可以培养学生的社会责任感（cultivate the sense of responsibility）。因为在做志愿服务的过程中，会帮助那些需要帮助的人，或者是帮助一些机构解决问题，比如美化环境，清扫街道等，这会让学生感觉自己是社会的一份子，这可以让他们实现个人的社会价值。 参加志愿服务活动还可以促进学生的社交能力 (facilitate their social interaction)。因为在活动的过程中他们需要学会如何与周围的人相处得融洽（get along well with other people），如何表达得加得体（how to express properly），如何解决棘手的问题（how to deal with thorny problems）。 参加志愿服务活动还可以适当得减少学生的课程压力（alleviate their pressure on courses）从而帮助他们获得好的学习成绩（which assists with their test performance）。因为在做志愿活动时，学生可以得到适当的休息，同时，社会活动的经历也让他们明白，只有通过努力，才能让自己立足在这个充满竞争的社会中（only by making painstaking efforts on study can they keep a favorable position in the society full of fierce competition），因此学生就会在学习上取得好的成绩。 - Rather than help their children do schoolwork, parents should encourage their children do their homework independently. 培养他们解决问题的能力 建立自信和成就感，不怕困难 (Self-confidence and fulfillment) 家长参与也有好处，帮助他们集中精力，但是不应帮助做作业，不然他们成长的空间就被剥夺了 be deprived of - At some universities, students take part in making decisions about the issues that affect daily life of everyone on campus, such as how many hours that the libraries should be open each day or what kinds of food should be served in the cafeteria. But at some universities, experts are hired to make these decisions, students almost never involved.Which approach do you prefer and why. To start with, when students consider about issues related to school life, such as library operating hours or dining options, they might only think about their own needs and preference while at the same time, some more relevant factors will be overlooked. 学生一般来讲比较考虑好处，很少考虑安全问题 学生和老师都很重要，学生考虑会忽略老师的意见 Furthermore, experts are more professional. 学生的定见能够被部分考虑，由于是直接关系学生日子的决议，但不应该要学生彻底做决议。 学生提出意见，专家进行决策 - Do you agree that it is better to work for business owned by someone else than to work for the business of one‘s own familyObviously, the benefits of working for a business owned by someone you do not know outweigh its disadvantages. 自由 社交网络 To start with, to work in an irrelevant company gives you a lot of freedom of choice since personal emotions and family relationship will not be taken into consideration when you make decisions. And besides, working for a business owned by strangers provides you a bunch of chances to expanding your social network instead of working with a lot of relatives. Admittedly, working in a business of your own family do facilitate your adjustment to the company because your relatives will help you know about how the whole system work and maintain quickly. However, if viewed from another angle, this point of view seems not plausible enough for one to choose family business, because if you have favorable communication ability, one could become acquaintance with colleagues and they are willing to help too. To sum up, facing the choice between working in family business and competing independently in job market, people should opt for the latter one. - Students aged 13-18 are taught different subjects by different teachers while younger students are taught by only one teacher all day long. Some people suggest it would benefit young students to be taught by different teachers.Do you agree with this view? Why or why not? 老师精力有限 学生注意力难集中 I agree with this statement that younger students will benefit from being taught by several different teacher every day. Teachers have limited time and energy. If they are to teach students all day long, they will be too tired to guarantee the teaching quality. 老师的时间和精力有限。如果一整天都不休息，一直给学生上课，这样老师会很累，教学效果也不好。 It is much harder for younger students to concentrate themselves. If they are taught by only one teacher all the day, the newness (the feeling of freshness and attraction) will soon wears off and they will feel bored, shifting their attention to something more interesting. 年纪更小的学生更难集中注意力。如果一整天都是一个老师在上课，学生很快就没有新鲜感，觉得无聊，注意力很快就会转移到其它更有趣的地方。 有的人说一个老师更容易产生依赖。但这是好事吗？ All in all, I do believe that elementary students should also be taught different subjects by different teachers. 总之，我认为，位于小学教育阶段的学生也应该由不同老师来教授不同科目。 http://www.sohu.com/a/159375812_292611 http://www.kekenet.com/toefl/201710/523980_2.shtml - If one of your friends has the opportunity to study in either one of two majors, which one of the following majors you will recommend:(1)A major that would allow your friend to complete studies and get a degree faster (so that he or she can get a full timejob sooner) (2)A major that would require longer time to study, but can make it more likely to get better employment opportunities and job offers in the future. Use specific reasons and examples to support your answer. “ 在学校是学习的好机会，机会十分难得 可以直接进入好公司，避免走弯路 找到工作早不代表能够拿到更高的收益 - A lot of high school students now cheat in homework assignments, by asking other students for answers.-asking parents to help stop the students from cheating -penalty or punishment to the students -asking teacher to create homework assignment that cannot be easily cheated” Which of the following do you think is the most efficient way to stop? Now, the education institutions are in a stage that cheating as a serious problem become a press matter of the moment. So, this phenomenon triggers a grave discussion on how to oppose cheating. In my opinion, The second one, which is stepping up efforts to penalty to the cheating students. To start with, if schools increase the punishment, students would not dare to cheat once they recognize that they have to pay a high price to do so, and school will receive quick returns with this method. Furthermore, this kind of methods can help cultivate students the sense of integrity and foster their conception of abidance of the rules whether in a school or in a society. Last but not least, it is difficult either for parents to effectively supervise children’s behavior or for teachers to design homework that is impossible or difficult to cheat. From what we discussed above, we can readily reach the conclusion that to cope with there increasing number of students who cheat in homework, school’s best choice is to impose the severe penalty on those who dare to cross the line. This is the most effective method to force them to willingly regulate behaviors by themselves. https://www.sohu.com/a/193435143_292611 http://www.xuexila.com/yc/3881162.html - Most adults believe that modern children (5-10 years old) behave worse than those in the past. What action parents should take do you think will have the most positive effect on children to help them be have better like respecting and treating others kindly?(1)limit the types of the TV programs and movies they watch (2)spend more time talking with children (3)supervise and monitor children while they are playing with their friends Use specific reasons and examples to support your answer. In my opinion, for parents, sacrificing their spare time to actively and friendly communicate with their children is a more effective way. By spending more time deeply communicating with them, parents can better understand their children and exert positive guidance. Correspondingly, their children also tend to accept their suggest, or even criticism rather than roughly ignore it. By comparison with the second one, the other two ways in the three options are somewhat unfriendly and compulsory, which would backfire. To sum up, faced with children’s undesirable behavior habits, compared with mandatory measures, such as limiting the types of TV programs or movies, and monitoring their interactions with their friends, directly speaking to or chatting with them in person is a tender and amiable way and as well as more effective and less side-effect way. https://www.jianshu.com/p/f4b7fa82ad0c - Parents give their children weekly money to buy whatever they want. Some people think this can cause bad habits and ideas about money in children. Others think the opposite.What’s your opinion? To start with, giving children weekly money as reward is conducive to motivate them to study hard. Besides, financial thinking as an important skill is useful to the modern society, meaning that parents ought to teach the children how to manage their own money. However, it is unwise to give too much money once a time when children get a good grade, since that will miss lead the children to be money oriented. But the goal is to encourage them to work hard and put a little pressure on them and at the same time make sure they are not going to be luxury. I will be fun to do so. - In the past people ate food that was better for their health than they do today.Do you agree or disagree with the following statement? Use specific reasons and examples to support your answer. 原材料更健康，绿色 饮食结构更健康，没有垃圾食品 选择更多，不代表更健康，相反我们会选择更想吃的，而不是更健康的 - Which area the government should fund to improve children’s education?(1)hiring more teachers to teach in a small class (2)preschool education before kinder-garten (3)providing some training courses so that teachers can be more professional 对性格影响大 学习基础知识 Preschool education can help to mold children’s character Preschool education can be conducive to the cultivation of some basic but critical abilities for children, which are the prerequisite capabilities to the future success. Certainly, hiring more teachers can be beneficial to children’s better development and providing training classes can enhance teacher;s professional skills. But both of them can not be seen as the fundamental approach to improve the children’s education. 然而，教育的主体从来都是学生，而学前教育恰恰是一个人一生中最重要的性格、能力塑造和培养阶段，所以普及学前教育才能从根本上让学生甚至整个社会受益。 - People in daily lives would frequently do the jobs that need creativity, such as the job you have never done before.Under this circumstance, do you prefer to work alone or work with others? 一起做有利于头脑风暴 一起做有利于找到漏洞 虽然有冲突，但是找性格做事相似的就OK - when you decide to find a place to travel and want to compare two places,-read information online -discussing with a friend who has been to those places which way do you think help you make a better decision? 上网非常全面可以得到很多信息 上网可以指定搜索自己想要看到的内容 虽然和朋友可以讨论，但是上网也可以和网友讨论 - You may choose between two professors who will be teaching a course that you must take at your university. If the following statements are the only information available to you about the differences between the two professors, A professor who proves to be very popular in students’ evaluations A professor who was recently given an award for outstanding research. which professor would you choose? Why? 亲和的教授更容易给予你学习的动力 亲和的教授说明他传授知识的能力更强 - In order to get a higher promotion and salary, many people chose to improve their job performance in two ways. Which one do you prefer? Why? -To do additional work and assignments -To actively participate in the group work 能够有更多的机会与领导合作，留下深刻的印象。 能够建立自己的交集网，得到同事的支持，在晋升的时候更有把握 自己做工作好，但是有时候可能自己做的工作不被别人看见，不是一个最有效的方法 - Do you agree or disagree with the following statement:The use of devices that can be connected to the internet, like computers, phones and ipads, should be prohibited from the classroom. Use specific reasons and examples to support your answer. First, they can allow for a wider variety of programming to appeal to more students. Furthermore, devices with internet access are now an integral part of modern society, so learning to use them effectively is essential. http://www.jxphgs.cn/toefl/write_moniti/625046/ - In order to be successful, people have two choices(1)To take risks (2)To keep cautious and careful Which one do you think is the better way to succeed? Give reason to explain your choice. From ancient time to the present, 小心和谨慎是两个必不可少的因素 小心可以帮助我们很好的发现机会 谨慎可以即使止损和发现错误 冒险可能可以发一笔横财，终归无法持续，终会失败 - A government spends money on all adults after 25-year-old on a training course for the most up-to-date skills at workplace. Do you think it is effective？Why or why not？ A society needs a competent workforce equipped with up-to-date skills. Otherwise, it will be unproductive and fall behind the time. However, when the administration spends money on a nationwide work skills development training program where eligible trainees are only adults at the age of 25 or older, it should not expect the desired return. First, the age of 25 may be a late point of start. Instead, an earlier age would be far desirable. Furthermore, the nationwide scale is too wide to be pragmatic. From what we have discussed above, we can readily draw a conclusion that I hardly can be optimistic about the effectiveness of the program. I think it is more likely to be a failure or a borderline pass than to be a success program. 机经提纲 - 9.7- In order to be successful, people have two choices To take risks To keep cautious and careful Which one do you think is the better way to succeed? Give reason to explain your choice. From ancient time to the present, 小心和谨慎是两个必不可少的因素 小心可以帮助我们很好的发现机会 谨慎可以即使止损和发现错误 冒险可能可以发一笔横财，终归无法持续，终会失败 - When you decide to find a place to travel and want to compare two places, which way do you think help you make a better decision? reading information online discussing with a friend who has been to those places 上网非常全面可以得到很多信息 上网可以指定搜索自己想要看到的内容 虽然和朋友可以讨论，但是上网也可以和网友讨论 - It’s the best way for teachers to help students become more interested in a subject by explaining how this subject can help students live better outside of the school. 学生不愿意听 可以，但不是最好，给奖学金也是个选择 https://wenku.baidu.com/view/98eb54d2b1717fd5360cba1aa8114431b90d8ed8.html - Parents give their children weekly money to buy whatever they want. Some people think this can cause bad habits and ideas about money in children. Others think the opposite. What’s your opinion？ To start with, giving children weekly money as reward is conducive to motivate them to study hard. Besides, financial thinking as an important skill is useful to the modern society, meaning that parents ought to teach the children how to manage their own money. However, it is unwise to give too much money once a time when children get a good grade, since that will miss lead the children to be money oriented. But the goal is to encourage them to work hard and put a little pressure on them and at the same time make sure they are not going to be luxury. I will be fun to do so. - Rather than help their children do schoolwork, parents should encourage their children do their homework independently. To start with, encouraging children to do their own their independently is conducive to cultivating self-confidence. Besides, the best way to teach children responsibility is to encourage them to finish tasks on their own. Admittedly, in some cases, parents should not hesitate to help their children due to the consideration of safety. - Do you agree that it is better to work for business owned by someone else than to work for the business of one‘s own family.Obviously, the benefits of working for a business owned by someone you do not know outweigh its disadvantages. To start with, to work in an irrelevant company gives you a lot of freedom of choice since personal emotions and family relationship will not be taken into consideration when you make decisions. And besides, working for a business owned by strangers provides you a bunch of chances to expanding your social network instead of working with a lot of relatives. Admittedly, working in a business of your own family do facilitate your adjustment to the company because your relatives will help you know about how the whole system work and maintain quickly. However, if viewed from another angle, this point of view seems not plausible enough for one to choose family business, because if you have favorable communication ability, one could become acquaintance with colleagues and they are willing to help too. To sum up, facing the choice between working in family business and competing independently in job market, people should opt for the latter one. - Most adults believe that modern children (5-10 years old) behave worse than those in the past. What action parents should take do you think will have the most positive effect on children to help them be have better like respecting and treating others kindly? Limit the types of the TV programs and movies they watch Spend more time talking with children Supervise and monitor children while they are playing with their friends In my opinion, for parents, sacrificing their spare time to actively and friendly communicate with their children is a more effective way. By spending more time deeply communicating with them, parents can better understand their children and exert positive guidance. Correspondingly, their children also tend to accept their suggest, or even criticism rather than roughly ignore it. By comparison with the second one, the other two ways in the three options are somewhat unfriendly and compulsory, which would backfire. To sum up, faced with children’s undesirable behavior habits, compared with mandatory measures, such as limiting the types of TV programs or movies, and monitoring their interactions with their friends, directly speaking to or chatting with them in person is a tender and amiable way and as well as more effective and less side-effect way. https://www.jianshu.com/p/f4b7fa82ad0c - High school students should be required to study many different subjects at same time or they should study only three or four subjects at a time.Teenagers are in the prime time of their whole life. What they learn in this period will become a huge factor that determining their whole life to a great extent. In order to better use this golden time, some education institution advocate that students are supposed to learn as many subjects as possible at this time. However, I don’t think it is a smart move to do so. Instead, studying three or four subjects at a time would serve them better. To start with, only three or four subjects at a time will ensure that students have enough time and energy for each of subjects to make further research to gain the thorough and deep understanding. And besides, when students are allow to choose three or four subjects at a time based on their own interest, they would perform better without under the huge amount of mental pressure. What can not be denied is that there are such versatile geniuses who can easily cope with many subjects at one time without any difficulty. However, only a small fraction of students have this talent. For the most of the high school students, only pursue the quantity of subjects not quality will definitely backfire. http://toefl.xdf.cn/201611/10567155.html - Students in a university club want to help others, but they can only choose one project a year, which one of the following is the best? help those students in a nearby primary school with reading and mathematics help people who cannot afford to build or rent a home to build a house visit and assist elderly people with daily tasks The student club is the critical constituent part of a high educated society, because it is the tower of strength to link the university to the community. The highest standard for a student club is serving the community well and at the same time could improve the students ability. To start with, by designing and constructing a building, the students from different majors will learn to transfer their academic knowledge to practice. And besides, offering primary school students help in reading and mathematics and taking care of the elder are not easy for students in universities and will take a lot of time of their prime time. To sum up, since the main purpose of students to attend this university club is to improve themselves and at the same time serve the community. So building houses is a project that can better yield tangible results. http://www.kekenet.com/toefl/201603/432562_3.shtml - Some teenagers take part in kinds of activities,such as musical classes,sports classes and so on,but others only focus on one activity which is important to them. Which idea do you support?Nowadays, with the booming attention to children’s education, we are undergoing a stage where children’s extra-curriculum are playing significant role. Thus, this phenomenon triggers an grave concern about whether teenagers should take part in this kind of activities such as musical and sports. In my opinion, it is very necessary for teenagers to be engaged in this variety activities. And my reasons and examples are given below. To start with, getting involved in more activities can be conducive to the well-rounded development of students. It is self-evident that different activities can equip participants with various skills and abilities. To illustrate this point of view, students attending musical concerts can learn how to appreciate the melodies and concertos of musical masterpieces and thus improve their aesthetic capability. Similarly, the team sports will help students cultivate a sense of cooperation and collective sense of honor. So, after having obtained the above skills, students will definitely can develop in an all-rounded way, with the result if becoming more competitive than others in the society. In contrast, specializing in only one kind of activity is a totally different picture. This kind of cultivation sometimes will backfire. It means that, if one is only be fostered in one field, his future development will face potential obstacles and difficulties. And the underlying reason is that the more and more complicated modern life necessitate the comprehensive prerequisite. Furthermore, taking part in many after-class activities can do better job in helping students find what they are really interested in. All in all, we can draw a conclusion that it is a wiser move for students to attend multiple activities in different fields, in order to obtain a holistic development and find what fascinate them most. http://toefl.zhan.com/tfziliao62893.html - The government can take a variety of actions to help protect the environment. Which one of the following do you think is the most important for the nation’s government to take to protect the environment. Fund the research to develop environmentally friendly energy sources such as solar and wind energy. Preserve the natural places like forests and protect the animals that live there. Enforce laws to prevent the pollution of air and water by large companies. Taking a panoramic view of human history, we can readily find that the natural environment plays and enormously important role in determining the future of each and every country. To start with, spending money in developing new energy which is friendly to environment can radically solve the various problems triggered by environment changes. 影响工业 影响环境同样影响人 respiratory disease Furthermore, there are conspicuous limitation of the other two options, because both of them will bring some notable side-effect which possibly will backfire. 保护动物只能暂时解决问题，不是长远之计 并不是所有国家都有实力不排放并且保持经济增长，保护环境损失经济，同样不可取 From what has been discussed above, we can safely draw a conclusion that funding research of environmental friendly energy will more preferable, because not only it is the pragmatic key to solving the environmental problems comprehensively, but also has little side-effect. http://www.kekenet.com/toefl/201802/539637_2.shtml - A city wants to help teachers of its high school students (age 14-18)improve their teaching. It is considering two plans: Choose a small group of excellent teachers; these teachers will attend a class led by an expert for additional training in how to teach effectively, and they will then come back to their schools and provide that training for other teachers in school. Provide additional training in teaching effectively for high school teachers,using online material that each teacher will study individually. Nowadays, with the booming attention to the education, will are on the stage that improving the teachers skill become a pressing matter of the moment. Thus, this kind of situation triggers a grave concern about which way to improve teacher’s skill is pragmatic and feasible. In my opinion, I incline to state that the best approach to enhance training of teachers is to provide every teacher with the training courses through the internet. To start with, face-to-face training courses could only serve a small scale of teachers and information passed on by them to other teachers might be incomplete or even worse because of inattention or forgetfulness, beside, these group of teacher also need time to absorb what they have learned from the courses. Furthermore, the online training course is more convenient and more practical for teachers. Some educational experts claim that face-to-face training is superior to online course because they allow all the attendee to discuss afterwards. However , if viewed from another angle, actually, online course is capable of this also. So, from what has been discussed above, we can safely reach the conclusion that it is more advisable to provide training to all the teacher online since it is more pragmatic and more effective than face-to-face training provided bu experts. https://www.douban.com/note/628011447/ http://www.kekenet.com/toefl/201709/523950_2.shtml https://www.testbig.com/independent-toefl-writing-essays/city-wants-help-teachers-its-high-school-students-ages-14-18 - A lot of high school students now cheat in homework assignments, by asking other students for answers. Which of the following do you think is the most efficient way to stop? asking parents to help stop the students from cheating penalty or punishment to the students asking teacher to create homework assignment that cannot be easily cheated Now, the education institutions are in a stage that cheating as a serious problem become a press matter of the moment. So, this phenomenon triggers a grave discussion on how to oppose cheating. In my opinion, The second one, which is stepping up efforts to penalty to the cheating students. To start with, if schools increase the punishment, students would not dare to cheat once they recognize that they have to pay a high price to do so, and school will receive quick returns with this method. Furthermore, this kind of methods can help cultivate students the sense of integrity and foster their conception of abidance of the rules whether in a school or in a society. Last but not least, it is difficult either for parents to effectively supervise children’s behavior or for teachers to design homework that is impossible or difficult to cheat. From what we discussed above, we can readily reach the conclusion that to cope with there increasing number of students who cheat in homework, school’s best choice is to impose the severe penalty on those who dare to cross the line. This is the most effective method to force them to willingly regulate behaviors by themselves. https://www.sohu.com/a/193435143_292611 http://www.xuexila.com/yc/3881162.html - Which area the government should fund to improve children’s education? hiring more teachers to teach in a small class preschool education before kindergarten providing some training courses so that teachers can be more professional Preschool education can help to mold children’s character Preschool education can be conducive to the cultivation of some basic but critical abilities for children, which are the prerequisite capabilities to the future success. Certainly, hiring more teachers can be beneficial to children’s better development and providing training classes can enhance teacher;s professional skills. But both of them can not be seen as the fundamental approach to improve the children’s education. 然而，教育的主体从来都是学生，而学前教育恰恰是一个人一生中最重要的性格、能力塑造和培养阶段，所以普及学前教育才能从根本上让学生甚至整个社会受益。 - Which one do you think is the most important fact or that affect the lasting time of friendship? help each other when one is in emergency have the same interests trust each other completely Friendship is a huge constituent part of a wonderful life, and everyone long for a long-lasting time of relationship with friends. So, this kind of aspiration triggers an interesting discussion about what is the most significant factor in determining the last time of friendship. In my opinion, I incline to state that the common interest is the vital key to a long lasting relationship. To start with, having the same interests is the basic guarantee to have mutual interest topic to talk to each other, which definitely will lead to many memorable experiences. Furthermore, it seems that helping others in emergency and mutual trust are also related to the lasting time of friendship. But the fact is both of them are not the fundamental reason for this, instead, they are the superficial causes. Beyond these superficial causes, the fundamental cause could trace back to the common interest. Because spending a lot of time together is the prerequisite to a well developed friendship which lead to further help in emergency and mutual trust. From this point of view, helping each other when one is in emergency and trusting each other completely are the result of a long lasting friendship, not the factor of the lasting time of the friendship. - A government spends money on all adults after 25-year-old on a training course for the most up-to-date skills at workplace. Do you think it is effective？Why or why not？A society needs a competent workforce equipped with up-to-date skills. Otherwise, it will be unproductive and fall behind the time. However, when the administration spends money on a nationwide work skills development training program where eligible trainees are only adults at the age of 25 or older, it should not expect the desired return. First, the age of 25 may be a late point of start. Instead, an earlier age would be far desirable. Furthermore, the nationwide scale is too wide to be pragmatic. From what we have discussed above, we can readily draw a conclusion that I hardly can be optimistic about the effectiveness of the program. I think it is more likely to be a failure or a borderline pass than to be a success program. - If you are going to graduate from the university and have to choose the final course, which professor will you choose？The one you used to sign up for courses or the one you have never learned from before？Everyone wants an happy ending of the university period, and the key factor of it is the final course. As for me, I will definitely choose the professor that once I used to sign up for courses. My reasons are given bellow. 从务实的角度来讲，我熟悉老师的课程内容，授课方式，能够更好的学习 从和老师沟通的角度来讲，我可以选择一个和我合得来的老师，这样轻松愉快的结束大学生涯。 - Some people have ambitious dreams and keep pursuing them, but other people always focus on realistic goals and try to achieve them. Which do you think is better and why?In all ages, human never give up on discussing the controversy that about whether to pursue ambitious dreams or to focus on realistic goals. It is common that young people are easy to get excited with the dream and impulsively pursuing the dream. However, I insist that people should always focus on the realistic goals. My reasons and examples are given below. An ambitious goal may be a great rallying cry, but it is hard to sustain the motivation of pursuing one over a long time, because eventually you will get tired and realize that the goal is to hard to reach and finally you will stop striving for it and the process will slide into stagnant. Achievable and realistic goals are good indication of a huge plan, and it can provide satisfaction and motivation when one achieve them. From what we discussed above, we can safely draw a conclusion that achievable goals are therefore superior to ones which are too extreme huge because they are not only offer you a distinct record of the process you have made towards a larger plan, but also provide the motivation which could prevent you from sliding into stagnant. http://bj.xdf.cn/zuowen/liuxue/toefl/duli/45122.html https://www.jupeixun.cn/news/81710.html http://www.kekenet.com/toefl/201808/558336_2.shtml - In order to get a higher promotion and salary, many people chose to improve their job performance in two ways. Which one do you prefer? Why? To do additional work and assignments To actively participate in the group work 能够有更多的机会与领导合作，留下深刻的印象。 能够建立自己的交集网，得到同事的支持，在晋升的时候更有把握 自己做工作好，但是有时候可能自己做的工作不被别人看见，不是一个最有效的方法 - As a student of university that has a long break between university semesters, the university requires all students to do one of the following for one month during the break: students must take a course on the subject that has no direct connection to their majors of study. students must volunteer to work in the city where the university is located or their hometowns to improve some aspects of life of the city or their own town. which one do you think is more beneficial for students in their university. Why? Schools should require students to volunteer on the projects in the university’s city or hometown. 参加志愿服务活动可以培养学生的社会责任感（cultivate the sense of responsibility）。因为在做志愿服务的过程中，会帮助那些需要帮助的人，或者是帮助一些机构解决问题，比如美化环境，清扫街道等，这会让学生感觉自己是社会的一份子，这可以让他们实现个人的社会价值。 参加志愿服务活动还可以促进学生的社交能力 (facilitate their social interaction)。因为在活动的过程中他们需要学会如何与周围的人相处得融洽（get along well with other people），如何表达得加得体（how to express properly），如何解决棘手的问题（how to deal with thorny problems）。 参加志愿服务活动还可以适当得减少学生的课程压力（alleviate their pressure on courses）从而帮助他们获得好的学习成绩（which assists with their test performance）。因为在做志愿活动时，学生可以得到适当的休息，同时，社会活动的经历也让他们明白，只有通过努力，才能让自己立足在这个充满竞争的社会中（only by making painstaking efforts on study can they keep a favorable position in the society full of fierce competition），因此学生就会在学习上取得好的成绩。 - At some universities,students take part in making decisions about the issues that affect daily life of everyone on campus, such as how many hours that the libraries should be open each day or what kinds of food should be served in the cafeteria. But at some universities,experts are hired to make these decisions, students almost never involved. Which approach do you prefer and why. To start with, when students consider about issues related to school life, such as library operating hours or dining options, they might only think about their own needs and preference while at the same time, some more relevant factors will be overlooked. 学生一般来讲比较考虑好处，很少考虑安全问题 学生和老师都很重要，学生考虑会忽略老师的意见 Furthermore, experts are more professional. 学生的定见能够被部分考虑，由于是直接关系学生日子的决议，但不应该要学生彻底做决议。 学生提出意见，专家进行决策 http://www.sohu.com/a/198299841_443514 - Students aged 13-18 are taught different subjects by different teachers while younger students are taught by only one teacher all day long. Some people suggest it would benefit young students to be taught by different teachers. Do you agree with this view? Why or why not?I agree with this statement that younger students will benefit from being taught by several different teacher every day. Teachers have limited time and energy. If they are to teach students all day long, they will be too tired to guarantee the teaching quality. 老师的时间和精力有限。如果一整天都不休息，一直给学生上课，这样老师会很累，教学效果也不好。 It is much harder for younger students to concentrate themselves. If they are taught by only one teacher all the day, the newness (the feeling of freshness and attraction) will soon wears off and they will feel bored, shifting their attention to something more interesting. 年纪更小的学生更难集中注意力。如果一整天都是一个老师在上课，学生很快就没有新鲜感，觉得无聊，注意力很快就会转移到其它更有趣的地方。 有的人说一个老师更容易产生依赖。但这是好事吗？ All in all, I do believe that elementary students should also be taught different subjects by different teachers. 总之，我认为，位于小学教育阶段的学生也应该由不同老师来教授不同科目。 http://www.sohu.com/a/159375812_292611 http://www.kekenet.com/toefl/201710/523980_2.shtml - If one of your friends has the opportunity to study in either one of two majors,which one of the following majors you will recommend: A major that would allow your friend to complete studies and get a degree faster(so that he or she can get a full time job sooner) A major that would require longer time to study, but can make it more likely to get better employment opportunities and job offers in the future Use specific reasons and examples to support your answer. 高起点能够带来巨大优势 可以直接进入好公司，避免走弯路","link":"/Blog/2019/09/18/Study-Notes-of-TOEFL-Writing-Part-Outline-of-Real-Test-Questions/"},{"title":"TPU 与 GPU 的未来竞争格局态势","text":"本文基于 SemiAnalysis 2025 年 11 月报告，聚焦谷歌 TPU 的技术升级、商业化进展及产业链布局，分析其对 AI 硬件竞争格局的影响。核心围绕 TPU v7 的性能突破（逼近英伟达 GPU）、成本优势（TCO 更低、利润率更高）、ICI 架构的扩展性创新，结合谷歌与 Anthropic、WULF 等的关键交易，阐述 TPU 从内部使用走向全面商业化的转变。同时梳理了 TPU 产业链生态，对比英伟达 GPU 生态，凸显谷歌在 AI 算力硬件领域的差异化竞争力，预示其将成为英伟达在 AI 训练 / 推理硬件市场的核心竞争对手。 核心观点【商业逻辑】 一、短期内，谷歌 TPU 外售的商业化落地为云服务提供商（CSP）提供了潜在 的替代选项，强化了 CSP 对英伟达的议价能力。 据 SemiAnalysis 报，OpenAI 以转向 TPU 采购为谈判条件，成功从英伟达获得约 30%的 GPU 采购折扣。 但当前冲击主要集中在议价环节，尚未形成实质性的份额替代。 二、除了 TPU 惯有的高性价比、扩展性、灵活性优势外，谷歌着重优化了 TPU 生态，大幅提升了外部可用性。 谷歌 2025 年加速优化了TPU 生态，原生支持 PyTorch，并在 vLLM 的 TPU 支持上进行大规模工程投入，接入开放推理生态，大幅提升 TPU 的外部可用性； TCO 优势突出，TPUv7 内部使用时 TCO 较 GB200 服务器低 44%，对外租赁时 TCO 较 GB200 低 30%、较 GB300 低 41%； 集群扩展性及灵活性领先，集群通过 ICI 3D Torus 网络支持最大 9216 颗芯片，OCS 技术实现数千种拓扑组合，适配多样并行需求且故障可快速重构。 三、TPU 对谷歌更为重要的意义在于构建全栈 AI 生态，而非出售 TPU 本身：通过芯片与模型架构协同设计，实现算力成本与效率最优，并赋能云业务，利用较低成本的 TPU 赚取高于其他云服务商的利润。长期竞争格局来看，TPU 完全颠覆英伟达 GPU 的概率较小，而较大概率作为英伟达 GPU 的补充，服务特定属性的客户群体： 英伟达凭借规模优势深度绑定供应链，在获取供应链资源方面具备最强的优先级和议价权； 谷歌 XLA 编译器、运行时代码仍未完全开源，导致 外部开发者在调试优化时面临较高技术门槛，尤其对缺乏定制化开发能力 的中小客户而言，适配成本显著高于 GPU。 TPUv8 升级幅度有限，而英伟达 Rubin 系列升级显著，缩小了 TCO 差距，且英伟达过去已证明了一年一迭代的能力，后续 Feynman 接力 Rubin 维持一年一迭代节奏，英伟达技术领先性有望持续领跑。TPU v8 设计较为保守，整体性能提升较为温和，沿用 HBM3E 内存，在 TPU v8AX 上提供 9.8TB/s 的带宽；英伟达 Rubin 采用 HBM4 内存，带宽提升至 20TB/s，功率从最初计划的 1800W 激进提升至 2300W（提升28%），缩小了和 TPU v8 的 TCO 差距。应该看到，英伟达已建立稳定的 一年一迭代节奏，持续保持技术代差优势。 【谷歌路线以及 TPU 的技术优势】 一、关于谷歌的路线：Anthropic 交易标志着这一努力中的一个重要里程碑：谷歌云 CEO 托马斯·库里安在谈判中发挥了核心作用。谷歌很早就做出了承诺，积极投资 Anthropic 的融资轮次，甚至同意放弃投票权，并将其所有权上限设定为 15%，以扩大 TPU 在谷歌内部之外的使用。 TPU 集群长期以来一直与英伟达的 AI 硬件不相上下，但它主要支持谷歌的内部工作负载。按照谷歌的典型风格，即使在 2018 年向谷歌云平台（GCP）客户开放 TPU 之后，也从未将其完全商业化。这种情况正开始改变。在过去几个月里，谷歌动员了整个技术栈的力量，通过谷歌云平台将TPU提供给外部客户，或者作为商业供应商销售完整的TPU系统。由于基础实验室中有前 DeepMind 的 TPU 人才，这一策略的实施变得更加顺利，这使得 Anthropic 能够在包括 TPU 在内的多种硬件上训练 Sonnet 和 Opus 4.5。 谷歌已经为 Anthropic 建造了一个大型设施。 二、成本优势显著，与英伟达形成差异化优势 TPU 技术迭代与性能追赶：设计理念转向大语言模型优化，TPU v7（Ironwood）性能逼近英伟达旗舰 GPU：采用 N3E 工艺，HBM3E 容量 192GB、带宽 7380GB/s，FP8 算力 4614 TFLOPs，与英伟达 GB200 差距缩小，上市时间仅晚几个季度；从 v4 到 v7，算力、带宽持续提升，功耗优化，v6 相比 v5p 算力翻倍。 成本效益优势显著：TPU v7 总拥有成本（TCO）大幅低于英伟达 GB200/GB300，内部版每小时仅 1.28 美元（GB200 为 2.28 美元）；FP8 算力、内存带宽、内存容量的单位 TCO 均优于英伟达，息税前利润率高于多数 GPU 云交易，成为 GCP 差异化优势。 三、谷歌的 ICI 扩展网络，是英伟达 NVLink 唯一真正的竞争对手。 ICI 3D Torus 架构核心优势：以 64 个 TPU 组成的 4x4x4 立方体为基本单元，支持 3D 环面扩展，最大规模达 9216 个 TPU；通过铜缆 + 光收发器 + OCS 实现连接，具备超大规模、高可重构性（支持数千种拓扑）、立方体可替代性、低成本（比同类交换网络省钱）、低延迟等优势。 谷歌与 Anthropic 的交易细节交易细节该交易的第一阶段涉及 40 万个 TPUv7 Ironwoods，成品机架价值约 100 亿美元，博通将直接向 Anthropic 出售这些产品。 Anthropic 是博通在最近一次财报电话会议中提到的第四个客户。Fluidstack 将负责现场安装、布线、老化测试、验收测试以及远程运维工作，Anthropic 则将物理服务器的管理工作外包出去。数据中心基础设施将由 TeraWulf（WULF）和 Cipher Mining（CIFR）提供。 剩余的 60 万个 TPUv7 单元将通过谷歌云平台（GCP）出租。 预估，这笔交易的已签约未交付订单（RPO）为 420 亿美元，占谷歌云平台第三季度报告的 490 亿美元积压订单增长的大部分。 未来几个季度，与 Meta、OAI、SSI 和 xAI 达成的额外交易可能会为谷歌云平台（GCP）带来更多的 RPO 以及直接的硬件销售。 WULF Compute 与 Fluidstack 的交易，以及谷歌**虽然其他超大规模企业已经扩大了自己的场地并获得了大量的托管空间，但谷歌的行动则更为迟缓。核心问题在于合同和管理方面。**每一个新的数据中心供应商都需要一份主服务协议，而这些协议涉及数十亿美元、多年期的承诺，自然会涉及一些行政流程。然而，谷歌的流程尤其缓慢，从初步讨论到签署主服务协议，往往需要长达三年的时间。谷歌的这种变通办法对那些希望转向人工智能数据中心基础设施的新云服务提供商和加密货币矿工具有重大影响。谷歌没有直接租赁，而是提供了一种信贷支持，即一种表外“欠条”，以便在 Fluidstack 无法支付其数据中心租金时介入。 这张图展示了 WULF Compute 与 Fluidstack 的交易，以及谷歌在其中的参与角色，结合信息可解读为：谷歌通过 “股权 + 信贷担保” 的方式，间接支持 Fluidstack 与 WULF 的大型数据中心租赁项目，而非直接提供硬件租赁服务。 核心交易：WULF 与 Fluidstack 的租赁合作 WULF Compute 与 Fluidstack 签订了3 份 10 年期租约，涉及 Lake Mariner 数据中心的 CB-3、CB-4 及新增的 CB-5 项目，合计提供超 360MW 的关键 IT 负载（CB-5 单项目就新增 160+MW）。 合同价值：初始 10 年期约 67 亿美元；若行使 10 年延期选项，可额外增加约 90 亿美元收入。 交付节点：360+MW 的合同容量预计 2026 年底前交付。 谷歌并未直接向 Fluidstack 提供硬件租赁，而是通过两种方式参与： 持股支持：持有 WULF 约 14% 的股份，该持股结构是为了在建设期间向贷款人提供保障。 信贷担保：为 Fluidstack 的债务提供32 亿美元的担保，以支持项目的债务融资。 Ironwood 已接近 BlackwellTPU 设计理念明显转变迎来大语言模型时代后，谷歌的TPU设计理念发生了明显转变。可以从最近两代专为大语言模型设计的 TPU 中看到这一点：TPUv6 Trillium（Ghostlite）和TPUv7 Ironwood（Ghostfish）就体现了这种变化。 从下面的图表中可以看到，TPU v4 和 v5 的计算吞吐量远低于当时英伟达的旗舰产品。TPU v6 在浮点运算性能上非常接近H100/H200，但它比 H100 晚推出了两年。到了 TPU v7，差距进一步缩小，其服务器仅晚几个季度上市，同时提供的峰值理论浮点运算性能几乎达到了同一水平。 TPU v6 Trillium 与 TPU v5p 采用相同的 N5 Node，硅片面积相近，但峰值理论浮点运算能力却惊人地提升了两倍，同时功耗显著降低。 对于Trillium，谷歌将每个脉动阵列的规模从128×128 瓦片扩大到 256 × 256 瓦片，扩大了四倍，这种阵列规模的增大带来了计算能力的提升。 TPU v7 Ironwood 是下一个迭代版本，谷歌在浮点运算能力、内存和带宽方面几乎完全缩小了与英伟达相应旗舰GPU的差距，尽管其全面上市时间比 Blackwell 晚了一年。与 GB200 相比，其浮点运算能力和内存带宽仅略有不足，配备 8-Hi HBM3E 时的容量与 GB200 相同，当然，这与配备 12-Hi HBM3E、容量为 288GB 的 GB300 相比存在明显差距。 理论上的绝对性能是一回事，但重要的是 每总拥有成本（TCO Total Cost of Ownership）的实际性能。 每总拥有成本（TCO Total Cost of Ownership）的实际性能虽然谷歌通过博通采购张量处理单元（TPU）并支付高额利润，但这远低于英伟达的利润，英伟达不仅在其销售的图形处理器（GPU）上获利丰厚，在包括中央处理器（CPU）、交换机、网络接口卡（NIC）、系统内存、线缆和连接器在内的整个系统上都能赚取高额利润。从谷歌的角度来看，这使得全3D环面配置下每块Ironwood芯片的总拥有成本（TCO）比 GB200 服务器的总拥有成本低约 44%。 这远远弥补了峰值 FLOPs 和峰值内存带宽约 10% 的缺口。这是从谷歌及其采购TPU服务器的价格角度来看的。 谷歌 TPU v7 的成本效率显著高于英伟达 GB200/GB300：虽然 TPU 的标称算力、带宽略低，但单位算力、内存对应的总成本（TCO）更低，尤其在 FP8 精度场景下优势明显；而英伟达的 FP4 高算力仅在特定场景有价值，但 TPU 不支持原生 FP4。 核心成本数据（每小时 / 每单元） 单位小时总成本： TPU 显著更低 —— 内部 TPU 仅$1.28/小时，外部TPU为 $1.60 / 小时；而英伟达 GB200 是 $2.28/小时、GB300 达 $2.73 / 小时。 资本成本占比： 英伟达（77.4%~79.0%）略高于 TPU（72.7%），说明 TPU 的运营成本占比相对更低。 “成本 - 性能” 效率（核心对比维度） FP8 算力的 TCO： TPU 内部版仅$0.28/每PFLop·小时，远低于英伟达GB200（$0.46）、GB300（$0.55）；外部TPU也仅$0.40。 内存带宽的 TCO： TPU 内部版$0.18/每TB/s·小时，大幅低于英伟达（$0.28~$0.34）。 内存容量的 TCO： TPU 内部版$6.67/每TB·小时，同样优于英伟达（$9.47~$11.87）。 TPU v7 的经济效益**TPU v7的经济效益显示出更高的息税前利润率（与其他观察到的大型GPU云交易）；只有 OCI 与 OpenAI 的合作接近这一水平。**即使考虑到博通在芯片级物料清单上的利润率叠加，谷歌仍能获得比商品化程度高得多的GPU交易高得多的利润率和回报。这正是TPU体系使谷歌云（GCP）成为真正差异化的云服务提供商（CSP）的地方。与此同时，像微软Azure这样ASIC项目举步维艰的公司，只能局限于在纯粹的商用硬件租赁业务中获得较为平庸的回报。 芯片间互连（ICI）——扩展横向扩展超大规模的关键基本组成单元：4×4×4 Cube谷歌用于 TPUv7 的 ICI 扩展网络的基本组成单元是一个由 64 个 TPU 组成的 4x4x4 三维环面。每个包含 64 个 TPU 的 4x4x4 立方体对应一个包含 64 个 TPU 的物理机架。这是一个理想的尺寸，因为所有 64 个TPU都可以相互电连接，同时仍能容纳在一个物理机架中。 这些 TPU 以 3D 环形结构相互连接，每个 TPU 总共连接 6 个相邻节点：在 X 、 Y 和 Z 轴的每个轴上各连接 2 个逻辑上相邻的 TPU。 每个 TPU 通过计算托盘内的 PCB 线路始终与另外 2 个 TPU 相连，但根据该 TPU 在 4x4x4 Cube 中的位置，它将通过直接连接铜缆（DAC）或光收发器与另外 4 个相邻 TPU 相连。 4x4x4 立方体内部的连接通过铜缆实现，而 4x4x4 立方体外的连接（包括回绕至立方体另一侧的连接以及与相邻 4x4x4 立方体的连接）将使用光收发器和光交叉连接器（OCS）。下图展示了一个 3D 环面网络：Z+ 面上的 TPU 2,3,4 通过 800G 光收发器（800G Optical Transceiver）并经由一个 OCS，与Z-面上的 TPU 2,3,1 建立了回绕至 Z 轴对面的连接。 如上所述，除了始终通过 PCB 线路连接的 2 个相邻 TPU 外，TPU 还将根据其在 4x4x4 立方体中的位置，使用 DAC、收发器或两者的组合连接到另外 4 个相邻的TPU。 4x4x4 立方体内部的 TPU 将仅通过 DAC 连接到其他 4 个相邻 TPU，立方体表面的 TPU 将通过 3 个 DAC 和 1 个光收发器进行连接，立方体边缘的 TPU 将通过 2 个光收发器和 2 个DAC进行连接，而立方体角落的TPU将通过 1 个DAC和 3 个光收发器进行连接。要记住某个 TPU 将使用多少个收发器，只需看该 TPU 有多少个面朝向立方体的“外部”即可。 上图以及下表总结了各种位置类型的 TPU 数量，可用于得出每个 TPU v7 配备 1.5 个光收发器的连接比例。这些收发器与光电路交换机（OCS）相连，光电路交换机可实现 4x4x4 立方体之间的连接。 以下是TPU v7 3D 环面机架连接组件的分类统计清单，清晰呈现不同位置 TPU 的连接配置及机架总量： 一个 64 颗 TPU 连接组件配置 TPU 类型 数量 铜缆数量 PCB 走线数量 光收发器数量 内部 TPU（立方体内部） 8 4 2 0 角落 TPU（立方体顶点） 8 1 2 3 边缘 TPU（立方体棱边，非顶点） 24 2 2 2 面 TPU（立方体表面，非棱边） 24 3 2 1 机架总连接组件数量 连接组件类型 机架总量 铜缆 80 PCB 走线 64 光收发器 96 内部 TPU 仅依赖铜缆 + PCB 走线完成机架内连接，无需光收发器；角落 / 边缘 / 面 TPU 通过光收发器实现跨机架的 3D 环面扩展，支撑大规模集群的低延迟通信。 将多个 64 TPU Cube 连接在一起谷歌的ICI扩展网络独具特色，它允许将多个 64 个 TPU 的 4x4x4 立方体以 3D 环面结构连接在一起，从而创建大规模的全局规模。TPUv7 宣称的最大全局规模为9216 个TPU，但如今，谷歌支持将 TPU 配置为多种不同的切片规模，范围从 4 个 TPU 一直到 2048 个TPU。 要了解环绕连接和立方体间连接是如何建立的，先看看如何在 4x4x4 的拓扑结构中创建一个 64 TPU 切片。 **使用由 64 个 TPU 组成的 4x4x4 单位立方体（对应一个物理的 64 TPU 机架）来构建这种拓扑结构。**4x4x4 立方体内部的所有 8 个 TPU 都可以通过铜缆与所有 6 个相邻 TPU 完全连接。如果某个 TPU 沿特定轴没有内部相邻的 TPU，它会进行环绕并连接到立方体另一侧的TPU。例如，TPU 4,1,4在Z+方向没有内部相邻的TPU，因此它会使用一个800G光收发器连接到分配给Z轴的光交叉连接（OCS），该 OCS 被配置为将此连接导向立方体的Z-侧，从而连接到 TPU 4,1,1。在Y-方向，TPU 1,1,1 会使用光收发器连接到 Y 轴 OCS，以链接到 TPU 1,4,1 的 Y+ 侧，依此类推。 4x4x4 立方体的每个面将通过 16 个不同的 OCS 进行连接：每个面的每个 TPU 对应一个 OCS。 例如，在下图中，在X+面上，TPU 4,3,2 连接到 OCS X,3,2 的输入端。OCS X,3,2 的输入端还将连接到 9216 个 TPU 集群中所有 144 个 4x4x4 立方体的 X+ 面上相同的TPU索引（4,3,2）。OCS X,3,2 的输出端随后将连接到集群中每个立方体上相同的 TPU 索引，不过这次是在X-面上——因此它将连接到集群所有144个立方体上的TPU 1,3,2。下图展示了立方体 A 的 X+ 面上的所有 16 个 TPU 如何通过 16 个 OCS 连接到立方体 B 的 X- 面上的 16 个 TPU。 这些连接使得任何立方体的“+”面都能与其他任何立方体的“-”面相连，从而在形成切片时实现立方体的完全可互换性。 有两个限制需要简要指出。首先，特定面上同一索引的TPU永远不能直接连接到不同的索引——因此 TPU 4,3,2 永远不能配置为连接到 TPU 1,2,3。其次，由于OCS本质上起到配线架的作用——连接在输入侧的TPU不能“回环”连接到同样连接在OCS输入侧的任何其他TPU——例如，TPU 4,3,2 永远不能连接到 TPU 4,3,3。因此，“+”面上的任何TPU都永远不能连接到其他任何立方体的“+”面，“-”面上的任何TPU也永远不能连接到其他任何立方体的“-”面。 96 TPU：4×4×8 Cube 进一步来看，看看如何设置一个 4x4x8 的拓扑结构。在这种配置中，通过沿 Z 轴连接两个 64 TPU 的 4x4x4 立方体来扩展切片。在这种情况下，OCS 将重新配置 TPU 4,1,4所连接的光端口，使其现在连接到 TPU 4,1,5，而不是像独立的 4x4x4 拓扑结构那样绕回连接到 TPU 4,1,1。进一步扩展来看，这两个 4x4x4 TPU立方体的每个立方体的 Z- 面和 Z+ 面将各有 16 个光连接，总共有64根光纤连接到16个 Z 轴 OCS 中。 需要提醒读者的是，下面所描绘的 A 立方体和 B 立方体不一定物理上相邻。相反，它们通过 OCS 连接，并且可能分别位于数据中心的完全不同的位置。 4096 TPU：16×16×16 Cube现在将转向一个更大的拓扑结构：16x16x16 拓扑结构，这使 TPU 数量达到 4096 个。在这个拓扑结构中总共使用 48 个 OCS 来连接 64 个立方体，每个立方体包含 64 个TPU。在下图中，每个彩色立方体代表一个包含 64 个 TPU 的 4x4x4 立方体。以右下角的 4x4x4 立方体为例，这个立方体通过 OCS 沿着 Y 轴与相邻的立方体相连。 9216 个 TPU 的最大规模是由 144 个 4×4×4 的立方体构成的，每个立方体需要 96 个光学连接，总共需要 13824 个端口。将总端口需求除以 288（每个 OCS 有 144 个输入端口和 144 个输出端口），意味着需要 48 个144×144 的 OCS 来支持这一最大规模。 为什么要使用 Google 的 ICI 3D Torus 架构？**超大规模：**最明显的优势是 TPUv7 Ironwood 支持的 9216 个 TPU的超大最大规模。尽管由于有效吞吐量降低这一缺点，9216 的最大切片规模可能很少被使用，但数千个 TPU 的切片能够且确实被普遍使用。这远远大于商业加速器市场和其他定制芯片供应商中常见的 64 或 72 个 GPU 的规模。 **可重构性与可替代性：**光交叉连接器（OCSs）的使用意味着网络拓扑本身支持网络连接的重新配置，以支持大量不同的拓扑结构——理论上可达数千种。谷歌的文档网站列出了10种不同的组合（本节前面的图片），但这些只是最常见的3D切片形状，实际上还有更多可供选择。 可重构性还为广泛多样的并行性打开了大门。在 64 或 72 个 GPU 的规模下，不同的并行组合通常局限于64的因数。而对于ICI横向扩展网络，实现能精确匹配所需的数据并行、张量并行和流水线并行组合的拓扑结构的可能性十分丰富。 开放式连接系统（OCSs）允许将任何立方体的任意“+”面与任何其他立方体的“-”面相连，这一事实意味着立方体具有完全的可替代性。任何一组立方体都可以组成切片。因此，无论出现任何故障、用户需求变化或使用情况改变，都不会阻碍新拓扑切片的形成。 **成本更低：**谷歌的ICI网络成本低于大多数可扩展交换网络。尽管由于使用了环形器，所采用的FR光学器件可能略贵一些，但网状网络减少了所需交换机和端口的总数，并消除了交换机之间连接产生的成本。 **低延迟和更好的局部性：**TPU之间使用直接链接意味着，对于物理位置彼此靠近或被重新配置为直接相互连接的TPU，可以实现更低的延迟。彼此靠近的TPU也具有更好的数据局部性。 谷歌的软件战略：拥抱开源推理生态谷歌调整了面向外部客户的软件战略，并已对其TPU团队的关键绩效指标（KPIs）以及该团队为人工智能/机器学习（AI/ML）生态系统做出贡献的方式进行了重大修改： Massive engineering effort on PyTorch TPU “native” support 对PyTorch TPU“原生”支持的大规模工程投入 Massive engineering effort on vLLM/SGLang TPU support 在 vLLM / SGLang 的 TPU 支持方面投入大量工程资源 **通过查看谷歌在各种TPU软件代码库中的贡献数量，不难发现其外部化策略。**我们可以看到，从3月开始，vLLM的贡献量有了显著增长。随后在5月，“tpu-inference”代码库被创建，这是官方的vLLM TPU统一后端，从那以后，相关活动便层出不穷。 Pytorch传统上，谷歌只在 Jax/XLA:TPU 框架（以及已停用的 TensorFlow/TF-Mesh）上提供一流支持，而将 TPU 上的 PyTorch 视为二等公民。它依赖于通过PyTorch/XLA 进行的 Lazy Tensor Graph Capture，而非提供 First-class Eager Execution Mode。此外，它不支持 PyTorch 原生分布式 API（torch.distributed.*）或 PyTorch 原生并行 API（DTensor、FSDP2、DDP 等），而是依赖于一些怪异的树外 XLA SPMD API（torch_xla.experimental.spmd_fsdp、torch_xla.distributed.spmd 等）。这给那些习惯了 GPU 上 PyTorch 原生 CUDA 后端、并尝试转向 TPU 的外部用户带来了非原生的次等体验。 10月，谷歌的“神奇队长”罗伯特·亨特在 XLA 代码库中悄然宣布，他们将从非原生的惰性张量后端转向“原生” TPU PyTorch 后端，该后端默认支持即时执行，并将集成 torch.compile、DTensor 以及 torch.distributed 等应用程序接口。他们将通过使用 PrivateUse1 TorchDispatch 键来实现这一目标。此举主要是为了满足 Meta 的需求，Meta 近期重新燃起了购买 TPU 的兴趣，且不愿转向 JAX。这也将让那些喜欢 PyTorch 而不喜欢 JAX 的用户也能使用 TPU。 这种新的 PyTorch 与 TPU 的结合，将为习惯在 GPU 上使用 PyTorch 的机器学习科学家提供更顺畅的过渡，使他们能够转而在 TPU 上使用 PyTorch，并利用 TPU在总拥有成本（TCO）方面更高的性能优势。 vLLM &amp; SGLang **CUDA生态系统占据优势的另一个领域是开放式生态推理。**从历史上看，vLLM 和 SGLang 将 CUDA 作为一等公民提供支持（而 ROCm 则是二等公民）。现在，谷歌希望加入 vLLM 和 SGLang 的开放式推理生态系统，并已宣布通过一种非常“独特”的集成方式，为 vLLM 和 SGLang 提供 TPU v5p/v6e 的 beta 版支持。 vLLM 和 SGLang 目前通过将 PyTorch 建模代码转换为 JAX，并利用现有的成熟 JAX TPU 编译流程来实现这一点。未来，一旦 PyTorch XLA RFC #9684（即原生TPU PyTorch 后端）得到实现，vLLM 和 SGLang 计划评估是否转而使用该后端，而非通过 TorchAX 将建模从 PyTorch 转换为 JAX。 谷歌和 vLLM 声称，这种向 jax 路径的转换不需要对 PyTorch 建模代码做任何修改，但鉴于 vLLM TPU 目前支持的模型数量极少，目前对此表示怀疑。 此外，谷歌已将部分TPU内核开源并集成到vLLM中，例如经过TPU优化的分页注意力内核、计算-通信重叠的GEMM内核以及其他一些量化矩阵乘法内核。 谷歌的产业链信息 Basket 名称 公司代码 公司中文名称 英文全称 核心业务领域 产业链角色 与 Basket 主题关联（AI 芯片核心逻辑） TPU Basket（张量处理单元相关） GOOG US 谷歌 Alphabet Inc. 互联网搜索、AI 技术研发、TPU 芯片设计 芯片设计（TPU 发明者） 全球首个 TPU（张量处理单元）研发方，用于自身 AI 模型训练 / 推理，开源 TPU 架构生态 AVGO US 博通 Broadcom Inc. 半导体组件、光纤通信、数据中心芯片 芯片设计 + 零部件供应 为 TPU 提供高速接口芯片、射频组件，支撑 TPU 与服务器的连接效率 LITE US 鲁门特姆 Lumentum Holdings Inc. 光通信器件、激光组件、光学模块 光学零部件供应 提供 TPU 服务器所需的高速光模块（数据传输核心部件） CLS US 康宁 Corning Incorporated 特种玻璃、光纤、半导体封装材料 材料供应 为 TPU 芯片封装、服务器机箱提供高耐热 / 高透光玻璃基板 TTMI US 泰科电子 TE Connectivity Ltd. 工业连接器、传感器、射频组件 连接器件供应 提供 TPU 与主板、服务器的高可靠性连接器（信号传输关键部件） FIX US 菲尼特里 Finisar Corporation 光通信模块、激光二极管、光学传感器 光模块供应 为 TPU 数据中心提供 100G/400G 高速光模块，保障 AI 算力集群通信 FLEX US 伟创力 Flex Ltd. 电子制造服务（EMS）、服务器组装 制造服务提供商 承接 TPU 服务器的组装、测试业务，属于 AI 算力硬件制造环节 Nvidia Basket（英伟达生态相关） NVDA US 英伟达 NVIDIA Corporation GPU 设计、AI 芯片、自动驾驶芯片 核心芯片设计（生态主导） 全球 AI 算力核心（GPU/HOLOCAUST 芯片），Basket 核心龙头，支撑 AI 训练 / 推理 ORCL US 甲骨文 Oracle Corporation 企业软件、云计算、数据库服务 云服务 + 软件生态 与 Nvidia 合作推出 Oracle Cloud Infrastructure（OCI），搭载 A100/H100 芯片提供 AI 云服务 MSFT US 微软 Microsoft Corporation 操作系统、云计算（Azure）、AI 服务 云服务 + 软件生态 Azure 云深度集成 Nvidia GPU，推出 Copilot AI 工具，是 Nvidia 芯片最大云客户之一 SMCI US 超微电脑 Super Micro Computer Inc. 高性能服务器、AI 算力集群硬件 硬件集成商 专为 Nvidia GPU 设计 “Ultra Server”，是 AI 算力集群核心硬件供应商 2382 TT 台达电 Delta Electronics, Inc. 电源管理系统、工业自动化、散热方案 配套硬件供应 为 Nvidia GPU 服务器提供高效电源和散热解决方案（AI 算力功耗核心需求） 6601138 CH 中芯国际 Semiconductor Manufacturing International Corporation 集成电路制造（晶圆代工） 芯片制造 国内唯一能量产 14nm 芯片的代工厂，潜在为 Nvidia 供应链提供中低端芯片制造支持 3231 TT 大立光 Largan Precision Co., Ltd. 手机 / 服务器摄像头镜头、光学组件 光学零部件供应 为搭载 Nvidia 芯片的 AI 终端（如自动驾驶汽车、边缘计算设备）提供镜头 3017 TT 日月光 Advanced Semiconductor Engineering, Inc. 芯片封装测试、半导体制造服务 芯片封装测试 为 Nvidia GPU 芯片提供先进封装（如 CoWoS）测试服务，提升芯片性能 APH US 安费诺 Amphenol Corporation 高频连接器、射频电缆、传感器 连接器件供应 提供 Nvidia GPU 与服务器主板的高频连接器，保障算力传输稳定性 8210 TT 联电 United Microelectronics Corporation (UMC) 集成电路制造（晶圆代工） 芯片制造 为 Nvidia 供应链提供中低端芯片代工（如汽车级 AI 芯片），补充台积电产能 2308 TT 台积电 Taiwan Semiconductor Manufacturing Company Limited 全球领先集成电路制造（晶圆代工） 核心芯片制造 独家代工 Nvidia A100/H100 等高端 AI 芯片（5nm/3nm 工艺），是 Nvidia 生态核心制造支柱 2059 TT 鸿海（富士康） Hon Hai Precision Industry Co., Ltd. 电子制造服务（EMS）、服务器组装 硬件制造集成商 组装 Nvidia GPU 服务器、AI 算力集群，是 Nvidia 硬件最大代工伙伴之一 034020 KS 三星电子 Samsung Electronics Co., Ltd. 半导体（存储 / 逻辑芯片）、电子设备 芯片制造 + 存储供应 为 Nvidia 提供 HBM（高带宽存储）芯片（AI GPU 核心配套），同时竞争高端芯片代工市场 Trainium Basket（亚马逊 AI 芯片相关） AMZN US 亚马逊 Amazon.com, Inc. 电子商务、云计算（AWS）、Trainium 芯片设计 芯片设计 + 云服务（生态主导） 自研 Trainium 芯片（用于 AWS AI 训练），Basket 核心龙头，对标 Nvidia GPU MRVL US 迈威尔科技 Marvell Technology Group Ltd. 数据中心芯片、存储控制器、网络芯片 配套芯片设计 为 AWS Trainium 服务器提供存储控制器和网络接口芯片，优化数据传输效率 6669 TT 联发科 MediaTek Inc. 移动芯片、AI 边缘芯片、物联网芯片 芯片设计 与 AWS 合作推出边缘 AI 芯片，兼容 Trainium 生态，支撑边缘 AI 推理场景 3661 TT 纬创 Wistron Corporation 电子制造服务（EMS）、服务器组装、云计算硬件 硬件制造集成商 为 AWS 组装搭载 Trainium 芯片的 AI 服务器，属于 AWS 算力硬件核心代工厂 2345 TT 奇美电子 Chi Mei Optoelectronics Corp. 显示面板、光学组件、半导体材料 显示 / 材料供应 为 AWS Trainium 服务器提供高分辨率显示面板（数据中心监控场景）和光学材料 ALAB US 奥罗拉微电子 Aurora Labs Ltd. 半导体设计工具、AI 芯片验证方案 设计工具供应 为 Trainium 芯片提供设计验证工具，加速芯片研发和量产效率 Reference[1] (SemiAnalysis) TPUv7: Google Takes a Swing at the King[2] (国泰海通证券) 《Gemini 3、TPU、端侧 AI 应用更新报告》 - 2025.12.02","link":"/Blog/2025/12/03/TPU%20%E4%B8%8E%20GPU%20%E7%9A%84%E6%9C%AA%E6%9D%A5%E7%AB%9E%E4%BA%89%E6%A0%BC%E5%B1%80%E6%80%81%E5%8A%BF/"},{"title":"The Design Principle of PyTorch Architecture","text":"The article introduces the detailed principles that drive the implementation of PyTorch and how these principles are reflected in the PyTorch architecture. Design principlesPyTorch’s success stems from weaving previous ideas into a design that balances speed and ease of use. There are four main principles behind our choices: Be Pythonic: Data scientists are familiar with the Python language, its programming model, and it stools. Put researchers first: PyTorch strives to make writing models, data loaders, and optimizers as easy and productive as possible. Provide pragmatic performance: To be useful, PyTorch needs to deliver compelling performance,although not at the expense of simplicity and ease of use. Worse is better: Given a fixed amount of engineering resources, and all else being equal, the time saved by keeping the internal implementation of PyTorch simple can be used to implement additional features, adapt to new situations, and keep up with the fast pace of progress in the field of AI. Therefore it is better to have a simple but slightly incomplete solution than a comprehensive but complex and hard to maintain design. Usability Centric DesignDeep learning models are just Python programsThe neural networks themselves evolved rapidly from simple sequences of feed forward layers into incredibly varied numerical programs often composed of many loops and recursive functions. To support this growing complexity, PyTorch foregos the potential benefits of a graph-meta programming based approach to preserve the imperative programming model of Python. PyTorch extends this to all aspects of deep learning workflows. Defining layers, composing models, loading data, running optimizers, and parallelizing the training process are all expressed using the familiar concepts developed for general purpose programming. This solution ensures that any new potential neural network architecture can be easily implemented with PyTorch. Interoperability and extensibilityEasy and efficient interoperability is one of the top priorities for PyTorch because it opens the possibility to leverage the rich ecosystem of Python libraries as part of user programs. Hence, PyTorch allows for bidirectional exchange of data with external libraries. For example, it provides a mechanism to convert between NumPy arrays and PyTorch tensors using the torch.from_numpy() function and .numpy() tensor method. Similar functionality is also available to exchange data stored using the DLPack format. Automatic differentiationIn its current implementation, PyTorch performs reverse-mode automatic differentiation, which computes the gradient of a scalar output with respect to a multivariate input. Differentiating functions with more outputs than inputs is more efficiently executed using forward-mode automatic differentiation, but this use case is less common for machine learning applications. Common Code SnippetsConfiguration12345678910111213141516171819202122# PyTorch versiontorch.__version__ # PyTorch versionconda update pytorch torchvision -c pytorch # Update PyTorch# CUDAtorch.version.cuda # Check corresponding CUDA Versiontorch.cuda.is_available() # Check whether there is CUDA supporttorch.cuda.empty_cache() # Manually release GPU storage after stops runningCUDA_VISIBLE_DEVICES=0,1 python train.py # Run on a specific GPU: Command Lineos.environ['CUDA_VISIBLE_DEVICES'] = '0,1' # Run on a specific GPU: Code# cuDNNtorch.backends.cudnn.version() # Corresponding cuDNN versiontorch.backends.cudnn.benchmark = True # Increase the speed, but calculation are slightly different due to the randomness.torch.backends.cudnn.deterministic = True # Avoid this randomness/fluctuation of results# GPU typetorch.cuda.get_device_name(0) # GPU type# Fixed random seedtorch.manual_seed(0)torch.cuda.manual_seed_all(0) Tensor123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# Tensor Informationtensor.type() # Data typetensor.size() # Shape of the tensor. It is a subclass of Python tupletensor.dim() # Number of dimensions.# Type convertions (Float in PyTorch is much faster than double.)torch.set_default_tensor_type(torch.FloatTensor) # Set default tensor type.tensor = tensor.cuda()tensor = tensor.cpu()tensor = tensor.float()tensor = tensor.long()# Torch.Tensor &lt;==&gt; NumPy.ndarrayndarray = tensor.cpu().numpy() # torch.Tensor -&gt; np.ndarray.tensor = torch.from_numpy(ndarray).float() # np.ndarray -&gt; torch.Tensor.# If ndarray has negative stride:# This means that your numpy array has undergone such operation: image = image[..., ::-1]# User Case: # If you don’t want to flip the image, if for example you have already trained a network with un-flipped images, then you can save and load the image before passing it for inference.# Solution:tensor = torch.from_numpy(ndarray.copy()).float()# Reshapetensor = torch.reshape(tensor, shape)# Shuffle the first dimensiontensor = tensor[torch.randperm(tensor.size(0))]# tensor [::-1]: Assume tensor has shape N*D*H*W.tensor = tensor[:, :, :, torch.arange(tensor.size(3) - 1, -1, -1).long()]# Replication Operation | New/Shared memory | Still in computation graph |tensor.clone() # | New | Yes |tensor.detach() # | Shared | No |tensor.detach.clone()() # | New | No |# Splicingtensor = torch.cat(list_of_tensors, dim=0) # Stitching along the given dimension: 3 * 10×5 -&gt; 30×5tensor = torch.stack(list_of_tensors, dim=0) # Add one more dimension: 3 * 10×5 -&gt; 3×10×5# One-hot CodeN = tensor.size(0)one_hot = torch.zeros(N, num_classes).long()one_hot.scatter_(dim=1, index=torch.unsqueeze(tensor, dim=1), src=torch.ones(N, num_classes).long())# Zero Elementstorch.nonzero(tensor) # Index of non-zero elementstorch.nonzero(tensor == 0) # Index of zero elementstorch.nonzero(tensor).size(0) # Number of non-zero elementstorch.nonzero(tensor == 0).size(0) # Number of zero elements# Equal Judgementtorch.allclose(tensor1, tensor2) # float tensortorch.equal(tensor1, tensor2) # int tensor# Expandtorch.reshape(tensor, (64, 512, 1, 1)).expand(64, 512, 7, 7) # Expand tensor of shape 64*512 to shape 64*512*7*7.# Matrix Multiplicationresult = torch.mm(tensor1, tensor2) # (m*n) * (n*p) -&gt; (m*p)result = torch.bmm(tensor1, tensor2) # Batch matrix multiplication: (b*m*n) * (b*n*p) -&gt; (b*m*p)result = tensor1 * tensor2 # Element-wise multiplication# Euclidean Distancedist = torch.sqrt(torch.sum((X1[:,None,:] - X2) ** 2, dim=2)) # X1: m*d, X2: n*d. Reference Official Resources **Paper: **PyTorch: An Imperative Style, High-Performance Deep Learning Library Github: PyTorch Examples PyTorch Forum PyTorch Documentation Zhihu: PyTorch Cookbook(Collection of commonly used code snippets) 一文理解 PyTorch: 附代码实例 Github awesome-pytorch-chinese awesome-Pytorch-list pytorch-handbook","link":"/Blog/2021/01/17/The-Design-Principle-of-Pytorch-Architecture/"},{"title":"What is Knative?  K for Kubernetes + Native","text":"Knative enables serverless workloads to run on Kubernetes clusters, and makes building and orchestrating containers with Kubernetes faster and easier. Why Kubernetes needs KnativeSometimes, Kubernetes could be a complex tool for develops while during the container orchestration: template multiple repetitive tasks, pulling code from remote repo, provisioning container image. And the most difficult part is to incorporating Kubernetes-managed resources into another CI/CD pipeline, because that will need custom coding and setup to smooth the pipeline, such as configuring network connections. That;s why we look Knative for help. Knative utilize one single YAML manifest file to creating the containers, performing network manipulation, also taking care of load balancing. Making containers serverlessServerless computing have several characteristic that differentiate itself from traditional computing model: Provisions computing resources on demand, scaling transparently based on requests - and scaling to zero when requests are no longer made. Offloads all infrastructure management tasks - scaling, scheduling, patching, provisioning, etc. - to the cloud provider, allowing developers to focus their time and effort on development and innovation. Enables cloud customers to pay only for resources being used - they never pay for idle capacity. All in all: event driven architecture run on cloud infrastructures and pay as you go. How Knative works: Knative componentsKnative sits on top of Kubernetes and adds three main components: Build, Serving, and Eventing. Build Knative uses Kubernetes APIs and other tools for its Build process. A developer can create a single manifest (typically a YAML file) that specifies all the variables - location of the source code, required dependencies, etc. - and Knative uses the manifest to automate the container build. Pulling source code from a code repository, such as GitHub Installing the underlying dependencies—such as environment variables and software libraries—that the code needs to run Building container images Putting container images into a registry where Kubernetes (and other developers) can find it. Serving The Serving component deploys and runs containers as scalable Knative services. Serving provides the following important capabilities: Configuration defines and maintains the state of a service. It also provides version management: Each modification to the configuration creates a new version of the service, and previous versions are saved. Intelligent service routing lets developers route traffic to different versions of the service. Suppose you’ve created new version of a service, but want to deploy it to a subset of users before migrating all users. Intelligent service routing lets you route a percentage of user requests to the new service and the rest of the request to a previous version; as you become more confident in the new service, you can route more traffic to it. Autoscaling. Knative can scale services up into the thousands of instances; it can also scale them down to zero - that is, no instances of the container at all - which is critical for supporting serverless applications. Eventing Knative queues and delivers those events to the appropriate containers, so there’s no need to write scripts or implement middleware for the functionality. Knative Event sources make it easier for developers to create connections to third-party event producers. ReferenceFormal Knative.dev Take-away What is Knative Serving? (A Colorful Guide) Blog IBM Cloud Learn Hub / What is Knative? Istio and Knative: Extending Kubernetes for a New Developer Experience Knative: The Serverless Environment for Kubernetes Fans Enterprise Software Development with Knative Alibaba Cloud - What Exactly Is Knative?","link":"/Blog/2022/06/22/What-is-Knative-K-for-Kubernetes-Native/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/Blog/2019/04/21/hello-world/"},{"title":"大模型的价格趋势与定价艺术","text":"分析当前主流大模型的价格趋势。 [TOC] 重要结论当前主流大模型的价格趋势 目前主流大模型在问题回复的资金消耗的差异是巨大的（近百倍的差距），这往往被使用者或者公司所忽略。 根据 模型价格消耗2.3 模型能力与硬件亲和度2.4 推理和训练的盈亏平衡分析推理为训练买单：实验室努力将模型生命周期计算中更多的部分分配给能带来收入的推理工作，且要尽可能追求最高的利润率。我们下方的表格 * 展示了在不同的推理利润率和计算分配情况下，计算成本的预期回报率。 *Simplified sensitivity analysis: neglects people costs and assumes all inference generates revenue. Can also be interpreted in terms of token count between inference &amp; training (2DN vs. 6DN, MFU: ~15% vs. ~45%). 2.5 算力集群的建设军备竞赛**计划中约 1 吉瓦规模的集群将于 2026 年投入使用：**在美国的实验室中，集群规模日益成为一个标志性特征，在招聘时尤其有用。如果估值依据的是集群规模而非采用率或财务指标，那么可能会形成一个更大的泡沫。 Code Name IT Power at YE 2026 Number of Chips Chip Type Total TFLOPS Provider xAI - Colossus 1,200 MW GB200/300 550,000 3,488,148,649 xAI Meta - Promethus 1,020 MW GB200/300 500,000 3,171,044,226 Meta OpenAI - Stargate 880 MW GB200/300 400,000 2,469,594,595 Oracle Anthropic - Project Rainer 780MW Tranium 2 800,000 1,040,000,000 AWS * 谷歌 DeepMind 也在爱荷华州、内布拉斯加州和俄亥俄州建立了许多值得关注的集群。然而，这些项目的分布式特性以及可用信息的缺乏，导致它们未被列入表格中。 2.6 1GW 的AI数据中心盈利水平分析2.6.1 资本支出，折旧与摊销，成本结构**NewStreet Research 给出了一个关于 1GW 数据中心的财务模型：500亿 CAPEX，110亿年总成本 ** 资本支出是建设数据中心的总投入，1GW AI 数据中心总 Capex 为 500 亿美元，具体构成如下： 计算与存储（Compute and storage）：300 亿美元，占总 Capex 的 60% GPU：210 亿美元（占总 Capex 的 42%），是核心硬件成本。每 GW 需要 60 万块芯片，单块 GPU 平均售价（ASP）3.5 万美元，单 GPU 平均功耗 1.7kW。 CPU：10 亿美元（占 2%）。 其他服务器和存储：80 亿美元（占 16%）。 网络（Networking）：60 亿美元，占总 Capex 的 12%。 建筑、电力与冷却（Building, power &amp; cooling）：140 亿美元，占总 Capex 的 28%，是支撑算力运行的基础设施成本。 D&amp;A 是将资本支出在资产使用寿命内逐年分摊的费用，直接影响年度成本结构： 不同资产的使用寿命决定了 D&amp;A 的年限：GPU、CPU、其他服务器存储、网络设备：使用寿命 5 年。建筑、电力与冷却设施：使用寿命 10 年。 年度 D&amp;A 总额为 86 亿美元（$8.6bn p.a.），具体拆分： GPU：42 亿美元 / 年（210 亿 ÷ 5 年），占总 D&amp;A 的 50%。 CPU：2 亿美元 / 年（10 亿 ÷ 5 年），占 2%。 其他服务器和存储：16 亿美元 / 年（80 亿 ÷ 5 年），占 18%。 网络：12 亿美元 / 年（60 亿 ÷ 5 年），占 14%。 建筑、电力与冷却：14 亿美元 / 年（140 亿 ÷10 年），占 16%。 年度全部成本为110 亿美元（$11bn p.a.），由 “D&amp;A” 和 “现金成本（Cash Costs）” 组成： D&amp;A 占比 75 - 80%：86 亿美元 / 年，是最主要的年度成本，反映了资产折旧对利润的持续压力。 现金成本占比 20 - 25%：24 亿美元 / 年，具体拆分： 电力：12 亿美元 / 年（占总成本的 11%）。年均能耗 8TWh，电价 $0.15/kWh（计算：8TWh×$0.15/kWh = $1.2bn）。 维护、软件及其他：12 亿美元 / 年（占总成本的 11%）。 2.6.2 数据中心盈利水平分析（以 Oracle &amp; OpenAI 的合作为例）虽然目前公开信息还无法精确计算出甲骨文在此笔交易中的最终盈利，但我们可以根据现有数据，对其盈利水平和财务模型进行一次深入的推演分析。下面这个表格梳理了与本次交易相关的一些关键已知数据和合理的估算参数，可以作为我们分析的基础。 项目 数据/估算 合同规模 4.5 GW (总计) 年度费用 300亿美元 硬件投资估算 ~400亿美元 (以阿比林1.2GW园区为例，部署约40万GPU) 甲骨文官方毛利率指引 30% - 40% (AI基础设施，扣除土地、数据中心、电力和计算设备成本后) 收入端：主要来自OpenAI支付的300亿美元/年的巨额租金。 成本端：主要包含以下几大块： 硬件折旧：这是最大头的成本。根据的分析，一个类似的GPU数据中心项目中，服务器折旧是成本中绝对的大头。如果4.5GW的总投资按数百亿美元计算，其每年的折旧费用将非常惊人。 电力成本：1GW的数据中心年耗电量约为8 TWh，电费约12亿美元。4.5GW的规模，年电费成本预计超过50亿美元。 托管与运维成本：包括场地租金、网络、冷却和维护等。在的模型中，这项与电费成本相加，年支出约20亿美元（针对较小规模）。 融资成本：如此大规模的投资，甲骨文很可能通过借款进行，由此产生的利息费用也是一笔不小的开支。 SemiAnalysis 针对这份交易也给出了一个盈利分析，以 40W 块 GB200 的数据中心来进行预测： 基础指标 Chips in Service（在用芯片数量）：每年稳定在 400,000 块（GB200 芯片）。 Compute Rental（算力租赁单价）：2.60 USD/hr/GPU，是算力服务的单位定价，为收入核算的基础。 收入端：算力租赁业务的规模与稳定性 Revenue（营业收入）：年营收在86.93 亿–91.60 亿美元区间，整体保持高位且小幅波动。 说明 “算力租赁” 是核心收入来源，市场需求稳定，具备较强的营收持续性。 成本端：构成与变化逻辑 （1）直接成本（影响毛利） Hosting Cost（托管成本）：年支出10.01 亿 – 12.27 亿美元，逐年上升。 反映算力集群的托管运维复杂度增加（如场地、基础服务外包成本上升）。 Electricity Cost（电力成本）：年支出7.55 亿 – 8.13 亿美元，逐年上升。 是算力运行的核心可变成本，与芯片规模、电价波动或能效优化节奏有关（按芯片数量比例换算，与前 1GW 模型的电力成本逻辑完全匹配）。 （2）固定成本（影响运营利润） Server Depreciation（服务器折旧）：年支出32.86 亿 – 32.95 亿美元，几乎无波动。 源于服务器类资产的 “年限平均法” 折旧（资产使用寿命固定），是核心固定成本。 Amortization of Installation/Fit Out cost（安装 / 装修成本摊销）：前 3 年每年 2 亿美元，第 4-5 年为 0。 此类资产（如机房装修、专项安装工程）摊销年限为 3 年，到期后不再产生摊销成本。 Repair and Maintenance（维修维护成本）：每年 2 亿美元，固定支出。 保障服务器、设施的正常运行，属于常规运维成本。 Sales and Marketing Cost（销售与营销成本）：前 4 年每年 4.6 亿美元，第 5 年 4.3 亿美元。 前期为拓展市场投入营销资源，后期业务成熟后小幅缩减，属于合理的费用优化。 Annual Maintenance Cost（年度维护成本）：第 2-5 年每年 1 亿美元（第 1 年无）。 可能是新增长期维护合同或设备老化后专项维护的支出，体现运维策略的阶段性调整。 四、盈利端：高毛利与持续盈利性 Gross Profit（毛利）：67.53 亿 – 74.04 亿美元，毛利规模大且支撑力强。 毛利 = 收入 - 托管成本 - 电力成本，反映 “算力租赁” 业务的核心盈利能力。 Operating Profit（运营利润）：34.21 亿 – 40.59 亿美元，运营效率突出。 运营利润 = 毛利 - 各类运营成本（折旧、摊销、维修、营销、维护），体现扣除所有运营成本后的盈利水平。 Operating Margin（运营利润率）：39% – 44%，属于高毛利行业的典型表现。 说明业务模式的盈利能力极强，成本管控与收入规模的协同效应显著。 Interest Expense（利息支出）：前 4 年每年8.87 亿 – 8.90 亿美元，第 5 年降至 4.45 亿美元。 前期因资本投入产生较高债务利息，第 5 年或因债务偿还、利率调整而大幅下降。 Profit Before Tax（税前利润）：29.76 亿 – 31.69 亿美元，是运营利润扣除利息后的盈利。 Income Tax Expense（所得税费用）：5.95 亿 – 6.34 亿美元，税率约 20%（所得税 / 税前利润），符合企业所得税常规水平。 Contribution Profit（税后利润，实际为净利润）：23.81 亿 – 25.35 亿美元，每年稳定创造 20 多亿美元税后利润。 反映业务在覆盖所有成本（运营 + 财务 + 税务）后，具备持续的盈利产出能力。 ReferenceWu, Yangzhen, et al. “Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models.” arXiv preprint arXiv:2408.00724 (2024).","link":"/Blog/2025/10/14/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BB%B7%E6%A0%BC%E8%B6%8B%E5%8A%BF%E4%B8%8E%E5%AE%9A%E4%BB%B7%E8%89%BA%E6%9C%AF/"},{"title":"运用股指期货套期保值模拟分析","text":"对于证券公司等机构投资者而言，如何完善一个完整的套期保值流程和套保策略是提高证券投资收益和规避风险的一个重要环节。股指期货相对于股票现货最重要的两个功能就是杠杆和做空。运用杠杆能够提高资金的使用效率，利用做空能够实现套期保值。 投资组合选择选择 50 支股票作为初始投资组合 目标以选定的 50 支股票构成的投资组合作为套保对象，规避系统性风险，且套保目标制定为完全套保，以实现完全对冲系统性风险。 套保步骤 测算该投资组合中各支股票基于沪深 300 指数的 $β$ 值 评估该投资组合市值于沪深 300 指数的走势的相关性（把两个值都画折线图，能看出趋势一致就可以） 采用空头套保策略，在持有股票组合不变的情况下做空股指期货合约（可剔除部分 $β$ 值过低的个股） 套保策略 套保合约 基于套保期限接近的原则选择套保合约 查询沪深 300 当期市值 合约乘数 套保比率 基于投资组合的β值测算套保比率（持有的期货合约头寸于现货组合头寸之间的比率） 套期保值比率计算模型： 风险最小化套期保值 ← 一般采用 单位风险补偿最大化套期保值 效用最大化套期保值 股指期货合约份数 根据不同模型计算出的套保比率得出所需股指期货合约份数 $$ 期货合约份数 = \\frac{现货资产市值 \\times 最优套保比率}{股指期货当期价格 \\times 合约乘数} $$ 风险规避效果 度量不同模型计算出的套保比率的风险规避效果 $$ H e=\\frac{\\sigma_{n}^{2}-\\sigma_{k}^{2}}{\\sigma_{n}^{2}} $$ 分子：未套保资产组合收益方差减去套保后资产组合收益方差 拟定交易费率和保证金比率 保证金 = 收盘价 × 合约乘数 × 合约分数 交易费率 = 成交价 × 交易单位（合约乘数）× 交易费率 × 合约分数 交易成本 = 交易费率 × 交易额 = 交易费率 × 合约分数 × 合约乘数 × 结算价格 期货获利 = 合约分数 × 期现差价 × 合约乘数 - 交易成本 动态调整 执行期货合约，动态调整期货头寸（待定，时间充裕就做，否则就只做两个月的套保） 最优套期保值比率计量模型$$H=\\frac{\\operatorname{Cov}\\left(R_{s}, R_{f}\\right)}{\\operatorname{Var}\\left(R_{f}\\right)}=\\frac{\\rho \\sigma_{s} \\sigma_{f}}{\\sigma_{f}^{2}}=\\rho \\frac{\\sigma_{s}}{\\sigma_{f}}$$ 其中 $R_s$ 为现货市场收益率，$R_f$ 为期货市场收益率 具体可选择模型 β 值 投资组合的 β 值 = 以资金比例为权重的各股票 β 系数的加权平均值 最小二乘回归模型（OLS） 以 $R_s$ 为被解释变量， $R_f$ 为解释变量做最小二乘回归，得到的回归系数 $β$ 即为最优套保比率。 双向自回归模型（B-VAR）$$\\begin{array}{l}{\\Delta \\ln S_{\\mathrm{t}}=C_{s}+\\sum_{i=1}^{l} \\alpha_{s} \\Delta \\ln S_{t-1}+\\sum_{i=1}^{l} \\beta_{s} \\Delta \\ln F_{t-1}+\\varepsilon_{s}} \\ {\\Delta \\ln F_{t}=C_{f}+\\sum_{i=1}^{l} \\alpha_{f} \\Delta \\ln F_{t-1}+\\sum_{i=1}^{l} \\beta_{f} \\Delta \\ln S_{t-1}+\\varepsilon_{f t}}\\end{array}$$其中 $C$ 为截距项，$α &amp; β$ 为回归系数，$ε$ 为服从独立同分布的随机误差项，要寻找最佳滞后值 $L$，从而使残差项的自相关性消除。最后根据公式计算出套保比率：$$h^{*}=\\frac{\\operatorname{Cov}\\left(\\varepsilon_{s t}, \\varepsilon_{f t}\\right)}{\\operatorname{Var}\\left(\\varepsilon_{f t}\\right)}$$克服 ols 可能忽略残差项自相关的缺陷 步骤： 确定最优滞后阶数，采用 AIC 方法，列出沪深 300 指数的前 5 阶 AIC，最小取值时的阶数即为最优滞后阶数。建立 B-var 模型：$$\\Delta \\ln \\mathrm{S}{1}=\\alpha+\\beta \\Delta \\ln \\mathrm{F}{\\mathrm{t}}+\\sum_{i=1}^{\\mathrm{k}} \\theta_{1} \\Delta \\ln \\mathrm{S}{-\\mathrm{i}}+\\sum{\\mathrm{i}=1}^{\\mathrm{m}} \\lambda_{1} \\Delta \\ln \\mathrm{F}{\\mathrm{t}-1}+\\varepsilon{\\mathrm{t}}$$**输入值：**解释变量为股指期货价格 被解释变量为现货价格 还需要输入滞后阶数 β 值即为所求套保比率 **需要展示的内容：**B-var 获得的 β 值 误差修正模型（ECM） 在 B-var 模型的基础上将两组数据回归得到的残差序列作为滞后项引入 ECM 作为解释变量。 步骤： 对 B-var 中得到的残差序列进行 ADF 检验，判断是否平稳；若平稳，则将序列作为滞后项引入作为解释变量进行回归。 需要展示内容： 残差项的 ADF 检验结果 ECM 的回归结果 广义自回归条件异方差模型（GARCH）$$\\Delta \\ln S_{t}=\\alpha+\\beta \\Delta \\ln F_{t-1}+\\gamma Z_{t-1}+\\varepsilon_{t}$$其中 $β$ 就是套保比率值 关于股指期货沪深 300 合约乘数：300 交易保证金：0.15 空头保证金：0.15 交易手续费：0.000050 最低交易保证金的收取标准：12% 交割日中国的 股指期货交割日 是合约交割月的第三个星期五 合约月份沪深 300 股指期货的合约月份有四个，即当月、下月及随后的两个季月，季月指 3 月、 6 月、 9 月、 12 月。也就是说，同时有四个合约在交易。比如，在 2010 年 3 月 2 日的沪深 300 股指期货仿真交易中，就同时有 IF1003、 IF1004、 IF1006、 IF1009 四个合约在交易，其中： IF1003 为当月合约， IF1004 为下月合约，IF1006 和 IF1009 为随后的两个季月合约。以 IF1006 为例， IF 为沪深 300 股指期货合约的交易代码，10 指 2010 年， 06 指到期交割月份为 6 月份。其余依此类推。 股指期货基差基差是股指期货标的指数价格与股指期货价格之间的差值，比如，在 4 月 6 日某时点，沪深 300 指数为 3250 点，IF1004 合约价格为 3310 点，则此时的基差为 3250 - 3310 = -60 点。 在股指期货交易中，由于现货价格与期货价格波动不一致、从而基差波动不确定而导致的风险被称为基差风险。 如果基差朝着有利方向变化，则不仅可以取得较好的套期保值效果，而且还可以获得额外的盈利；反之，不仅会影响套保效果，甚至还会蒙受损失。比如，在买入套期保值中，由于在套保结束时要买入现货并卖出期货合约，当基差变弱、即现货价格相对更低而期货价格相对更高时，投资者买低卖高就可能获利。 实验步骤 收集数据 股票数据 从所有深市 A 股中选取了 50 支股票用作初始股票组合，时间从 2009年 1 月 1 日到 2010 年 12 月 31 日 股指期货数据 选取了沪深 300从 2009年 1 月 1 日到 2010 年 12 月 31 日的现货价格日收盘价，主力合约期货价格日收盘价，从而得到基差序列。期货、现货收益率均采用对数收益率。数据来源于国泰安数据库。 验证投资组合市值于沪深 300 指数的走势的相关性 股指期货和股指数据的基差序列检验（预期结论） 正态性检验 检验基差序列是否满足正态分布，运用的统计量为 JB 统计量。JB 统计量十分显著，不满足正态分布。峰度大于 3，偏态为左偏，基差分布呈现典型的“尖峰肥尾”。 平稳性检验 用 ADF 检验基差序列的平稳性。基差 ADF 值大于置信水平为 1% 时的临界值，P 值显著，因此基差序列平稳。 检验序列是否存在“单位根”，确保回归回归结果中不存在“伪回归”。 检验方法： 计算样本数据的 ADF 值，并和 5% 置信水平下的 t 检验临界值作比较，如果 ADF 检验值大于 5% 置信水平下的 t 检验临界值则序列不平稳。 检验结果不平稳则对原序列进行一阶差分，得到新的序列，再进行 ADF 检验（步骤同上）。 一阶段差分代码： 1genr st = log(st) - log(st(-1)) 其中 $st$ 为原序列 需要展示的内容：沪深 300 期货与现货价格几次（一般为两次）平稳检验的结果，包括 ADF 值和 5% 置信水平下 t 检验的值。 协整检验 使用 E-G 检验法，检验两组变量的协整关系。（E-G检验法的使用条件是两组变量的单整阶数一样） 步骤如下： 提取两组数据 OLS 检验后的残差项 e 对残差项 e 进行 ADF 测试 如果残差项序列不存在单位根，则通过 E-G 检验，说明两组序列具有长期协整关系。 **需要展示的内容：**两组数据的残差项 e 进行 ADF 测试后的结果 最优套期保值比率的估计 计算期货合约份数 计算套保结果 得出最终结论","link":"/Blog/2019/05/20/%E8%BF%90%E7%94%A8%E8%82%A1%E6%8C%87%E6%9C%9F%E8%B4%A7%E5%A5%97%E6%9C%9F%E4%BF%9D%E5%80%BC%E6%A8%A1%E6%8B%9F%E5%88%86%E6%9E%90/"}],"tags":[{"name":"Reinforce Learning","slug":"Reinforce-Learning","link":"/Blog/tags/Reinforce-Learning/"},{"name":"AWS","slug":"AWS","link":"/Blog/tags/AWS/"},{"name":"Certified Machine Learning - Specialty","slug":"Certified-Machine-Learning-Specialty","link":"/Blog/tags/Certified-Machine-Learning-Specialty/"},{"name":"Certified","slug":"Certified","link":"/Blog/tags/Certified/"},{"name":"Serverless","slug":"Serverless","link":"/Blog/tags/Serverless/"},{"name":"Apps","slug":"Apps","link":"/Blog/tags/Apps/"},{"name":"NMF","slug":"NMF","link":"/Blog/tags/NMF/"},{"name":"Machine-Learning","slug":"Machine-Learning","link":"/Blog/tags/Machine-Learning/"},{"name":"Clustering","slug":"Clustering","link":"/Blog/tags/Clustering/"},{"name":"Spark","slug":"Spark","link":"/Blog/tags/Spark/"},{"name":"Serverless - Terraform","slug":"Serverless-Terraform","link":"/Blog/tags/Serverless-Terraform/"},{"name":"CNN","slug":"CNN","link":"/Blog/tags/CNN/"},{"name":"Deep-Learning","slug":"Deep-Learning","link":"/Blog/tags/Deep-Learning/"},{"name":"NLP","slug":"NLP","link":"/Blog/tags/NLP/"},{"name":"EKK","slug":"EKK","link":"/Blog/tags/EKK/"},{"name":"Log","slug":"Log","link":"/Blog/tags/Log/"},{"name":"Word2Vec","slug":"Word2Vec","link":"/Blog/tags/Word2Vec/"},{"name":"Django","slug":"Django","link":"/Blog/tags/Django/"},{"name":"Docker","slug":"Docker","link":"/Blog/tags/Docker/"},{"name":"React","slug":"React","link":"/Blog/tags/React/"},{"name":"AWS - Serverless","slug":"AWS-Serverless","link":"/Blog/tags/AWS-Serverless/"},{"name":"Coffee","slug":"Coffee","link":"/Blog/tags/Coffee/"},{"name":"Android","slug":"Android","link":"/Blog/tags/Android/"},{"name":"JetPack","slug":"JetPack","link":"/Blog/tags/JetPack/"},{"name":"Kotlin","slug":"Kotlin","link":"/Blog/tags/Kotlin/"},{"name":"BERT","slug":"BERT","link":"/Blog/tags/BERT/"},{"name":"GPT-2","slug":"GPT-2","link":"/Blog/tags/GPT-2/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/Blog/tags/Deep-Learning/"},{"name":"Econometrics","slug":"Econometrics","link":"/Blog/tags/Econometrics/"},{"name":"AI","slug":"AI","link":"/Blog/tags/AI/"},{"name":"Overview of AWS","slug":"Overview-of-AWS","link":"/Blog/tags/Overview-of-AWS/"},{"name":"LaTeX","slug":"LaTeX","link":"/Blog/tags/LaTeX/"},{"name":"MySQL","slug":"MySQL","link":"/Blog/tags/MySQL/"},{"name":"TOEFL","slug":"TOEFL","link":"/Blog/tags/TOEFL/"},{"name":"PyTorch","slug":"PyTorch","link":"/Blog/tags/PyTorch/"},{"name":"Cloud Native - Kubernetes","slug":"Cloud-Native-Kubernetes","link":"/Blog/tags/Cloud-Native-Kubernetes/"},{"name":"Knative","slug":"Knative","link":"/Blog/tags/Knative/"}],"categories":[{"name":"Algorithm","slug":"Algorithm","link":"/Blog/categories/Algorithm/"},{"name":"AWS","slug":"AWS","link":"/Blog/categories/AWS/"},{"name":"Reinforce Learning","slug":"Algorithm/Reinforce-Learning","link":"/Blog/categories/Algorithm/Reinforce-Learning/"},{"name":"Overview","slug":"AWS/Overview","link":"/Blog/categories/AWS/Overview/"},{"name":"AWS Certified Solution Architect Associate","slug":"AWS/AWS-Certified-Solution-Architect-Associate","link":"/Blog/categories/AWS/AWS-Certified-Solution-Architect-Associate/"},{"name":"AWS Certified Machine Learning Specialty","slug":"AWS/AWS-Certified-Machine-Learning-Specialty","link":"/Blog/categories/AWS/AWS-Certified-Machine-Learning-Specialty/"},{"name":"Architecture","slug":"AWS/Architecture","link":"/Blog/categories/AWS/Architecture/"},{"name":"NMF","slug":"Algorithm/NMF","link":"/Blog/categories/Algorithm/NMF/"},{"name":"Big Data","slug":"Big-Data","link":"/Blog/categories/Big-Data/"},{"name":"Serverless","slug":"AWS/Serverless","link":"/Blog/categories/AWS/Serverless/"},{"name":"Terraform","slug":"Terraform","link":"/Blog/categories/Terraform/"},{"name":"Deep Learning","slug":"Algorithm/Deep-Learning","link":"/Blog/categories/Algorithm/Deep-Learning/"},{"name":"Natural Language Processing","slug":"Algorithm/Natural-Language-Processing","link":"/Blog/categories/Algorithm/Natural-Language-Processing/"},{"name":"Development","slug":"Development","link":"/Blog/categories/Development/"},{"name":"Spark","slug":"Big-Data/Spark","link":"/Blog/categories/Big-Data/Spark/"},{"name":"NLP","slug":"Algorithm/NLP","link":"/Blog/categories/Algorithm/NLP/"},{"name":"Lifestyle","slug":"Lifestyle","link":"/Blog/categories/Lifestyle/"},{"name":"Serverless","slug":"Terraform/Serverless","link":"/Blog/categories/Terraform/Serverless/"},{"name":"Finance","slug":"Finance","link":"/Blog/categories/Finance/"},{"name":"AI","slug":"AI","link":"/Blog/categories/AI/"},{"name":"Log Stack","slug":"Development/Log-Stack","link":"/Blog/categories/Development/Log-Stack/"},{"name":"LaTeX","slug":"LaTeX","link":"/Blog/categories/LaTeX/"},{"name":"Full Stack","slug":"Development/Full-Stack","link":"/Blog/categories/Development/Full-Stack/"},{"name":"SQL","slug":"Development/SQL","link":"/Blog/categories/Development/SQL/"},{"name":"English Study","slug":"English-Study","link":"/Blog/categories/English-Study/"},{"name":"Cloud Native","slug":"Cloud-Native","link":"/Blog/categories/Cloud-Native/"},{"name":"Android","slug":"Development/Android","link":"/Blog/categories/Development/Android/"},{"name":"Econometrics","slug":"Finance/Econometrics","link":"/Blog/categories/Finance/Econometrics/"},{"name":"Analytics","slug":"AI/Analytics","link":"/Blog/categories/AI/Analytics/"},{"name":"Kubernetes","slug":"Cloud-Native/Kubernetes","link":"/Blog/categories/Cloud-Native/Kubernetes/"},{"name":"Asset Management","slug":"Finance/Asset-Management","link":"/Blog/categories/Finance/Asset-Management/"},{"name":"Knative","slug":"Cloud-Native/Kubernetes/Knative","link":"/Blog/categories/Cloud-Native/Kubernetes/Knative/"}],"pages":[{"title":"","text":"404 *{margin:0;padding:0;outline:none;font-family:\\5FAE\\8F6F\\96C5\\9ED1,宋体;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;-khtml-user-select:none;user-select:none;cursor:default;font-weight:lighter;} .center{margin:0 auto;} .whole{width:100%;height:100%;line-height:100%;position:fixed;bottom:0;left:0;z-index:-1000;overflow:hidden;} .whole img{width:100%;height:100%;} .mask{width:100%;height:100%;position:absolute;top:0;left:0;background:#000;opacity:0.6;filter:alpha(opacity=60);} .b{width:100%;text-align:center;height:400px;position:absolute;top:50%;margin-top:-230px}.a{width:150px;height:50px;margin-top:30px}.a a{display:block;float:left;width:150px;height:50px;background:#fff;text-align:center;line-height:50px;font-size:18px;border-radius:25px;color:#333}.a a:hover{color:#000;box-shadow:#fff 0 0 20px} p{color:#fff;margin-top:40px;font-size:24px;} #num{margin:0 5px;font-weight:bold;} var num=4; function redirect(){ num--; document.getElementById(\"num\").innerHTML=num; if(num","link":"/Blog/404.html"},{"title":"Archives","text":"","link":"/Blog/archives/index.html"},{"title":"友情链接 Links","text":"卡拉云低代码工具 https://kalacloud.com 卡拉云是一个用来开发后台系统的平台，它可以帮助你快速开发在公司内部使用的后台系统。 上面的友情链接顺序不分先后，申请或者互换友链请在下方留言。Let’s change the world with code and heart.","link":"/Blog/links/index.html"},{"title":"Tags","text":"","link":"/Blog/tags/index.html"},{"title":"Categories","text":"","link":"/Blog/categories/index.html"},{"title":"","text":"My Footprint HipaperA fashional newspaper, blog theme for Hexo. ☞ Preview Demo | 查看中文使用文档 Installation Get it from GitHub 1$ git clone https://github.com/iTimeTraveler/hexo-theme-hipaper.git themes/hipaper Enable Modify theme setting in _config.yml to hipaper. 1234# Extensions## Plugins: http://hexo.io/plugins/## Themes: http://hexo.io/themes/theme: hipaper 3. Update 12$ cd themes/hipaper$ git pull FeaturesLogo: Image or TextYou can set a image as your logo instead of original text title. Like this: just enable avatar field in hipaper/_config.yml. 12345678# Put your avatar.jpg into `hexo-site/themes/hipaper/source/` directory.# url is target link (E.g. `url: https://hexo.io/logo.svg` or `url: css/images/mylogo.jpg`)avatar: enable: true width: 124 height: 124 bottom: 10 url: https://hexo.io/logo.svg Code HighlightHipaper use Tomorrow Theme for your code block. We have six options in total: default, normal, night, night blue, night bright, night eighties Above preview picture is default theme. the image below show other five Highlight themes. Modify highlight_theme in hipaper/_config.yml. 12345# Code Highlight theme# Available value:# default | normal | night | night eighties | night blue | night bright# https://github.com/chriskempson/tomorrow-themehighlight_theme: default SidebarYou can put your sidebar in left side, right side or bottom of your site by editing sidebar setting.Hipaper provides 7 built-in widgets: search social recent_posts category tag tagcloud archive All of them are enabled by default. You can edit them in widget setting. SearchHipaper use Insight Search to help you search anything inside your site without any third-party plugin. 12345# Searchsearch: insight: true # you need to install `hexo-generator-json-content` before using Insight Search swiftype: # enter swiftype install key here baidu: false # you need to disable other search engines to use Baidu search, options: true, false Attention: You need to install hexo-generator-json-content before using Insight Search. 1$ npm install -S hexo-generator-json-content FancyboxHipaper uses Fancybox to showcase your photos. You can use Markdown syntax or fancybox tag plugin to add your photos. 123![img caption](img url){% fancybox img_url [img_thumbnail] [img_caption] %} Comment supportHipaper has native support for DuoShuo &amp; Disqus comment systems. Modify the following snippets to Hipaper hipaper/_config.yml: 123# comment ShortName, you can choose only ONE to display.duoshuo_shortname: iTimeTravelerdisqus_shortname: Browser support ContributingAll kinds of contributions (enhancements, new features, documentation &amp; code improvements, issues &amp; bugs reporting) are welcome. Looking forward to your pull request. Special thanks to ATHEMES, who designed the original theme FASHIONISTA for Wordpress. LicenseHipaper is under the MIT license. See the LICENSE file for details.","link":"/Blog/about/index.html"}]}