<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>Convolutional Neural Networks | Gao Haojun</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="A comprehensive understanding of Convolutional Neural Networks, including Convolution and pooling.">
<meta name="keywords" content="Machine-Learning,CNN,Deep-Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Convolutional Neural Networks">
<meta property="og:url" content="http://vincentgaohj.github.io/Blog/2019/04/30/Convolutional-Neural-Networks/index.html">
<meta property="og:site_name" content="Gao Haojun">
<meta property="og:description" content="A comprehensive understanding of Convolutional Neural Networks, including Convolution and pooling.">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://static.zybuluo.com/MOFD/2p3jbhnkh0ajshew3o8x81cd/v2-59c8bcf17c24119810ad3071b960f1ba_r.jpg">
<meta property="og:image" content="http://static.zybuluo.com/MOFD/gx3jiz12v5r6lsamfry5d3yt/v2-860cdc53a489be168e9a12845c7eadc4_r.jpg">
<meta property="og:image" content="http://static.zybuluo.com/MOFD/4zgbavl17f2nsfoximxeiqxm/%E6%8D%95%E8%8E%B7.PNG">
<meta property="og:image" content="http://static.zybuluo.com/MOFD/8cydpq6u5niyboa3rqku5vgb/1.PNG">
<meta property="og:image" content="http://static.zybuluo.com/MOFD/zrsfeooypt36zj9b7n5dszxr/tempsnip.png">
<meta property="og:image" content="http://static.zybuluo.com/MOFD/c1jbidjvki0w7pflpxpn1ii3/2.PNG">
<meta property="og:image" content="http://static.zybuluo.com/MOFD/p62m5i0fx7ap6wulkc3zxggx/1.JPG">
<meta property="og:image" content="http://static.zybuluo.com/MOFD/vpoja1owd0v9zclxa1ujvp9e/%E6%8D%95%E8%8E%B7.PNG">
<meta property="og:image" content="http://static.zybuluo.com/MOFD/wb16j0s8g3a6amlhx6oncnma/1093303-20170430195106397-414671399.jpg">
<meta property="og:updated_time" content="2019-05-20T13:23:46.806Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Convolutional Neural Networks">
<meta name="twitter:description" content="A comprehensive understanding of Convolutional Neural Networks, including Convolution and pooling.">
<meta name="twitter:image" content="http://static.zybuluo.com/MOFD/2p3jbhnkh0ajshew3o8x81cd/v2-59c8bcf17c24119810ad3071b960f1ba_r.jpg">
  
    <link rel="alternate" href="/Blog/atom.xml" title="Gao Haojun" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/Blog/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/Blog/" id="logo">Gao Haojun</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/Blog/">Home</a>
        
          <a class="main-nav-link" href="/Blog/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/Blog/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://vincentgaohj.github.io/Blog"></form>
      </div>
    </div>
  </div>
  <script>
    
    var number_of_banners = 6;
    var randnum = Math.floor(Math.random() * number_of_banners + 1);
    document.getElementById("banner").style.backgroundImage = "url(/Blog/css/images/banner" + randnum + ".jpg)";
    
</script>
</header>
      <div class="outer">
        <section id="main"><article id="post-Convolutional-Neural-Networks" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/Blog/2019/04/30/Convolutional-Neural-Networks/" class="article-date">
  <time datetime="2019-04-30T04:52:43.000Z" itemprop="datePublished">2019-04-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Convolutional Neural Networks
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><ul>
<li>本文适合已经对向后传播（Backpropagation）神经网络有所了解的同学进一步学习卷积神经网络（CNN），感到困难的同学可以自行学习BP后再阅读。</li>
</ul>
<a id="more"></a>
<hr>
<h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline:"></a>Outline:</h2><p>1.卷积（Convolution）</p>
<ul>
<li>什么是卷积</li>
<li>卷积为什么能够提取text，image中的特征</li>
<li>在NLP中直观的理解卷积</li>
<li>卷积的意义</li>
</ul>
<p>2.池化（Pooling）</p>
<ul>
<li>什么是池化</li>
<li>几种池化的方法</li>
<li>池化的意义</li>
</ul>
<h2 id="卷积（Convolution）"><a href="#卷积（Convolution）" class="headerlink" title="卷积（Convolution）"></a>卷积（Convolution）</h2><h3 id="什么是卷积"><a href="#什么是卷积" class="headerlink" title="什么是卷积"></a>什么是卷积</h3><p>我们现在有$f(x)和g(x)$这两个函数，卷积是发生在这两个函数之间的计算。</p>
<p>卷积的数学定义如下：</p>
<p>连续定义：</p>
<script type="math/tex; mode=display">
（f*g）(n)=\int_{-\infty}^{\infty}f(\tau)g(n-\tau)d\tau</script><p>离散定义：</p>
<script type="math/tex; mode=display">
(f * g)(n)=\sum_{\tau=_\infty}^{\infty}f(\tau)g(n-\tau)</script><p>如果是低年级的同学（就像我）没有接触过卷积一定会很困扰，这个看起来很复杂的东西到底是什么？$n,\tau$有没有什么特别的意义？（实际上就是个符号，在没有情景时没有现实意义）。关于公式的来源这里不赘述，同学们也不必纠结（以后有些课程中会讲到）。我们定义出这个公式，是因为其在现实中有着广泛的应用，本文着重从现实意义解释卷积。</p>
<p>我们可以观察到求和时$f,g$的自变量取值是一个定值$\tau+(n-\tau)=n$（$n$由我们想求解何处卷积而决定），<strong>这就是理解卷积的关键</strong>。卷积实在将两个函数相乘，再使他们的自变量和为常数后求积分。下面举几个例子。</p>
<h4 id="信号处理"><a href="#信号处理" class="headerlink" title="信号处理"></a>信号处理</h4><p>$f(t)$表示0-20时间内的离散信号输入,$g(t)$表示系统对一个输入信号响应随时间的衰减。在这个每隔一秒输入一个信号地系统，其某时刻地信号强度和之前输入地信号均有关(叠加的)。比如在$t=8$时，$t=0$输入的信号就衰减成$f(0)g(10)$其他的都以此类推，即此时的信号强度为：</p>
<script type="math/tex; mode=display">
(f*g)(10)=\sum_{\tau=0}^{10}f(\tau)g(10-\tau)</script><p><img src="http://static.zybuluo.com/MOFD/2p3jbhnkh0ajshew3o8x81cd/v2-59c8bcf17c24119810ad3071b960f1ba_r.jpg" alt="v2-59c8bcf17c24119810ad3071b960f1ba_r.jpg-187.8kB"></p>
<p>这正是上面所说的卷积，可以看出其在信号处理有这样的应用。如果我们把一个信号的衰减程度$g$看作它的权重，某一时刻的信号强度就可以看作之前输入信号的<strong>加权叠加</strong>。在这里我们可以说，卷积是一个函数以另一个函数为权重的<strong>加权叠加</strong>。（关于卷积还有另一种理解：将函数反转、反褶然后求和什么的，不是很直观，有兴趣的同学可以自行查看）。</p>
<p>至此我们说了两个关键点：</p>
<ul>
<li><strong>卷积是对两个数自变量和为常数的函数的积的求和（根本数学定义）。 </strong></li>
<li><strong>卷积中一个函数可以看作另一个函数的权，卷积可以看作加权叠加（一种理解方式）。</strong></li>
</ul>
<p>带着这两个观点我们再看一个例子</p>
<h4 id="掷骰子"><a href="#掷骰子" class="headerlink" title="掷骰子"></a>掷骰子</h4><p>同时掷两枚相同的骰子，求骰子的和为6的可能性。很明显骰子出现某个点数的概率是关于点数的常函数（函数值一直是1/6），这时两个函数自变量和为定值6，正是卷积发挥作用的场合。</p>
<p>概率为：​</p>
<script type="math/tex; mode=display">
(f*g)(6)=\sum_{\tau=1}^{5}f(\tau)g(6-\tau)</script><p>下图为和的所有可能，均可用卷积计算。<br><img src="http://static.zybuluo.com/MOFD/gx3jiz12v5r6lsamfry5d3yt/v2-860cdc53a489be168e9a12845c7eadc4_r.jpg" alt="v2-860cdc53a489be168e9a12845c7eadc4_r.jpg-94.1kB"></p>
<p>这个例子中把一个函数理解为另一个的权解释性并不好，所以关键的是理解数学定义后运用到合适的情境中。但在NLP和computer vision的领域中用<strong>加权叠加</strong>可以很好的解释卷积的作用。</p>
<hr>
<h3 id="卷积如何提取text，image特征"><a href="#卷积如何提取text，image特征" class="headerlink" title="卷积如何提取text，image特征"></a>卷积如何提取text，image特征</h3><p>我们在 NLP 和 Computer Vision 中使用 CNN 是因为其能够提取 text ，image 中的特征，反映在出现特定特征时卷积会得到较大值（什么算较大值是规定好的）。这一机制具体如何实现请继续往下阅读，先在这里说明以免引起困惑。</p>
<hr>
<h3 id="NLP和Computer-Vision中的卷积"><a href="#NLP和Computer-Vision中的卷积" class="headerlink" title="NLP和Computer Vision中的卷积"></a>NLP和Computer Vision中的卷积</h3><h4 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h4><p>一段文本作为词向量（非one-hot）输入神经网络，hidden-layer的神经元就会进行读取，CNN中正式以卷积操作来完成读取的。</p>
<p><strong>几个重要的概念</strong>：</p>
<p>（统一写下，之后详解）</p>
<ul>
<li>filter过滤器、kernel卷积核、neuron神经元，实际上在CNN中指的都是神经网络的神经元。</li>
<li>depth深度：一层卷积层（就是计算卷积的hidden-layer）有几个神经元depth就是几。</li>
<li>size:表示为$n\times n$的形式，指的是receptive field-窗口的大小。</li>
<li>stride步长：窗口一次移动的长度。</li>
</ul>
<p>我们现在有这样一个text：“tentative deal reached to keep government open”并用如下的词向量表示（有四列常称有4个channels）。</p>
<p><img src="http://static.zybuluo.com/MOFD/4zgbavl17f2nsfoximxeiqxm/%E6%8D%95%E8%8E%B7.PNG" alt="捕获.PNG-34.5kB"></p>
<p>CNN的神经元用卷积来获取这样的text。紫色方框是刚刚提到的receptive field（窗口），size为$4\times4$. 在文本矩阵从上到下按照设置的stride（步长）移动窗口（设置1就每次移动一格，设置2就移动两格），每次得到的矩阵都用我们的filter\kernel（由训练得到，就是神经元中的参数）与其进行卷积运算<strong>（矩阵中对应位置相乘再将所有位置的积相加，得到一个数就是卷积的结果）</strong>，比如第一个窗口卷积为-1.0，第二个为-0.5.这样得到了右边只有一个channel的向量完成了文本的输入。把filter矩阵中的对应位置元素看作窗口中对应位置元素的权重，就符合<strong>加权叠加。</strong><img src="http://static.zybuluo.com/MOFD/8cydpq6u5niyboa3rqku5vgb/1.PNG" alt="1.PNG-60.7kB"></p>
<p>这一步如何体现卷积的<strong>数学思想</strong>呢？</p>
<p>假设窗口中的矩阵为</p>
<script type="math/tex; mode=display">
g=
\left[
 \begin{matrix}
   a_{1,1} & a_{1,2} & a_{1,3} & a_{1,3} \\
   a_{2,1} &  a_{2,2} & a_{2,3} & a_{2,3} \\
   a_{3,1} & a_{3,2} & a_{3,3} & a_{3,3} \\
  \end{matrix} 
\right]</script><p>filter矩阵为：</p>
<script type="math/tex; mode=display">
f=
\left[
 \begin{matrix}
   b_{1,1} & b_{1,2} & b_{1,3} & b_{1,3} \\
   b_{2,1} & b_{2,2} & b_{2,3} & b_{2,3} \\
   b_{3,1} & b_{3,2} & b_{3,3} & b_{3,3} \\
  \end{matrix} 
\right]=\left[
 \begin{matrix}
   3 & 1& 2 & -3 \\
   -1 & 2 &1 & -3 \\
   1 & 1 & -1 & 1 \\
  \end{matrix} 
\right]</script><p>如果把我们的filter中心对称一下，</p>
<script type="math/tex; mode=display">
f=
\left[
 \begin{matrix}
   b_{1,1} & b_{1,2} & b_{1,3} & b_{1,3} \\
   b_{2,1} & b_{2,2} & b_{2,3} & b_{2,3} \\
   b_{3,1} & b_{3,2} & b_{3,3} & b_{3,3} \\
  \end{matrix} 
\right]=
\left[
 \begin{matrix}
1 & -1 & 1 & 1 \\
   -3 & 1 & 2 & -1 \\
   -3 & 2 & 1 & 3 \\
  \end{matrix} 
\right]</script><p>现在想得到和刚刚相同权重分配效果，不能让对应位置相乘，等效计算应该是</p>
<script type="math/tex; mode=display">
convolution=\sum_{n=1}^{3}a_{n,n}b_{4-n,4-n}</script><p>完美的符合了卷积的数学定义<strong>，自变量相加等于一个定值</strong>$（4,4）$。这样我们就了解了卷积在NLP中的运作方式了。</p>
<p>但是得到的向量比原先的文本短，这并不是好的结果，我们希望得到长度相同的结果，方法如下图所示：</p>
<p>在输入的始末添加<strong>padding（填充层</strong>）在从上到下进行相同得分步骤就能得到和text（未添加padding之前）相同长度的结果了。</p>
<p><img src="http://static.zybuluo.com/MOFD/zrsfeooypt36zj9b7n5dszxr/tempsnip.png" alt="tempsnip.png-82.4kB"></p>
<p>但是结果只有一个channel，表示一个feature，有点少了。这就是一个卷积层拥有多个神经元的作用，每个神经元都有一个filter\kernel（权重矩阵）来提取不同的特征：</p>
<p><img src="http://static.zybuluo.com/MOFD/c1jbidjvki0w7pflpxpn1ii3/2.PNG" alt="2.PNG-102kB"></p>
<p>第一个filter可能专注于polite，这段文本是不是礼貌的，如果礼貌就应该得到较大值；第二个可能是关于food，如果出现食物相关的应该得到较大值。至于得到较大值的原因， 那就是通过学习不断更新filter矩阵使其能够有这样的能力（Backpropagation）。</p>
<hr>
<h3 id="卷积的优势"><a href="#卷积的优势" class="headerlink" title="卷积的优势"></a>卷积的优势</h3><h4 id="为什么这样滑动receptive-field计算卷积就能获取feature呢？我们有这样的前提："><a href="#为什么这样滑动receptive-field计算卷积就能获取feature呢？我们有这样的前提：" class="headerlink" title="为什么这样滑动receptive field计算卷积就能获取feature呢？我们有这样的前提："></a>为什么这样滑动receptive field计算卷积就能获取feature呢？我们有这样的前提：</h4><ol>
<li><p><strong>许多feature不需要看整个文本只需要看其中的一部分即可</strong>;<br>比如，想知道有没有关于食物的phrase，只要你扫描的关于食物的部分，得到了较大的卷积值就ok，不需要整篇文章一起看.</p>
</li>
<li><p><strong>相同的feature可能会出现在text的不同位置，这样使用同一filter扫描不同位置就能得到;</strong><br>比如，对于food整篇文章就能公用一组参数.</p>
</li>
<li><p><strong>忽略我们截取的部分的语言合理性；</strong><br>Regardless of whether phrase is grammatical<br>Not very linguistically or cognitively plausible</p>
</li>
</ol>
<h4 id="使用卷积有这样的优势："><a href="#使用卷积有这样的优势：" class="headerlink" title="使用卷积有这样的优势："></a>使用卷积有这样的优势：</h4><ol>
<li><p><strong>共享权重，使得参数大幅度减少，减轻计算压力；</strong></p>
</li>
<li><p><strong>一般的全连接神经网络不关心各个输入之间的相关性，CNN这样就考虑了局部的特性；</strong></p>
</li>
</ol>
<p>为什么要使用卷积来分配权重，直接给每个词向量的每一维设置权重就好了呀。这就是以往全连接神经网络的思想，但这样的参数量过于庞大。在上面的例子中如果使用全谅解神经网络需要$8\times4=32$个参数才能提取text的一个feature但是CNN只需要12个。如果文本增长全连接神经网络的参数量也会上升，但卷积神经网络矩阵中仍是12个参数。</p>
<p>正是因为这样的特性使得CNN在ML占有一席之地，特别实在图像处理领域，全连接神经网络将每一个像素都分别赋予权重使得GPU负担过重，而卷积很好的解决了这个问题。把一部分一部分像素一起考虑，对于一个feature只使用一组参数，大大减少了图像识别的压力。比如我想知道这张图有没有手机，只需要对手机图像训练一组权重再对图像一部分一部分求卷积即可。</p>
<hr>
<h2 id="池化（Pooling）"><a href="#池化（Pooling）" class="headerlink" title="池化（Pooling）"></a>池化（Pooling）</h2><h3 id="什么是池化"><a href="#什么是池化" class="headerlink" title="什么是池化"></a>什么是池化</h3><p>相比卷积来说池化是一个比较简单的概念，其核心思想在于<strong>突出</strong>卷积计算的结果中<strong>重要的信息。</strong></p>
<h3 id="池化方法"><a href="#池化方法" class="headerlink" title="池化方法"></a>池化方法</h3><h4 id="Max-pooling"><a href="#Max-pooling" class="headerlink" title="Max pooling"></a>Max pooling</h4><p>取我们计算得到的矩阵每个channel的最大值，这样我们就能知道这段text是否含有我们想要的内容：如果第一行最大值足够大可能文本就是礼貌的。<br><img src="http://static.zybuluo.com/MOFD/p62m5i0fx7ap6wulkc3zxggx/1.JPG" alt="1.JPG-18.3kB"></p>
<h4 id="Average-Pooling"><a href="#Average-Pooling" class="headerlink" title="Average Pooling"></a>Average Pooling</h4><p>用每一个channel的平均值，这样我们就知道这段text在什么程度上是polite。</p>
<p><img src="http://static.zybuluo.com/MOFD/vpoja1owd0v9zclxa1ujvp9e/%E6%8D%95%E8%8E%B7.PNG" alt="捕获.PNG-14.3kB"></p>
<p>二者相比较大多数情况下MAX效果会好一些。因为text中feature的某些标志是稀少的，比如用了几个敬语就可以使一段话看起来礼貌；同时大多数的词和礼貌并无关系，比如and， or ，however。</p>
<h4 id="Local-max-Pooling"><a href="#Local-max-Pooling" class="headerlink" title="Local-max Pooling"></a>Local-max Pooling</h4><p>在<strong>图像处理</strong>中常用的方法，即选定一些区域，用这些区域的最大值来表示这些区域。作用为在保留主要特征的前提下压缩图片，减少数据和参数的量。</p>
<p><img src="http://static.zybuluo.com/MOFD/wb16j0s8g3a6amlhx6oncnma/1093303-20170430195106397-414671399.jpg" alt="1093303-20170430195106397-414671399.jpg-23.7kB"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://vincentgaohj.github.io/Blog/2019/04/30/Convolutional-Neural-Networks/" data-id="cjwlymb4b0004l8w7o7ylfuup" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/Blog/tags/CNN/">CNN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/Blog/tags/Deep-Learning/">Deep-Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/Blog/tags/Machine-Learning/">Machine-Learning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/Blog/2019/05/01/A-Comprehensive-Look-at-The-Empirical-Performance-of-Equity-Premium-Prediction/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          A Comprehensive Look at The Empirical Performance of Equity Premium Prediction
        
      </div>
    </a>
  
  
    <a href="/Blog/2019/04/21/Insight-into-Word2Vec/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Insight into Word2Vec</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/Blog/tags/CNN/">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/Blog/tags/Clustering/">Clustering</a></li><li class="tag-list-item"><a class="tag-list-link" href="/Blog/tags/Deep-Learning/">Deep-Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/Blog/tags/Econometrics/">Econometrics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/Blog/tags/LaTeX/">LaTeX</a></li><li class="tag-list-item"><a class="tag-list-link" href="/Blog/tags/Machine-Learning/">Machine-Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/Blog/tags/NMF/">NMF</a></li><li class="tag-list-item"><a class="tag-list-link" href="/Blog/tags/Word2Vec/">Word2Vec</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/Blog/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/Blog/tags/Clustering/" style="font-size: 10px;">Clustering</a> <a href="/Blog/tags/Deep-Learning/" style="font-size: 10px;">Deep-Learning</a> <a href="/Blog/tags/Econometrics/" style="font-size: 15px;">Econometrics</a> <a href="/Blog/tags/LaTeX/" style="font-size: 10px;">LaTeX</a> <a href="/Blog/tags/Machine-Learning/" style="font-size: 20px;">Machine-Learning</a> <a href="/Blog/tags/NMF/" style="font-size: 10px;">NMF</a> <a href="/Blog/tags/Word2Vec/" style="font-size: 10px;">Word2Vec</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/Blog/archives/2019/06/">June 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/Blog/archives/2019/05/">May 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/Blog/archives/2019/04/">April 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/Blog/2019/06/06/Constructing-Topical-Concept-Hierarchical-Taxonomy-of-Tourist-Attraction/">Constructing Topical Concept Hierarchical Taxonomy of Tourist Attraction</a>
          </li>
        
          <li>
            <a href="/Blog/2019/05/20/运用股指期货套期保值模拟分析/">运用股指期货套期保值模拟分析</a>
          </li>
        
          <li>
            <a href="/Blog/2019/05/14/Study-Notes-of-LaTeX/">Study Notes of LaTeX</a>
          </li>
        
          <li>
            <a href="/Blog/2019/05/01/A-Comprehensive-Look-at-The-Empirical-Performance-of-Equity-Premium-Prediction/">A Comprehensive Look at The Empirical Performance of Equity Premium Prediction</a>
          </li>
        
          <li>
            <a href="/Blog/2019/04/30/Convolutional-Neural-Networks/">Convolutional Neural Networks</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Gao Haojun<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/Blog/" class="mobile-nav-link">Home</a>
  
    <a href="/Blog/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/Blog/fancybox/jquery.fancybox.css">
  <script src="/Blog/fancybox/jquery.fancybox.pack.js"></script>


<script src="/Blog/js/script.js"></script>



  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!--<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>