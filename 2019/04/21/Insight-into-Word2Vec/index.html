<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Insight into Word2Vec - Gao Haojun</title><link rel="manifest" href="/Blog/manifest.json"><meta name="application-name" content="Gao Haojun"><meta name="msapplication-TileImage" content="/img/favicon.jpeg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Gao Haojun"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="A comprehensive understanding of Word2Vec, including background, development, and formula derivation."><meta property="og:type" content="blog"><meta property="og:title" content="Insight into Word2Vec"><meta property="og:url" content="http://vincentgaohj.github.io/Blog/2019/04/21/Insight-into-Word2Vec/"><meta property="og:site_name" content="Gao Haojun"><meta property="og:description" content="A comprehensive understanding of Word2Vec, including background, development, and formula derivation."><meta property="og:locale" content="en_US"><meta property="og:image" content="https://static.leiphone.com/uploads/new/article/740_740/201706/594b306c8b3b1.png?imageMogr2/format/jpg/quality/90"><meta property="og:image" content="http://mccormickml.com/assets/word2vec/training_data.png"><meta property="og:image" content="http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png"><meta property="og:image" content="http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png"><meta property="og:image" content="http://mccormickml.com/assets/word2vec/output_weights_function.png"><meta property="og:image" content="https://img-blog.csdn.net/20180902220822202?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JpdGNhcm1hbmxlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"><meta property="og:image" content="http://qiniu.gaohaojun.cn/one-word.png"><meta property="og:image" content="http://qiniu.gaohaojun.cn/cbow.png"><meta property="og:image" content="http://qiniu.gaohaojun.cn/skip-gram.jpg"><meta property="article:published_time" content="2019-04-21T05:58:25.000Z"><meta property="article:modified_time" content="2022-02-22T04:44:13.278Z"><meta property="article:author" content="Haojun(Vincent) Gao"><meta property="article:tag" content="NLP"><meta property="article:tag" content="Word2Vec"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://static.leiphone.com/uploads/new/article/740_740/201706/594b306c8b3b1.png?imageMogr2/format/jpg/quality/90"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://vincentgaohj.github.io/Blog/2019/04/21/Insight-into-Word2Vec/"},"headline":"Insight into Word2Vec","image":["http://mccormickml.com/assets/word2vec/training_data.png","http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png","http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png","http://mccormickml.com/assets/word2vec/output_weights_function.png","http://qiniu.gaohaojun.cn/one-word.png","http://qiniu.gaohaojun.cn/cbow.png","http://qiniu.gaohaojun.cn/skip-gram.jpg"],"datePublished":"2019-04-21T05:58:25.000Z","dateModified":"2022-02-22T04:44:13.278Z","author":{"@type":"Person","name":"Haojun(Vincent) Gao"},"publisher":{"@type":"Organization","name":"Gao Haojun","logo":{"@type":"ImageObject","url":"http://vincentgaohj.github.io/img/logo.jpg"}},"description":"A comprehensive understanding of Word2Vec, including background, development, and formula derivation."}</script><link rel="canonical" href="http://vincentgaohj.github.io/Blog/2019/04/21/Insight-into-Word2Vec/"><link rel="icon" href="/Blog/img/favicon.jpeg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/Blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?192470116504c325dfc73c99f66225ed";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-N6S845NT1J" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-N6S845NT1J');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 6.0.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/Blog/"><img src="/Blog/img/logo.jpg" alt="Gao Haojun" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/Blog/">Home</a><a class="navbar-item" href="/Blog/archives">Archives</a><a class="navbar-item" href="/Blog/categories">Categories</a><a class="navbar-item" href="/Blog/tags">Tags</a><a class="navbar-item" href="/Blog/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/VincentGaoHJ"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-04-21T05:58:25.000Z" title="2019-4-21 1:58:25 ├F10: PM┤">2019-04-21</time></span><span class="level-item"><a class="link-muted" href="/Blog/categories/Algorithm/">Algorithm</a><span> / </span><a class="link-muted" href="/Blog/categories/Algorithm/Natural-Language-Processing/">Natural Language Processing</a></span><span class="level-item">39 minutes read (About 5887 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Insight into Word2Vec</h1><div class="content"><p>A comprehensive understanding of Word2Vec, including background, development, and formula derivation</p>
<span id="more"></span>
<h2 id="背景：自然语言处理"><a href="#背景：自然语言处理" class="headerlink" title="背景：自然语言处理"></a>背景：自然语言处理</h2><p>自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。</p>
<p>自然语言处理的最最基础的部分就是要让计算机能够识别人类的语言，因此词向量也就应运而生了。词向量顾名思义就是以向量的形式表示词。</p>
<h2 id="几种常见的模型（由简单到复杂）"><a href="#几种常见的模型（由简单到复杂）" class="headerlink" title="几种常见的模型（由简单到复杂）"></a>几种常见的模型（由简单到复杂）</h2><h3 id="词袋模型"><a href="#词袋模型" class="headerlink" title="词袋模型"></a>词袋模型</h3><p>词袋模型的基本思想就是将文档中出现的所有词用一个袋子装起来。词袋模型不考虑词出现的先后顺序，并且认为文本中每个词出现的概率都是独立的，不依靠其他词的出现。</p>
<p>词袋模型可以看成是用向量来表示句子。即</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>词1</th>
<th>词2</th>
<th>……</th>
<th>词n</th>
</tr>
</thead>
<tbody>
<tr>
<td>句子1</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>句子2</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>……</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>句子m</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>纵坐标为选定的特征词（可以是文档中所有出现的词，也可以是选定的某一些词）。可以认为最初的空格里面全是0，当句子中出现了某个词就在相应的位置做加一的操作，最后得到的数字的组合就是相应的句子的向量。</p>
<h3 id="one-hot编码"><a href="#one-hot编码" class="headerlink" title="one-hot编码"></a>one-hot编码</h3><p>one-hot编码是词向量模型的一种体现。这种编码是将文档中出现的所有的词用一个1和N-1个0表示，N是文档中出现的单词的个数。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>维度1</th>
<th>维度2</th>
<th>……</th>
<th>维度n</th>
</tr>
</thead>
<tbody>
<tr>
<td>词1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>词2</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>……</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>词n</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>one-hot编码的好处是可以很好地表示离散数据，每一个词都可以用向量的形式表示出来。且一定程度上起到了升维的作用。在回归，分类，聚类等机器学习的算法中，特征之间距离的计算或者相似度的计算是非常重要的，而我们常用的距离或者相似度的计算都是在欧式空间的相似度计算。使用one-hot编码将离散特征的取值数字化表示到了欧式空间，离散特征的某个取值就对应与欧式空间的某个点。将离散型特征使用one-hot编码，有时确实会让特征之间的距离计算更加合理。</p>
<p>而坏处也比较明显，one-hot是词袋模型的体现，因此有着词袋模型的固有缺点（即忽略掉了词出现的顺序和词的上下文之间的联系）。并且one-hot编码会产生一个较大的稀疏矩阵，在用one-hot编码得到的向量计算时，如果词的维度很大可能造成维灾难而无法计算。</p>
<h3 id="统计语言模型"><a href="#统计语言模型" class="headerlink" title="统计语言模型"></a>统计语言模型</h3><p>N-Gram模型算是对上面词袋模型的补充吧。词袋模型认为文本中每个词出现的概率都是独立的，而这与我们的常规认知是不相符合的（不然完形填空该怎么做啊），而N-Gram模型则认为可以通过计算概率的方法来获得上下文之间的关系，并且参考的上下文的词越多，预测的越准确。</p>
<p>词袋模型的基本思想是用概率来表示上下文之间的关系</p>
<p>一个经典的例子：假设我们有一个由n个词组成的句子</p>
<script type="math/tex; mode=display">
\mathrm { s } = \left( w _ { 1 } , w _ { 2 } , w _ { 3 } , \ldots \ldots w _ { \mathrm { n } } \right)</script><p>如何衡量它的概率呢？让我们假设，每一个单词$w<em>i$都要依赖于从第一个单词$w_i$到它之前一个单词$w</em>{i-1}$的影响:</p>
<script type="math/tex; mode=display">
\begin{array} { c } 
\begin{array} { c } { p ( s ) = p \left( w _ { 1 } , w _ { 2 } , w _ { 3 } , \ldots , w _ { n } \right) = p \left( w _ { 1 } \right) \cdot p \left( w _ { 2 } | w _ { 1 } \right) \cdot p \left( w _ { 3 } | w _ { 2 } , w _ { 1 } \right) \cdot \ldots } \\ { \cdot p \left( w _ { n } | w _ { n - 1 } , w _ { n - 2 } , \ldots , w _ { 1 } \right) } \end{array}
\end{array}</script><p>这样求一个句子出现的概率就变成了求若干个条件概率。但这样又会出现参数过多的问题。因此我们引入马尔科夫假设，即一个词出现的概率只与它之前若干个出现的单词有关。</p>
<p>假设一个词出现的概率只与它前面$n$个词出现的概率相关，<strong>N-Gram</strong>也就由此而来了。通常情况下$n$越大，得到的概率越准确但同时计算量也越大，因此$n$一般取2或者3。</p>
<h3 id="N-Gram-思想"><a href="#N-Gram-思想" class="headerlink" title="N-Gram 思想"></a>N-Gram 思想</h3><p>一个先导的知识就是贝叶斯公式，这里关于贝叶斯公式不做过多的描述：</p>
<script type="math/tex; mode=display">
P(B|A) = \frac{P(A|B)P(B)}{P(A)}</script><script type="math/tex; mode=display">
P(B_i|A) = \frac{P(A|Bi)P(Bi)}{P(A)} \\
= \frac{P(A|Bi)P(Bi)}{\sum_{j=1}^{n} P(B_j)P(A|B_j)}</script><p>从理解的角度可以认为是：</p>
<script type="math/tex; mode=display">
P(规律|现象) = \frac{P(现象|规律)P(规律)}{P(现象)}</script><p>考虑$p(w_k|w_1^{k-1})$的近似计算，利用Bayes公式，有：</p>
<script type="math/tex; mode=display">
p(w_k|w_1^{k-1}) = \frac{p(w_1^k)}{p(w_1^{k-1})}</script><p>根据大数定理，我们可以近似表示：</p>
<script type="math/tex; mode=display">
p(w_k|w_1^{k-1}) \approx \frac{Count(w_1^k)}{Count(w_1^{k-1})}</script><p>举个简单的例子就是：</p>
<script type="math/tex; mode=display">
p(“你”|“我爱”) \approx \frac{Count(“我爱你”)}{Count(“我爱”)}</script><p>从上述公式能够看出，一个词出现的概率和它之前出现的所有词都相关，如果假定一个词出现的概率只与它前面的固定书目的词相关，这就是N-Gram模型的<strong>重要思想</strong>，其中的数学逻辑就是做了一个$N-1$阶的<strong>Markov假设</strong>，认为一个词出现的概率只与它前面的$N-1$个词相关，也就是：</p>
<script type="math/tex; mode=display">
p(w_k|w_1^{k-1}) \approx \frac{p(w_1^k)}{p(w_{k-n+1}^{k-1})}</script><p>利用大数定律：</p>
<script type="math/tex; mode=display">
p(w_k|w_1^{k-1}) \approx \frac{Count(w_1^k)}{Count(w_{k-n+1}^{k-1})}</script><p>举例：假如$N=2$</p>
<script type="math/tex; mode=display">
p(w_k|w_1^{k-1}) \approx \frac{Count(w_{k-1},w_{k})}{Count(w_{k-1})}</script><script type="math/tex; mode=display">
p(“你”|“我爱”) \approx \frac{Count(“爱你”)}{Count(“爱”)}</script><p><strong>所以N-Gram模型其实一个计算一个句子出现概率的模型，当我们需要计算一个句子出现的概率时，只需要计算相关的概率参数，然后将他们连乘起来就好了。</strong></p>
<p><strong>那么，我们怎么利用这样的思想，用机器学习的方法将语言进行建模呢？方法就是将我们所考虑的问题建成一个模型，然后构造出一个目标函数，然后对这个目标函数进行优化，从而求得一组最优的参数，然后利用这一组最优的参数进行预测。</strong></p>
<p>对于统计语言模型，可以利用<strong>最大似然</strong>的概念，将目标函数设置为：</p>
<script type="math/tex; mode=display">
\prod_{Corpus} p(w|Context(w))</script><p>利用前文的n-gram思想，$Context(w)=w_{i-n+1}^{i-1}$</p>
<p>实际中，我们采用极大对数似然，将乘法转换为加法，那么目标函数则为：</p>
<script type="math/tex; mode=display">
\sum_{Corpus} \log p(w|Context(w))</script><p>目标就是<strong>最大化极大对数似然函数。</strong></p>
<p><strong>一种简单的思路来解决这个问题，就是建立一个关于$w$和它上下文$Context(w)$的函数：</strong></p>
<script type="math/tex; mode=display">
p(w|Context(w)) = F(w,Context(w),\theta)</script><p>这样一来就不用每次都遍历数据库了，只需要训练模型得到参数集，之后使用参数集中的参数，那么输入唯一之后输出也就唯一了。</p>
<h3 id="两个重要模型"><a href="#两个重要模型" class="headerlink" title="两个重要模型"></a>两个重要模型</h3><p>词向量界中有两个很重要的模型：</p>
<ul>
<li><strong>CBOW (Continuous Bag-of-Words Model)</strong></li>
<li><strong>Skip-gram (Continuous Skip-gram Model)</strong></li>
</ul>
<p>Word2Vec 模型中，主要有 Skip-Gram 和 CBOW 两种模型，从直观上理解，Skip-Gram 是给定 input word 来预测上下文。而 CBOW 是给定上下文，来预测 input word。</p>
<p>Word2Vec 模型实际上分为了两个部分，<strong>第一部分为建立模型，第二部分是通过模型获取嵌入词向量</strong>。Word2Vec 的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵 —— 后面我们将会看到这些权重在 Word2Vec 中实际上就是我们试图去学习的 “word vectors”。基于训练数据建模的过程，我们给它一个名字叫 “Fake Task”，意味着建模并不是我们最终的目的。</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201706/594b306c8b3b1.png?imageMogr2/format/jpg/quality/90" alt="Skip-Gram 模型"></p>
<p>这两个模型是两个思想，基于这两个思想，Word2Vec给出了两个框架，一个利用了霍夫曼树（Hierarchical Softmax），一个利用了负采样（Negative Sampling）。</p>
<p><strong>CBOW模型的目标函数为：</strong></p>
<script type="math/tex; mode=display">
\sum_{w \in Corpus} \log p(w|Context(w))</script><p><strong>Skip-gram模型的目标函数为：</strong></p>
<script type="math/tex; mode=display">
\sum_{w \in Corpus} \log p(Context(w)|w)</script><h3 id="Skip-gram-模型讲解"><a href="#Skip-gram-模型讲解" class="headerlink" title="Skip-gram 模型讲解"></a>Skip-gram 模型讲解</h3><h4 id="Fake-Task"><a href="#Fake-Task" class="headerlink" title="Fake Task"></a>Fake Task</h4><p>训练模型的真正目的是获得模型基于训练数据学得的隐层权重。为了得到这些权重，我们首先要构建一个完整的神经网络作为我们的 “Fake Task”，后面再返回来看通过 “Fake Task” 我们如何间接地得到这些词向量。</p>
<p>下面的图中给出了一些我们的训练样本的例子。我们选定句子 <strong>“The quick brown fox jumps over lazy dog”</strong>，设定我们的窗口大小为 2（window_size=2），也就是说我们仅选输入词前后各两个词和输入词进行组合。下图中，蓝色代表 input word，方框内代表位于窗口内的单词。</p>
<p><img src="http://mccormickml.com/assets/word2vec/training_data.png" alt=""></p>
<p>我们的模型将会从每对单词出现的次数中习得统计结果。例如，我们的神经网络可能会得到更多类似（“Soviet“，”Union“）这样的训练样本对，而对于（”Soviet“，”Sasquatch“）这样的组合却看到的很少。因此，当我们的模型完成训练后，给定一个单词”Soviet“作为输入，输出的结果中”Union “或者” Russia “要比”Sasquatch“被赋予更高的概率。</p>
<p>我们如何来表示这些单词呢？首先，我们都知道神经网络只能接受数值输入，我们不可能把一个单词字符串作为输入，因此我们得想个办法来表示这些单词。最常用的办法就是基于训练文档来构建我们自己的词汇表（vocabulary）再对单词进行 one-hot 编码。</p>
<p><img src="http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png" alt="the architecture of our neural network"></p>
<p>隐层没有使用任何激活函数，但是输出层使用了 sotfmax。</p>
<p>我们基于成对的单词来对神经网络进行训练，训练样本是 (input word, output word) 这样的单词对，input word 和 output word 都是 one-hot 编码的向量。最终模型的输出是一个概率分布。</p>
<h4 id="隐藏层部分"><a href="#隐藏层部分" class="headerlink" title="隐藏层部分"></a>隐藏层部分</h4><p>看下面的图片，左右两张图分别从不同角度代表了输入层 - 隐层的权重矩阵。左图中每一列代表一个 10000 维的词向量和隐层单个神经元连接的权重向量。从右边的图来看，每一行实际上代表了每个单词的词向量。</p>
<p><img src="http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png" alt="word vectors"></p>
<p><strong>所以我们最终的目标就是学习这个隐层的权重矩阵。</strong></p>
<h4 id="输出层部分"><a href="#输出层部分" class="headerlink" title="输出层部分"></a>输出层部分</h4><p>经过神经网络隐层的计算，ants 这个词会从一个 1 x 10000 的向量变成 1 x 300 的向量，再被输入到输出层。输出层是一个 Softmax 回归分类器（这部分会在后面稍微展开一些），它的每个结点将会输出一个 0-1 之间的值（概率），这些所有输出层神经元结点的概率之和为 1。</p>
<p>下面是一个例子，训练样本为 (input word: “ants”， output word: “car”) 的计算示意图。</p>
<p><img src="http://mccormickml.com/assets/word2vec/output_weights_function.png" alt="output neuron"></p>
<h4 id="模型理解"><a href="#模型理解" class="headerlink" title="模型理解"></a>模型理解</h4><p>这部分官方已经给出了非常好的解释，这里就直接上英文原文吧：</p>
<blockquote>
<p>Ok, are you ready for an exciting bit of insight into this network?</p>
<p>If two different words have very similar “contexts” (that is, what words are likely to appear around them), then our model needs to output very similar results for these two words. And one way for the network to output similar context predictions for these two words is if <em>the word vectors are similar</em>. So, if two words have similar contexts, then our network is motivated to learn similar word vectors for these two words! Ta da!</p>
<p>And what does it mean for two words to have similar contexts? I think you could expect that synonyms like “intelligent” and “smart” would have very similar contexts. Or that words that are related, like “engine” and “transmission”, would probably have similar contexts as well.</p>
<p>This can also handle stemming for you – the network will likely learn similar word vectors for the words “ant” and “ants” because these should have similar contexts.</p>
</blockquote>
<h3 id="Softmax-详解"><a href="#Softmax-详解" class="headerlink" title="Softmax 详解"></a>Softmax 详解</h3><p>在机器学习尤其是深度学习中，softmax 是个非常常用而且比较重要的函数，尤其在多分类的场景中使用广泛。他把一些输入映射为 0-1 之间的实数，并且归一化保证和为 1，因此多分类的概率之和也刚好为 1。 </p>
<p>网上有一张图很好的说明了softmax的计算方法：</p>
<p><img src="https://img-blog.csdn.net/20180902220822202?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JpdGNhcm1hbmxlZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="softmax"></p>
<p>假设有一个数组 $V$，$V_i$表示 $V$ 中的第$ i $个元素，那么这个元素的 softmax 值为：</p>
<script type="math/tex; mode=display">
S_i = \frac{e^i}{\sum_j e^j}</script><p><strong>该元素的 softmax 值，就是该元素的指数与所有元素指数和的比值。</strong></p>
<h2 id="模型公式推导"><a href="#模型公式推导" class="headerlink" title="模型公式推导"></a>模型公式推导</h2><p>这里给出了几个模型的公式推导：</p>
<ul>
<li>神经网络模型（One-Word模型）</li>
<li>CBOW 模型</li>
<li>Skip-gram 模型</li>
</ul>
<h3 id="One-Word-模型"><a href="#One-Word-模型" class="headerlink" title="One-Word 模型"></a>One-Word 模型</h3><p><strong>One-Word</strong>是用神经网络来实现<strong>N-Gram</strong>（$n=2$时）。即<strong>N-Gram</strong>是思想，而<strong>One-Word</strong>模型是实现方法。即一个用<strong>one-hot</strong>编码的词作为输入，通过第一个权重矩阵得到隐藏层，再通过第二个权重矩阵的到输出层的前身，该前身再做<strong>Softmax</strong>得到输出端的向量表示。</p>
<p><img src="http://qiniu.gaohaojun.cn/one-word.png" alt="one-word"></p>
<p>设：词汇量的大小为$V$，隐藏层的大小为$N$。输入向量是一个<strong>one-hot</strong>编码的向量，<strong>one-hot</strong>编码的向量表示为$(x_1,x_2,…,x_v)$，其中只有一个$x_k$为1，其余的均为0。姑且认为$X(V \times 1), h(N \times 1), Y(V \times 1)$都是列向量</p>
<p>输入层和隐藏层之间是一个$V \times N$的矩阵$W_{V \times N}$</p>
<script type="math/tex; mode=display">
\left( \begin{array}{cccc}{\mathrm{W}_{11}} & {\mathrm{W}_{12}} & {\dots} & {\mathrm{W}_{1 n}} \\ {\mathrm{W}_{21}} & {\mathrm{W}_{22}} & {\dots} & {\mathrm{W}_{2 n}} \\ {\cdots} & {\cdots} & {\cdots} & {\cdots} \\ {\mathrm{W}_{\mathrm{vl}}} & {\mathrm{W}_{\mathrm{v} 2}} & {\dots} & {\mathrm{W}_{\mathrm{vn}}}\end{array}\right)</script><script type="math/tex; mode=display">
h=x^T W</script><p>则$h$为$\left( \begin{array}{llll}{\mathrm{W}<em>{\mathrm{kl}}} &amp; {\mathrm{W}</em>{\mathrm{k} 2}} &amp; {\dots} &amp; {\mathrm{W}_{\mathrm{kn}}}\end{array}\right)^{\mathrm{T}}$，即$W$矩阵的第$k$行的转置 （因为$x_k$为1，其余的均为0）</p>
<p>隐藏层到输出层是一个$N \times V$的权重矩阵</p>
<p>形式和上面那个矩阵类似，只是矩阵的元素用$\mathrm{w}_{\mathrm{ij}}^{\prime}$表示</p>
<script type="math/tex; mode=display">
\mathrm{U}_{\mathrm{j}}=\mathrm{W}_{\mathrm{j}}^{\prime} \mathrm{h}</script><p>注：$\mathrm{W}_{j}^{‘}$为$W$矩阵的第$j$行，$U_j$为输出层的第$j$个位置的前身</p>
<p>$U_j$经过一个<strong>Softmax回归</strong>得到$y_j$，$y_j$为正经的输出层的第$j$个位置</p>
<script type="math/tex; mode=display">
\mathrm{P}\left(\mathrm{w}_{j} | \mathrm{w}_{I}\right)=y_{j}=\frac{e^{j}}{\sum_{i=1}^{V} e^{i}}</script><p>即在输入已经确定的情况下，输出的值为$w_j$的概率为这个</p>
<p>更新$W$和$W^{‘}$矩阵</p>
<p>首先定义一个损失函数（我们希望这个损失函数最小）（至于为什么这么定义还是没太理解）</p>
<script type="math/tex; mode=display">
\mathrm{E}=-\mathrm{P}\left(\mathrm{w}_{\mathrm{o}} | \mathrm{w}_{\mathrm{i}}\right)=\mathrm{U}_{\mathrm{j}^{*}}-\log \left(\sum_{i=1}^{V} e^{i}\right)</script><p>首先更新$W^{‘}$矩阵：</p>
<p>这就需要用$E$来对$\mathrm{w}^{‘}_{\mathrm{ij}}$来求偏导，以获得最快更新$W^{‘}$的方向（即梯度方向）</p>
<p>在求偏导之前需要知道这样的前提 即</p>
<script type="math/tex; mode=display">
\frac{\partial \mathrm{E}}{\partial \mathrm{U}_{i}}=\mathrm{y}_{\mathrm{i}}-\mathrm{t}_{\mathrm{i}}</script><p>当预测准确的时候$t_i$为1，否则为0，这个计算公式可以看成是输出层的的预测误差，至于结果为什么是这样的暂时还不是很清楚</p>
<p>在上述前提下</p>
<script type="math/tex; mode=display">
\frac{\partial \mathrm{E}}{\partial \mathrm{W}_{\mathrm{ij}}}=\frac{\partial \mathrm{E}}{\partial \mathrm{U}_{\mathrm{i}}} * \frac{\partial \mathrm{U}_{\mathrm{i}}}{\partial \mathrm{w}_{\mathrm{ij}}}=\left(y_{i}-t_{i}\right) * h_{i}</script><p>则权重更新方程为</p>
<script type="math/tex; mode=display">
\mathbf{W}_{i j}^{\prime}=\mathbf{W}_{i j}^{\prime}-\eta\left(y_{i}-t_{i}\right) * h_{i}</script><p>其次更新$W$矩阵<br>和上文差不多，也是需要$W$对$\mathrm{W}_{\mathrm{ij}}$求偏导</p>
<p>求之前也需要知道一个前提：</p>
<script type="math/tex; mode=display">
\frac{\partial \mathrm{E}}{\partial h_{i}}=\sum_{i=1}^{V} \frac{\partial \mathrm{E}}{\partial U_{i}} \frac{\partial U_{i}}{\partial h_{i}}=\sum_{i=1}^{V}\left(y_{i}-t_{i}\right) * w_{i j}^{\prime}</script><p>此式子的结果用$\mathrm{EH}_{\mathrm{i}}$代为表示：</p>
<script type="math/tex; mode=display">
\mathrm{EH}=\left(\mathrm{EH}_{1} \cdot \mathrm{EH}_{2}, \ldots . . \mathrm{EH}_{\mathrm{N}}\right)</script><p>在此前提下求$E$对$\mathrm{W}_{i j}$的偏导数为：</p>
<script type="math/tex; mode=display">
\frac{\partial \mathrm{E}}{\partial \mathrm{w}_{i j}}=\frac{\partial \mathrm{E}}{\partial \mathrm{h}_{j}} \frac{\partial \mathrm{h}_{j}}{\partial \mathrm{w}_{i j}}=E H_{j} * X_{i}</script><p>由于X非1即0，所以权重更新公式为：</p>
<script type="math/tex; mode=display">
\mathrm{V}_{w i}=\mathrm{V}_{w i}-\eta E H</script><p>其中$\mathbf{V}_{w i}$为one-hot编码中非零行所对应的矩阵的行，其他行不用关心。</p>
<h3 id="CBOW模型（连续词袋模型）"><a href="#CBOW模型（连续词袋模型）" class="headerlink" title="CBOW模型（连续词袋模型）"></a>CBOW模型（连续词袋模型）</h3><ul>
<li><strong>基本概念</strong></li>
</ul>
<p>cbow的基本思想是用中心词的上下文的c个词来预测中心词。</p>
<p>连续词袋模型模型相当于是<strong>One-Word</strong>模型的补充，<strong>One-Word</strong>是一个输入，一个输出，<strong>CBOW</strong>是c个输入，1个输出。</p>
<p><img src="http://qiniu.gaohaojun.cn/cbow.png" alt="cbow模型"></p>
<p>$\mathbf{X}<em>{1 k}$到$\mathbf{X}</em>{c k}$ 是上下文从第一个到第C个单词的one-hot编码，这C个one-hot编码通过相应位置加和求平均的方法得到一个$1 \times V$的向量，该向量再乘以我们期望得到的第一个矩阵$\mathrm{W}_{\mathrm{V} \times \mathrm{N}}$来得到隐藏层的向量$h_i$（一个$1 \times N$的向量），即</p>
<script type="math/tex; mode=display">
\mathrm{h}=\frac{1}{C} W\left(\sum_{i=1}^{C} x_{i}\right)</script><p>上图的$h<em>i$表示的意思可以与上上图的$h_i$相比较。然后$h$乘以另一个我们期望得到的矩阵$\mathrm{W}</em>{\mathrm{N}^{‘} \mathrm{V}}^{*}$，得到一个$1 \times V$的向量U，再用Softmax得到一个$1 \times V$的向量$Y$，其中$Y_i$最大的那个值就是期待的中心词。通过不断地学习来调整$W$和$W^{‘}$的值。</p>
<ul>
<li><strong>CBOW矩阵的调整</strong></li>
</ul>
<p>建立一个损失函数：</p>
<script type="math/tex; mode=display">
\mathrm{E}=-\log \left(\mathrm{P}\left(\mathrm{w}_{\mathrm{o}} | \mathrm{W}_{\mathrm{I}, 1} \ldots \mathrm{W}_{\mathrm{I}, \mathrm{c}}\right)\right)\\
=-U_{j^{*}}+\log \left(\sum_{j}^{V} e^{u_{i}}\right)\\
=-\mathrm{V}_{\mathrm{w}_{\mathrm{o}}}^{\prime T} \cdot h+\log \left(\sum_{i=1}^{V} e^{v_{v_{i}} T} \cdot h\right)</script><p>然后和上面one-word模型更新两个矩阵的方法类似  都是对相应的矩阵的元素求导得到梯度来更新矩阵。</p>
<script type="math/tex; mode=display">
\mathrm{V}_{\mathrm{wj}}^{\prime}=\mathrm{V}_{\mathrm{wj}}^{\prime}-\eta\left(\mathrm{y}_{\mathrm{i}}-\mathrm{t}_{\mathrm{i}}\right) \mathrm{h}</script><p>用上述公式来更新$W^{‘}$，其中$j=1,2, \dots \dots ,\mathrm{V}$</p>
<p>其中$\mathrm{V}^{\prime}_{\mathrm{w} j}$为隐藏层到输出等的矩阵的第$j$列</p>
<script type="math/tex; mode=display">
\mathrm{V}_{\mathrm{w}, \mathrm{I}, \mathrm{c}}=\mathrm{V}_{\mathrm{w}, \mathrm{I}, \mathrm{c}}-\frac{1}{\mathrm{C}} \eta \cdot \mathrm{EH}</script><p>用上述公式来更新$W$</p>
<p>其中$\mathrm{V}_{\mathrm{w}, \mathrm{I}, \mathrm{c}}$是输入上下文单词的第c个单词的输入向量。其中$c=1,2, \cdots \cdots ,\mathrm{C}$。</p>
<h3 id="skip-gram模型"><a href="#skip-gram模型" class="headerlink" title="skip-gram模型"></a>skip-gram模型</h3><ul>
<li><strong>模型概述</strong></li>
</ul>
<p>Skip-Gram模型可以看成是与CBOW模型相反的，即用一个中心词来推测其附近的c个上下文（注：得到的c个上下文不考虑与中心词之间的距离的影响）。</p>
<p><img src="http://qiniu.gaohaojun.cn/skip-gram.jpg" alt="Skip-Gram"></p>
<ul>
<li><strong>数学推导</strong></li>
</ul>
<p>隐藏层$h$仍然是矩阵$W$的第$k$行（$X_k=1$的情况下）</p>
<p>与之前稍微有点不同的是Skip-Gram模型的输出有多个，每个输出都使用的相同的$W^{‘}$来计算</p>
<script type="math/tex; mode=display">
\mathrm{P}\left(\mathrm{w}_{c, j}=w_{o, c} | w_{I}\right)=y_{c, j}=\frac{e^{u_{c, j}}}{\sum_{i=1}^{V} e^{u_{i}}}</script><p>其中$\mathrm{W}<em>{c, j}$是输出层第$c$个输出的第$j$个单词，而$w</em>{o, f}$是第c个输出的实际输出的单词，$\mathrm{U}_{c, j}$是h与$W^{‘}$的第$j$行向量相乘的结果，由于乘的都是同一个第$j$行，所以：</p>
<script type="math/tex; mode=display">
\mathrm{U}_{\mathrm{c}, \mathrm{j}}=\mathrm{U}_{\mathrm{j}}=V_{w_{j}}^{\prime T} h</script><p>再次构造损失函数：</p>
<script type="math/tex; mode=display">
\mathrm{E}=-\log \left(\mathrm{P}\left(w_{0,1}, w_{0,2}, \ldots w_{0, \mathrm{C}} | w_{\mathrm{I}}\right)\right) \\
=-\sum_{i=1}^{C} U_{j_{i}^{*}}+\operatorname{Clog}\left(\sum_{i=1}^{V} U_{i^{\prime}}\right) \\</script><script type="math/tex; mode=display">
\frac{\partial \mathrm{E}}{\partial \mathrm{U}_{c, j}}=y_{c, j}-t_{c, j}</script><p>用$\mathrm{EI}=\left{\mathrm{EI}<em>{1}, \mathrm{EI}</em>{2}, \ldots . \mathrm{EI}_{\mathrm{v}}\right}$来表示所有上下文单词的预测误差之和，即：</p>
<script type="math/tex; mode=display">
\mathrm{EI}_{\mathrm{j}}=\sum_{\mathrm{i}=1}^{\mathrm{C}} \mathrm{y}_{\mathrm{i}, \mathrm{j}}-\mathrm{t}_{\mathrm{i}, \mathrm{j}}</script><p>$\mathrm{W}^{\prime}$的权重矩阵更新公式为：</p>
<script type="math/tex; mode=display">
w_{i, j}^{\prime}=w_{i, j}^{\prime}-\eta^{*} E I_{j}^{*} h_{i}</script><p>W的权重矩阵更新公式为：</p>
<script type="math/tex; mode=display">
\mathrm{V}_{\mathrm{w}_{1}}=\mathrm{V}_{\mathrm{w}_{1}}-\eta \mathrm{EH}</script><p>其中</p>
<script type="math/tex; mode=display">
\mathrm{EH}=\sum_{j=1}^{V} E I_{j} * w_{i, j}^{\prime}</script><h4 id="参考来源："><a href="#参考来源：" class="headerlink" title="参考来源："></a>参考来源：</h4><p><a target="_blank" rel="noopener" href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Chris McCormick · Machine Learning Tutorials and Insights</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/7160330.html">刘建平 Pinard · CBOW 与 Skip-Gram 模型基础</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/peghoty/p/3857839.html">peghoty · word2vec 中的数学原理详解</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41664845/article/details/82971728">算法原理以及公式推导</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27234078?utm_source=qq&amp;utm_medium=social&amp;utm_oi=989578670297628672">理解Word2Vec之Skip-Gram模型</a></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Insight into Word2Vec</p><p><a href="http://vincentgaohj.github.io/Blog/2019/04/21/Insight-into-Word2Vec/">http://vincentgaohj.github.io/Blog/2019/04/21/Insight-into-Word2Vec/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Haojun(Vincent) Gao</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2019-04-21</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2022-02-22</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-globe"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/Blog/tags/NLP/">NLP</a><a class="link-muted mr-2" rel="tag" href="/Blog/tags/Word2Vec/">Word2Vec</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=62145317b846610019d3dc05&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" href="https://afdian.net/@vincent_gaohj" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>Afdian.net</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/Blog/img/alipay.jpg" alt="Alipay"></span></a><a class="button donate" href="https://www.buymeacoffee.com/gaohaojun" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/Blog/2019/04/30/Convolutional-Neural-Networks/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Convolutional Neural Networks</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/Blog/2019/04/21/hello-world/"><span class="level-item">Hello World</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://vincentgaohj.github.io/Blog/2019/04/21/Insight-into-Word2Vec/';
            this.page.identifier = '2019/04/21/Insight-into-Word2Vec/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'vincentgaohj' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><figure class="media-left"><a class="image" href="/Blog/2022/04/11/Deploying-AWS-Lambda-with-Terraform/"><img src="/Blog/gallery/Deploying-AWS-Lambda-with-Terraform/bailey_zindel.jpg" alt="Deploying AWS Lambda with Terraform"></a></figure><div class="media-content"><p class="date"><time dateTime="2022-04-11T06:02:44.000Z">2022-04-11</time></p><p class="title"><a href="/Blog/2022/04/11/Deploying-AWS-Lambda-with-Terraform/">Deploying AWS Lambda with Terraform</a></p><p class="categories"><a href="/Blog/categories/Terraform/">Terraform</a> / <a href="/Blog/categories/Terraform/Serverless/">Serverless</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/Blog/2022/02/16/Overview-of-AWS-Machine-Learning-Services/"><img src="/Blog/gallery/Overview-of-AWS-Machine-Learning-Services/cover.jpg" alt="Overview of AWS: Machine Learning Services (2022 Edition)"></a></figure><div class="media-content"><p class="date"><time dateTime="2022-02-16T06:02:44.000Z">2022-02-16</time></p><p class="title"><a href="/Blog/2022/02/16/Overview-of-AWS-Machine-Learning-Services/">Overview of AWS: Machine Learning Services (2022 Edition)</a></p><p class="categories"><a href="/Blog/categories/AWS/">AWS</a> / <a href="/Blog/categories/AWS/Overview/">Overview</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/Blog/2022/02/16/CICD-Deployment-with-AWS-SAM-using-Github-Action/"><img src="/Blog/gallery/CICD-Deployment-with-AWS-SAM-using-Github-Action/cover.jpg" alt="CI/CD Deployment with AWS SAM using Github Action"></a></figure><div class="media-content"><p class="date"><time dateTime="2022-02-16T06:02:44.000Z">2022-02-16</time></p><p class="title"><a href="/Blog/2022/02/16/CICD-Deployment-with-AWS-SAM-using-Github-Action/">CI/CD Deployment with AWS SAM using Github Action</a></p><p class="categories"><a href="/Blog/categories/AWS/">AWS</a> / <a href="/Blog/categories/AWS/Serverless/">Serverless</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/Blog/2021/10/30/LAMBADA-Method-How-to-use-Data-Augmentation-in-NLU/"><img src="/Blog/gallery/LAMBADA-Method-How-to-use-Data-Augmentation-in-NLU/cover.png" alt="LAMBADA Method: How to use Data Augmentation in NLU?"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-10-30T14:44:24.000Z">2021-10-30</time></p><p class="title"><a href="/Blog/2021/10/30/LAMBADA-Method-How-to-use-Data-Augmentation-in-NLU/">LAMBADA Method: How to use Data Augmentation in NLU?</a></p><p class="categories"><a href="/Blog/categories/Algorithm/">Algorithm</a> / <a href="/Blog/categories/Algorithm/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/Blog/2021/10/26/Not-Enough-Data-Deep-Learning-to-the-Rescue/"><img src="/Blog/gallery/Not-Enough-Data-Deep-Learning-to-the-Rescue/cover.jpg" alt="Not Enough Data? Deep Learning to the Rescue!"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-10-26T14:44:24.000Z">2021-10-26</time></p><p class="title"><a href="/Blog/2021/10/26/Not-Enough-Data-Deep-Learning-to-the-Rescue/">Not Enough Data? Deep Learning to the Rescue!</a></p><p class="categories"><a href="/Blog/categories/Algorithm/">Algorithm</a> / <a href="/Blog/categories/Algorithm/Natural-Language-Processing/">Natural Language Processing</a></p></div></article></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/Blog/categories/AWS/"><span class="level-start"><span class="level-item">AWS</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/Blog/categories/AWS/AWS-Certified-Machine-Learning-Specialty/"><span class="level-start"><span class="level-item">AWS Certified Machine Learning Specialty</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/Blog/categories/AWS/AWS-Certified-Solution-Architect-Associate/"><span class="level-start"><span class="level-item">AWS Certified Solution Architect Associate</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/Blog/categories/AWS/Architecture/"><span class="level-start"><span class="level-item">Architecture</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/Blog/categories/AWS/Overview/"><span class="level-start"><span class="level-item">Overview</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/Blog/categories/AWS/Serverless/"><span class="level-start"><span class="level-item">Serverless</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/Blog/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">9</span></span></a><ul><li><a class="level is-mobile" href="/Blog/categories/Algorithm/Deep-Learning/"><span class="level-start"><span class="level-item">Deep Learning</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/Blog/categories/Algorithm/NMF/"><span class="level-start"><span class="level-item">NMF</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/Blog/categories/Algorithm/Natural-Language-Processing/"><span class="level-start"><span class="level-item">Natural Language Processing</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/Blog/categories/Algorithm/Reinforce-Learning/"><span class="level-start"><span class="level-item">Reinforce Learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/Blog/categories/Big-Data/"><span class="level-start"><span class="level-item">Big Data</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/Blog/categories/Big-Data/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/Blog/categories/Development/"><span class="level-start"><span class="level-item">Development</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/Blog/categories/Development/Android/"><span class="level-start"><span class="level-item">Android</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/Blog/categories/Development/Full-Stack/"><span class="level-start"><span class="level-item">Full Stack</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/Blog/categories/Development/Log-Stack/"><span class="level-start"><span class="level-item">Log Stack</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/Blog/categories/Development/SQL/"><span class="level-start"><span class="level-item">SQL</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/Blog/categories/English-Study/"><span class="level-start"><span class="level-item">English Study</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/Blog/categories/Finance/"><span class="level-start"><span class="level-item">Finance</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/Blog/categories/Finance/Asset-Management/"><span class="level-start"><span class="level-item">Asset Management</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/Blog/categories/Finance/Econometrics/"><span class="level-start"><span class="level-item">Econometrics</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/Blog/categories/LaTeX/"><span class="level-start"><span class="level-item">LaTeX</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/Blog/categories/Lifestyle/"><span class="level-start"><span class="level-item">Lifestyle</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/Blog/categories/Terraform/"><span class="level-start"><span class="level-item">Terraform</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/Blog/categories/Terraform/Serverless/"><span class="level-start"><span class="level-item">Serverless</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/Blog/tags/AWS/"><span class="tag">AWS</span><span class="tag">19</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/AWS-Serverless/"><span class="tag">AWS - Serverless</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/Android/"><span class="tag">Android</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/Apps/"><span class="tag">Apps</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/BERT/"><span class="tag">BERT</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/CNN/"><span class="tag">CNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/Certified/"><span class="tag">Certified</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/Certified-Machine-Learning-Specialty/"><span class="tag">Certified Machine Learning - Specialty</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/Clustering/"><span class="tag">Clustering</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/Coffee/"><span class="tag">Coffee</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/Deep-Learning/"><span class="tag">Deep-Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/Django/"><span class="tag">Django</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/Docker/"><span class="tag">Docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/EKK/"><span class="tag">EKK</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/Econometrics/"><span class="tag">Econometrics</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/GPT-2/"><span class="tag">GPT-2</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/JetPack/"><span class="tag">JetPack</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/Kotlin/"><span class="tag">Kotlin</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/LaTeX/"><span class="tag">LaTeX</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/Log/"><span class="tag">Log</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/Machine-Learning/"><span class="tag">Machine-Learning</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/MySQL/"><span class="tag">MySQL</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/NLP/"><span class="tag">NLP</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/NMF/"><span class="tag">NMF</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/Overview-of-AWS/"><span class="tag">Overview of AWS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/PyTorch/"><span class="tag">PyTorch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/React/"><span class="tag">React</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/Reinforce-Learning/"><span class="tag">Reinforce Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/Serverless/"><span class="tag">Serverless</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/Serverless-Terraform/"><span class="tag">Serverless - Terraform</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/Spark/"><span class="tag">Spark</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/TOEFL/"><span class="tag">TOEFL</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/Blog/tags/Word2Vec/"><span class="tag">Word2Vec</span><span class="tag">1</span></a></div></div></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/Blog/img/self.jpg" alt="Haojun(Vincent) Gao"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Haojun(Vincent) Gao</p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/Blog/archives"><p class="title">45</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/Blog/categories"><p class="title">26</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/Blog/tags"><p class="title">34</p></a></div></div></nav></div></div><!--!--></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/Blog/"><img src="/Blog/img/logo.jpg" alt="Gao Haojun" height="28"></a><p class="is-size-7"><span>&copy; 2022 Haojun(Vincent) Gao</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Gallery" href="https://www.gaohaojun.com/"><i class="fab fa-fighter-jet"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Linkedin" href="https://www.instagram.com/vincent_gaohj/"><i class="fab fa-linkedin"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Instagram" href="https://www.instagram.com/vincent_gaohj/"><i class="fab fa-instagram"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/VincentGaoHJ"><i class="fab fa-github"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/Blog/js/column.js"></script><script src="/Blog/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/Blog/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/Blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/Blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/Blog/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>